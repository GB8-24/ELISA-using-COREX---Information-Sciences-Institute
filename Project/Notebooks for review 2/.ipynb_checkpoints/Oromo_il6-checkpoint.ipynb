{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"CorEx Hierarchical Topic Models\n",
    "Use the principle of Total Cor-relation Explanation (CorEx) to construct\n",
    "hierarchical topic models. This module is specially designed for sparse count\n",
    "data and implements semi-supervision via the information bottleneck.\n",
    "Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical\n",
    "Representations of High-Dimensional Data.\" AISTATS, 2015.\n",
    "Gallagher et al. \"Anchored Correlation Explanation: Topic Modeling with Minimal\n",
    "Domain Knowledge.\" TACL, 2017.\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu)\n",
    "Ryan J. Gallagher\n",
    "David Kale\n",
    "Lily Fierro\n",
    "License: Apache V2\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "#import corex_topic as ct\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\il6_original.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4d9e617a93f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read input data which is in .txt file line by line, split by tab(\\t) to a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\il6_original.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mlist_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\il6_original.txt'"
     ]
    }
   ],
   "source": [
    "# read input data which is in .txt file line by line, split by tab(\\t) to a list\n",
    "list_data=[]\n",
    "with open('C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\il6_original.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name the columns of datframe\n",
    "raw_data = pd.DataFrame(list_data,columns=['doc_id','text_data','class_type','additional']) \n",
    "#drop if any additional columns gets created as part of reading process\n",
    "raw_data=raw_data.drop('additional',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# strip columns for leading and trailing white spaces\n",
    "raw_data['doc_id']=raw_data.doc_id.str.strip()\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n docs x m attributes\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set doc_id as index\n",
    "raw_data= raw_data.set_index('doc_id')\n",
    "# change \"class_type\" column to \"categorical\" datatype\n",
    "raw_data['class_type'] = raw_data['class_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['class_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eval_incident - This should be considered as part of “indomain”\n",
    "\n",
    "raw_data.loc[raw_data['class_type']==\"eval_incident\", 'class_type'] = \"indomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT (Retweet) keyword removal\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('RT','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punctuation removal on 'text_data' column\n",
    "#print(string.punctuation)\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "#print(punct)\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: regex.sub('', x))\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove @names mentioned as part of tweets\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('\\@[a-zA-Z0-9]+','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate word count (length) of the document\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove emoji's from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Date Removal/ number removal from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\\d]+','',x))\n",
    "\n",
    "# strip whitespaces again \n",
    "\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "\n",
    "# calculate length again\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the documents with no words after pre-processing\n",
    "raw_data = raw_data.loc[raw_data.length>0]\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing documents with less than 5 words\n",
    "raw_data = raw_data.loc[raw_data.length>5]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the letters to lower case\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document count across 4 class labels (unk,nondomain,indomain,eval_incident)\n",
    "print(raw_data.class_type.value_counts())\n",
    "plt.hist('class_type',data=raw_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the formatted data to a new text file for future reference\n",
    "\n",
    "reset_data=raw_data.reset_index()\n",
    "reset_data.to_csv('C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\Oromo.txt', header=True, index=False, sep='\\t', mode='w',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stats to understand document length across 4 class labels\n",
    "raw_data.groupby('class_type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document Term matrix (binary matrix) with max_df =0.995, min_df =0.001\n",
    "vectorizer = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=True)\n",
    "doc_word_mat = vectorizer.fit_transform(raw_data.text_data)\n",
    "doc_word_mat = ss.csr_matrix(doc_word_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 25 topics\n",
    "\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=1500, verbose=False, seed=3192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The estimated probabilities of topics for each document can be accessed through p_y_given_x.\n",
    "print(topic_model.p_y_given_x) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "print(topic_model.labels) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color = '#4e79a7', width = 0.5)\n",
    "plt.xlabel('Topic', fontsize = 16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "\n",
    "#socho始aa socho始an socho始uun  socho始uu  \n",
    "anchor_words = [[\"hoongee\",\"hongee\"],[\"galaana\",\"galaanaa\"],[\"balaa\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=1500,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "topic_classification=anchored_topic_model.labels[:,0:3] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data=raw_data.reset_index()\n",
    "result[\"doc_id\"]=raw_data[\"doc_id\"]\n",
    "result[\"true_class_label\"]=raw_data[\"class_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.loc[(result['topic1']==True) | (result['topic2']==True) |  (result['topic3']==True) , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False) & (result['topic3']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.true_class_label !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.true_class_label, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-089e64aee950>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mF1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mF1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision' is not defined"
     ]
    }
   ],
   "source": [
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_word_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cd36565d9cc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0manchored_topic_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCorex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3192\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0manchored_topic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_word_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0manchor_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchor_strength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_word_mat' is not defined"
     ]
    }
   ],
   "source": [
    "# Anchor words\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "\n",
    "#socho始aa socho始an socho始uun  socho始uu  \n",
    "anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],\n",
    "                [\"balaa\",\"dhibee\",\"irraa\"],\n",
    "                [\"sochii\",\"lafaa\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=1500,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Corex' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a048da6ebd34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtopic_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0manchored_topic_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}: '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-420dac8b9291>\u001b[0m in \u001b[0;36mget_topics\u001b[1;34m(self, n_words, topic, print_words)\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mtopic_ns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;31m# Determine whether to return column word labels or indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m             \u001b[0mprint_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Corex' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "topic_classification=anchored_topic_model.labels[:,0:4] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\",\"topic4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data=raw_data.reset_index()\n",
    "result[\"doc_id\"]=raw_data[\"doc_id\"]\n",
    "result[\"true_class_label\"]=raw_data[\"class_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.loc[(result['topic1']==True) | (result['topic2']==True) |  (result['topic3']==True) |(result['topic4']==True) , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False) & (result['topic3']==False) & (result['topic4']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 7)\n",
      "shape after filtering unknown docs (1970, 7)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.true_class_label !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted++</th>\n",
       "      <th>Predicted--</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual++</th>\n",
       "      <td>248</td>\n",
       "      <td>1558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual--</th>\n",
       "      <td>27</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted++  Predicted--\n",
       "Actual++          248         1558\n",
       "Actual--           27          137"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.true_class_label, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9018181818181819"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13732004429678848"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2383469485824123"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20151002_H0040LX9Z</th>\n",
       "      <td>itoophiyaa keessatti laakkofsi namoota hongeef...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020537_20161206_H0040MR72</th>\n",
       "      <td>balaa hoongee naannolee shanitti uumameef deeg...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20170426_H0040MPVA</th>\n",
       "      <td>aanaalee godina guraagee balaan hoongee mudate...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020505_20150902_H0040MO8I</th>\n",
       "      <td>ijoo dubbii abo ummatoota beela irraa baraaruu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020535_20151124_H0040MR6K</th>\n",
       "      <td>diiniyaas alamuu laliisaa itiyoopiyaan maalif ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160205_H0040LX8T</th>\n",
       "      <td>ummata beelaye duʼa jalaa baraaruuf gargaarsa ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020412_20150929_H0040MR7V</th>\n",
       "      <td>hoongee fi gogiinsa oromiyaa keessaa lammiilee...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020533_20170318_H0040MO5O</th>\n",
       "      <td>sababa walitti buʼiinsa daangaa oromiyaa fi su...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020533_20170120_H0040MO5P</th>\n",
       "      <td>daangaa naannoo oromiyaa fi sumaalee irratti w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020548_20151216_H0040MR7K</th>\n",
       "      <td>birhanu m lenjiso lolli wayyaaneen oromorratti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_NW_020423_20151002_H0040LX9Z  itoophiyaa keessatti laakkofsi namoota hongeef...   \n",
       "IL6_WL_020537_20161206_H0040MR72  balaa hoongee naannolee shanitti uumameef deeg...   \n",
       "IL6_NW_020532_20170426_H0040MPVA  aanaalee godina guraagee balaan hoongee mudate...   \n",
       "IL6_NW_020505_20150902_H0040MO8I  ijoo dubbii abo ummatoota beela irraa baraaruu...   \n",
       "IL6_WL_020535_20151124_H0040MR6K  diiniyaas alamuu laliisaa itiyoopiyaan maalif ...   \n",
       "IL6_NW_020423_20160205_H0040LX8T  ummata beelaye duʼa jalaa baraaruuf gargaarsa ...   \n",
       "IL6_NW_020412_20150929_H0040MR7V  hoongee fi gogiinsa oromiyaa keessaa lammiilee...   \n",
       "IL6_NW_020533_20170318_H0040MO5O  sababa walitti buʼiinsa daangaa oromiyaa fi su...   \n",
       "IL6_NW_020533_20170120_H0040MO5P  daangaa naannoo oromiyaa fi sumaalee irratti w...   \n",
       "IL6_WL_020548_20151216_H0040MR7K  birhanu m lenjiso lolli wayyaaneen oromorratti...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_NW_020423_20151002_H0040LX9Z      7  \n",
       "IL6_WL_020537_20161206_H0040MR72      7  \n",
       "IL6_NW_020532_20170426_H0040MPVA      5  \n",
       "IL6_NW_020505_20150902_H0040MO8I      4  \n",
       "IL6_WL_020535_20151124_H0040MR6K      4  \n",
       "IL6_NW_020423_20160205_H0040LX8T      4  \n",
       "IL6_NW_020412_20150929_H0040MR7V      3  \n",
       "IL6_NW_020533_20170318_H0040MO5O      3  \n",
       "IL6_NW_020533_20170120_H0040MO5P      2  \n",
       "IL6_WL_020548_20151216_H0040MR7K      2  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 1\n",
    "\n",
    "\n",
    "topic1_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic1==True),\"doc_id\"]\n",
    "\n",
    "topic1_docs_id.shape\n",
    "\n",
    "topic1_docs =raw_data.loc[raw_data.doc_id.isin(topic1_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic1_docs= topic1_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],[\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic1_docs['count'] = topic1_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa').sum())\n",
    "\n",
    "topic1_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic1_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020445_20160426_H0040M4CT</th>\n",
       "      <td>balaa lolaa dirree dawaatti qaqqabeen namoonni...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160419_H0040LX80</th>\n",
       "      <td>biyyatti ajjeefamuu fi biyyoota ollaattii abdi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20160105_H0040MPV6</th>\n",
       "      <td>koomishinichi karoora balaa lolaa ittisuu fi l...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020423_20150814_H0040MR7C</th>\n",
       "      <td>haalli qilleensa el nino cimaan uumamuuf deema...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020445_20160507_H0040M4CF</th>\n",
       "      <td>ministeerichi balaa lolaa mudatu hirʼisuuf hoj...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20170426_H0040MPV3</th>\n",
       "      <td>rooba tibbanaan oromiyaatti namoonni kuma  taʼ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020427_20170210_H0040LYS7</th>\n",
       "      <td>mudannoon weerara maqaa liyyuu polisiin murnii...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020117_20160705_H0040LMLC</th>\n",
       "      <td>bokkaan kalee barraaqa adaamaatti roobe waan h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOD7</th>\n",
       "      <td>karoora mootummaan wayyaanee ergamtuu isaa opd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_NW_020445_20160426_H0040M4CT  balaa lolaa dirree dawaatti qaqqabeen namoonni...   \n",
       "IL6_NW_020423_20160419_H0040LX80  biyyatti ajjeefamuu fi biyyoota ollaattii abdi...   \n",
       "IL6_NW_020532_20160105_H0040MPV6  koomishinichi karoora balaa lolaa ittisuu fi l...   \n",
       "IL6_WL_020423_20150814_H0040MR7C  haalli qilleensa el nino cimaan uumamuuf deema...   \n",
       "IL6_NW_020445_20160507_H0040M4CF  ministeerichi balaa lolaa mudatu hirʼisuuf hoj...   \n",
       "IL6_NW_020532_20170426_H0040MPV3  rooba tibbanaan oromiyaatti namoonni kuma  taʼ...   \n",
       "IL6_NW_020427_20170210_H0040LYS7  mudannoon weerara maqaa liyyuu polisiin murnii...   \n",
       "IL6_NW_020117_20160705_H0040LMLC  bokkaan kalee barraaqa adaamaatti roobe waan h...   \n",
       "IL6_WL_020405_20170307_H0040LOD7  karoora mootummaan wayyaanee ergamtuu isaa opd...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020415_20160225_H0040LT3D     16  \n",
       "IL6_NW_020445_20160426_H0040M4CT      8  \n",
       "IL6_NW_020423_20160419_H0040LX80      7  \n",
       "IL6_NW_020532_20160105_H0040MPV6      7  \n",
       "IL6_WL_020423_20150814_H0040MR7C      6  \n",
       "IL6_NW_020445_20160507_H0040M4CF      6  \n",
       "IL6_NW_020532_20170426_H0040MPV3      6  \n",
       "IL6_NW_020427_20170210_H0040LYS7      5  \n",
       "IL6_NW_020117_20160705_H0040LMLC      5  \n",
       "IL6_WL_020405_20170307_H0040LOD7      4  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 2\n",
    "\n",
    "\n",
    "topic2_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic2==True),\"doc_id\"]\n",
    "\n",
    "topic2_docs_id.shape\n",
    "\n",
    "\n",
    "topic2_docs =raw_data.loc[raw_data.doc_id.isin(topic2_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic2_docs= topic2_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],[\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic2_docs['count'] = topic2_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'galaana|galaanaa|guutuu|lolaa').sum())\n",
    "\n",
    "topic2_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic2_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020539_20160403_H0040MR7Q</th>\n",
       "      <td>bishaantu rasaasa taʼe yooseef hambaa tolaa bi...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOE5</th>\n",
       "      <td>sabummaa mormaa sabummaa deggeruu sbo bitootes...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOFT</th>\n",
       "      <td>shira lukkeelee fi gooftoota isaani gaomee dhu...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20160105_H0040MPV6</th>\n",
       "      <td>koomishinichi karoora balaa lolaa ittisuu fi l...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020411_20170318_H0040LSJJ</th>\n",
       "      <td>sirni tpleprdf wayta ummata wayyaba gara lammi...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020412_20170427_H0040MR6F</th>\n",
       "      <td>ibsa ijjannoo barattoota oromoo yuunibarsiitii...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020117_20160420_H0040LMNX</th>\n",
       "      <td>balaa doonii godaantota  galaafaterraa namni l...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160419_H0040LX80</th>\n",
       "      <td>biyyatti ajjeefamuu fi biyyoota ollaattii abdi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020408_20160402_H0040M6A0</th>\n",
       "      <td>#oromoprotests daily april   sbo ebla   sbovol...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020539_20160403_H0040MR7Q  bishaantu rasaasa taʼe yooseef hambaa tolaa bi...   \n",
       "IL6_WL_020405_20170307_H0040LOE5  sabummaa mormaa sabummaa deggeruu sbo bitootes...   \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_WL_020405_20170307_H0040LOFT  shira lukkeelee fi gooftoota isaani gaomee dhu...   \n",
       "IL6_NW_020532_20160105_H0040MPV6  koomishinichi karoora balaa lolaa ittisuu fi l...   \n",
       "IL6_NW_020411_20170318_H0040LSJJ  sirni tpleprdf wayta ummata wayyaba gara lammi...   \n",
       "IL6_WL_020412_20170427_H0040MR6F  ibsa ijjannoo barattoota oromoo yuunibarsiitii...   \n",
       "IL6_NW_020117_20160420_H0040LMNX  balaa doonii godaantota  galaafaterraa namni l...   \n",
       "IL6_NW_020423_20160419_H0040LX80  biyyatti ajjeefamuu fi biyyoota ollaattii abdi...   \n",
       "IL6_NW_020408_20160402_H0040M6A0  #oromoprotests daily april   sbo ebla   sbovol...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020539_20160403_H0040MR7Q     22  \n",
       "IL6_WL_020405_20170307_H0040LOE5     18  \n",
       "IL6_WL_020415_20160225_H0040LT3D     16  \n",
       "IL6_WL_020405_20170307_H0040LOFT     16  \n",
       "IL6_NW_020532_20160105_H0040MPV6     13  \n",
       "IL6_NW_020411_20170318_H0040LSJJ     12  \n",
       "IL6_WL_020412_20170427_H0040MR6F     11  \n",
       "IL6_NW_020117_20160420_H0040LMNX     11  \n",
       "IL6_NW_020423_20160419_H0040LX80     11  \n",
       "IL6_NW_020408_20160402_H0040M6A0     10  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 3\n",
    "\n",
    "\n",
    "topic3_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic3==True),\"doc_id\"]\n",
    "\n",
    "topic3_docs_id.shape\n",
    "\n",
    "\n",
    "topic3_docs =raw_data.loc[raw_data.doc_id.isin(topic3_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic3_docs= topic3_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic3_docs['count'] = topic3_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'balaa|dhibee|irraa').sum())\n",
    "\n",
    "topic3_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic3_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOET</th>\n",
       "      <td>qeerroo diddaan barattoota oromoo bifa qindaay...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020525_20160831_H0040MNBX</th>\n",
       "      <td>#oromoprotests #dhaamsa qeerroo bilisummaa oro...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOD7</th>\n",
       "      <td>karoora mootummaan wayyaanee ergamtuu isaa opd...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOFT</th>\n",
       "      <td>shira lukkeelee fi gooftoota isaani gaomee dhu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOSG</th>\n",
       "      <td>wayyaaneen shira marsaa ffaa magaalaa amboo ir...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020520_20160716_H0040MMVK</th>\n",
       "      <td>injifannoo uummanni oromoo baatii saddet darba...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20150514_H0040LXAY</th>\n",
       "      <td>sochiin dachii oromiyaa eprdfopdo irratti mana...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020427_20150806_H0040LYRC</th>\n",
       "      <td>uummatni hidhaa fi reebichaan saree iyyuu ni x...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020525_20160218_H0040MNCV</th>\n",
       "      <td>godina arsii lixaatti magaalotaa fi baadiyaan ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_WL_020405_20170307_H0040LOET  qeerroo diddaan barattoota oromoo bifa qindaay...   \n",
       "IL6_WL_020525_20160831_H0040MNBX  #oromoprotests #dhaamsa qeerroo bilisummaa oro...   \n",
       "IL6_WL_020405_20170307_H0040LOD7  karoora mootummaan wayyaanee ergamtuu isaa opd...   \n",
       "IL6_WL_020405_20170307_H0040LOFT  shira lukkeelee fi gooftoota isaani gaomee dhu...   \n",
       "IL6_WL_020405_20170307_H0040LOSG  wayyaaneen shira marsaa ffaa magaalaa amboo ir...   \n",
       "IL6_WL_020520_20160716_H0040MMVK  injifannoo uummanni oromoo baatii saddet darba...   \n",
       "IL6_NW_020423_20150514_H0040LXAY  sochiin dachii oromiyaa eprdfopdo irratti mana...   \n",
       "IL6_NW_020427_20150806_H0040LYRC  uummatni hidhaa fi reebichaan saree iyyuu ni x...   \n",
       "IL6_WL_020525_20160218_H0040MNCV  godina arsii lixaatti magaalotaa fi baadiyaan ...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020415_20160225_H0040LT3D      7  \n",
       "IL6_WL_020405_20170307_H0040LOET      6  \n",
       "IL6_WL_020525_20160831_H0040MNBX      5  \n",
       "IL6_WL_020405_20170307_H0040LOD7      5  \n",
       "IL6_WL_020405_20170307_H0040LOFT      4  \n",
       "IL6_WL_020405_20170307_H0040LOSG      4  \n",
       "IL6_WL_020520_20160716_H0040MMVK      4  \n",
       "IL6_NW_020423_20150514_H0040LXAY      3  \n",
       "IL6_NW_020427_20150806_H0040LYRC      3  \n",
       "IL6_WL_020525_20160218_H0040MNCV      3  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 4\n",
    "\n",
    "\n",
    "topic4_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic4==True),\"doc_id\"]\n",
    "\n",
    "topic4_docs_id.shape\n",
    "\n",
    "\n",
    "topic4_docs =raw_data.loc[raw_data.doc_id.isin(topic4_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic4_docs= topic4_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic4_docs['count'] = topic4_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'sochii|lafaa').sum())\n",
    "\n",
    "topic4_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic4_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternative approach guided-LDA\n",
    "# reference links below\n",
    "#1) https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\n",
    "#2)https://github.com/vi3k6i5/GuidedLDA\n",
    "\n",
    "import numpy as np\n",
    "import guidedlda\n",
    "model = guidedlda.GuidedLDA(n_topics=25, n_iter=1500, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5871x8360 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 247375 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document-Term Matrix-(count matrix)\n",
    "vectorizer2 = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = False,lowercase=True)\n",
    "doc_word_mat2 = vectorizer2.fit_transform(raw_data.text_data)\n",
    "doc_word_mat2 = ss.csr_matrix(doc_word_mat2)\n",
    "doc_word_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words2 = list(np.asarray(vectorizer2.get_feature_names()))\n",
    "word2id = dict((v, idx) for idx, v in enumerate(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "\n",
    "seed_topic_list = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],\n",
    "                [\"balaa\",\"dhibee\",\"irraa\"],\n",
    "                [\"sochii\",\"lafaa\"]]\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 5871\n",
      "INFO:guidedlda:vocab_size: 8360\n",
      "INFO:guidedlda:n_words: 442803\n",
      "INFO:guidedlda:n_topics: 25\n",
      "INFO:guidedlda:n_iter: 1500\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "c:\\users\\user\\anaconda3\\lib\\site-packages\\guidedlda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -5295904\n",
      "INFO:guidedlda:<20> log likelihood: -3738637\n",
      "INFO:guidedlda:<40> log likelihood: -3625005\n",
      "INFO:guidedlda:<60> log likelihood: -3571601\n",
      "INFO:guidedlda:<80> log likelihood: -3544784\n",
      "INFO:guidedlda:<100> log likelihood: -3528939\n",
      "INFO:guidedlda:<120> log likelihood: -3518160\n",
      "INFO:guidedlda:<140> log likelihood: -3507952\n",
      "INFO:guidedlda:<160> log likelihood: -3503433\n",
      "INFO:guidedlda:<180> log likelihood: -3497244\n",
      "INFO:guidedlda:<200> log likelihood: -3493299\n",
      "INFO:guidedlda:<220> log likelihood: -3490710\n",
      "INFO:guidedlda:<240> log likelihood: -3488527\n",
      "INFO:guidedlda:<260> log likelihood: -3485648\n",
      "INFO:guidedlda:<280> log likelihood: -3482464\n",
      "INFO:guidedlda:<300> log likelihood: -3482612\n",
      "INFO:guidedlda:<320> log likelihood: -3481994\n",
      "INFO:guidedlda:<340> log likelihood: -3478770\n",
      "INFO:guidedlda:<360> log likelihood: -3477980\n",
      "INFO:guidedlda:<380> log likelihood: -3476858\n",
      "INFO:guidedlda:<400> log likelihood: -3476305\n",
      "INFO:guidedlda:<420> log likelihood: -3474892\n",
      "INFO:guidedlda:<440> log likelihood: -3473329\n",
      "INFO:guidedlda:<460> log likelihood: -3472098\n",
      "INFO:guidedlda:<480> log likelihood: -3473004\n",
      "INFO:guidedlda:<500> log likelihood: -3471395\n",
      "INFO:guidedlda:<520> log likelihood: -3470699\n",
      "INFO:guidedlda:<540> log likelihood: -3469978\n",
      "INFO:guidedlda:<560> log likelihood: -3468548\n",
      "INFO:guidedlda:<580> log likelihood: -3466309\n",
      "INFO:guidedlda:<600> log likelihood: -3467309\n",
      "INFO:guidedlda:<620> log likelihood: -3464731\n",
      "INFO:guidedlda:<640> log likelihood: -3461233\n",
      "INFO:guidedlda:<660> log likelihood: -3461813\n",
      "INFO:guidedlda:<680> log likelihood: -3460234\n",
      "INFO:guidedlda:<700> log likelihood: -3460575\n",
      "INFO:guidedlda:<720> log likelihood: -3459368\n",
      "INFO:guidedlda:<740> log likelihood: -3458649\n",
      "INFO:guidedlda:<760> log likelihood: -3456979\n",
      "INFO:guidedlda:<780> log likelihood: -3456387\n",
      "INFO:guidedlda:<800> log likelihood: -3457525\n",
      "INFO:guidedlda:<820> log likelihood: -3456615\n",
      "INFO:guidedlda:<840> log likelihood: -3455203\n",
      "INFO:guidedlda:<860> log likelihood: -3454941\n",
      "INFO:guidedlda:<880> log likelihood: -3454730\n",
      "INFO:guidedlda:<900> log likelihood: -3453846\n",
      "INFO:guidedlda:<920> log likelihood: -3455197\n",
      "INFO:guidedlda:<940> log likelihood: -3452683\n",
      "INFO:guidedlda:<960> log likelihood: -3451933\n",
      "INFO:guidedlda:<980> log likelihood: -3449595\n",
      "INFO:guidedlda:<1000> log likelihood: -3451020\n",
      "INFO:guidedlda:<1020> log likelihood: -3451234\n",
      "INFO:guidedlda:<1040> log likelihood: -3449721\n",
      "INFO:guidedlda:<1060> log likelihood: -3448704\n",
      "INFO:guidedlda:<1080> log likelihood: -3447457\n",
      "INFO:guidedlda:<1100> log likelihood: -3448027\n",
      "INFO:guidedlda:<1120> log likelihood: -3447045\n",
      "INFO:guidedlda:<1140> log likelihood: -3447686\n",
      "INFO:guidedlda:<1160> log likelihood: -3445872\n",
      "INFO:guidedlda:<1180> log likelihood: -3445984\n",
      "INFO:guidedlda:<1200> log likelihood: -3446254\n",
      "INFO:guidedlda:<1220> log likelihood: -3446955\n",
      "INFO:guidedlda:<1240> log likelihood: -3447986\n",
      "INFO:guidedlda:<1260> log likelihood: -3446345\n",
      "INFO:guidedlda:<1280> log likelihood: -3444909\n",
      "INFO:guidedlda:<1300> log likelihood: -3446063\n",
      "INFO:guidedlda:<1320> log likelihood: -3446989\n",
      "INFO:guidedlda:<1340> log likelihood: -3444449\n",
      "INFO:guidedlda:<1360> log likelihood: -3444130\n",
      "INFO:guidedlda:<1380> log likelihood: -3444084\n",
      "INFO:guidedlda:<1400> log likelihood: -3443933\n",
      "INFO:guidedlda:<1420> log likelihood: -3441809\n",
      "INFO:guidedlda:<1440> log likelihood: -3442142\n",
      "INFO:guidedlda:<1460> log likelihood: -3441182\n",
      "INFO:guidedlda:<1480> log likelihood: -3440274\n",
      "INFO:guidedlda:<1499> log likelihood: -3441239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x1c8a89f0518>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: nu na koo keenya ani kan akka si hin kee\n",
      "Topic 1: afaan oromoo gadaa keessatti aadaa sirna bara ilmaan ture itti\n",
      "Topic 2: nu kan kana fi hin akka waan keenya of irratti\n",
      "Topic 3: fi oromoo qabsoo bilisummaa abo ummata irratti jiru akka isaa\n",
      "Topic 4: fi akka nama keessaa hin ka itoophiyaa biyya irratti hiriira\n",
      "Topic 5: keessatti godina itti aanaa magaalaa fxg jira fi wallaggaa oromoo\n",
      "Topic 6: kan fi oromoo ture irraa akka kun yeroo keessa kana\n",
      "Topic 7: akka fi kan finfinnee obbo naannoo oromiyaa hojii irratti itti\n",
      "Topic 8: fi oromoo jiru wayyaanee ummata hin isaa wayyaaneen kan ilmaan\n",
      "Topic 9: mana kan akka hidhaa isaanii obbo fi oromoo hin isaa\n",
      "Topic 10: mana aku yg nak yang ada ni tak di tu\n",
      "Topic 11: hin kan fi akka oromoo dha malee utuu tokko tahu\n",
      "Topic 12: fi oromoo uummata wayyaanee irraa irratti kan hin oromiyaa qeerroo\n",
      "Topic 13: oromo oromoo fi ethiopia qeerroo sbo bilisummaa oduu sagalee qophii\n",
      "Topic 14: fi hin kan akka irraa oromoo irratti ummata isaa ummatni\n",
      "Topic 15: fi akka mootummaan jira jiru itoophiyaa balaa keessatti addunyaa bishaan\n",
      "Topic 16: ganda ang ng mo na ko sa ni ka talaga\n",
      "Topic 17: fii isaa qabsoo kan akka irraa oromoo biyya itti tan\n",
      "Topic 18: fi code jedhan ka hin akka irratti mp kbps isaanii\n",
      "Topic 19: hin yoo fi waan malee kana kan tokko nama isa\n",
      "Topic 20: fi godina oromorevolution wayyaanee waraana irratti kan bahaa waraanaa harargee\n",
      "Topic 21: magaalaa fi jiru keessatti mormii oromiyaa jira akka kan hiriira\n",
      "Topic 22: kan fi kuni irratti ummata isaa ummanni ture yeroo keessatti\n",
      "Topic 23: akka kan kana irratti kun yeroo waan itti hin keessatti\n",
      "Topic 24: mana the to que de in and of ganda eu\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(words2)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
