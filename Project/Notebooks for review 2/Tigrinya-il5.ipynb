{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CorEx Hierarchical Topic Models\n",
    "Use the principle of Total Cor-relation Explanation (CorEx) to construct\n",
    "hierarchical topic models. This module is specially designed for sparse count\n",
    "data and implements semi-supervision via the information bottleneck.\n",
    "Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical\n",
    "Representations of High-Dimensional Data.\" AISTATS, 2015.\n",
    "Gallagher et al. \"Anchored Correlation Explanation: Topic Modeling with Minimal\n",
    "Domain Knowledge.\" TACL, 2017.\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu)\n",
    "Ryan J. Gallagher\n",
    "David Kale\n",
    "Lily Fierro\n",
    "License: Apache V2\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "#import corex_topic as ct\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data which is in .txt file line by line, split by tab(\\t) to a list\n",
    "list_data=[]\n",
    "with open('C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\il5_uromo.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the columns of the datframe\n",
    "raw_data = pd.DataFrame(list_data,columns=['doc_id','text_data','class_type']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip columns for leading and trailing white spaces\n",
    "raw_data['doc_id']=raw_data.doc_id.str.strip()\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 3)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n docs x m attributes\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set doc_id as index of the datframe\n",
    "raw_data= raw_data.set_index('doc_id')\n",
    "\n",
    "# change \"class_type\" column to \"categorical\" datatype\n",
    "raw_data['class_type'] = raw_data['class_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_incident - This should be considered as part of “indomain”\n",
    "\n",
    "raw_data.loc[raw_data['class_type']==\"eval_incident\", 'class_type'] = \"indomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 2)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 2)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RT (Retweet) keyword removal\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('RT','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 2)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punctuation removal on 'text_data' column\n",
    "#print(string.punctuation)\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "#print(punct)\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: regex.sub('', x))\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 2)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove @names mentioned as part of tweets\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('\\@[a-zA-Z0-9]+','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word count (length) of the document\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by document length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji's from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Removal/ number removal from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\\d]+','',x))\n",
    "\n",
    "# strip whitespaces again \n",
    "\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "\n",
    "# calculate length again\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4894, 3)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents with no words after pre-processing\n",
    "raw_data = raw_data.loc[raw_data.length>0]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4573, 3)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing documents with less than 5 words (very short documents)\n",
    "raw_data = raw_data.loc[raw_data.length>5]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk          3126\n",
      "nondomain     821\n",
      "indomain      626\n",
      "Name: class_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28800f3aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# document count across 4 class labels (unk,nondomain,indomain,eval_incident)\n",
    "\n",
    "print(raw_data.class_type.value_counts())\n",
    "plt.hist('class_type',data=raw_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the formatted data to a new text file for future reference\n",
    "reset_data=raw_data.reset_index()\n",
    "reset_data.to_csv('C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\Tigrinya.txt', header=True, index=False, sep='\\t', mode='w',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indomain</th>\n",
       "      <td>626.0</td>\n",
       "      <td>78.095847</td>\n",
       "      <td>129.818952</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>121.75</td>\n",
       "      <td>1286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nondomain</th>\n",
       "      <td>821.0</td>\n",
       "      <td>35.053593</td>\n",
       "      <td>91.601947</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.00</td>\n",
       "      <td>932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unk</th>\n",
       "      <td>3126.0</td>\n",
       "      <td>87.422265</td>\n",
       "      <td>250.076025</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>55.00</td>\n",
       "      <td>4900.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length                                                        \n",
       "             count       mean         std  min   25%   50%     75%     max\n",
       "class_type                                                                \n",
       "indomain     626.0  78.095847  129.818952  6.0  16.0  23.0  121.75  1286.0\n",
       "nondomain    821.0  35.053593   91.601947  6.0  15.0  21.0   24.00   932.0\n",
       "unk         3126.0  87.422265  250.076025  6.0  13.0  20.0   55.00  4900.0"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic stats to understand document length across 3 class labels\n",
    "raw_data.groupby('class_type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Term matrix (binary matrix) with max_df =0.995, min_df =0.001\n",
    "\n",
    "vectorizer = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=False)\n",
    "doc_word_mat = vectorizer.fit_transform(raw_data.text_data)\n",
    "doc_word_mat = ss.csr_matrix(doc_word_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4573, 6737)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 25 topics\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=1500, verbose=False, seed=3192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x2880010e3c8>"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ከምቲ,እንታይ,እንተ,ሓቂ,ዓለም,እሞ,ይግባእ,የብሉን,ነታ,ታሪኽ\n",
      "1: እቲ,ናይ,ናብ,ሓበሬታ,ነቲ,እዋን,ምስ,ኣብቲ,እዩ,ኣብዚ\n",
      "2: ሕጂ,ንሱ,መን,ሓንቲ,ዓቢ,ስለዝኾነ,ግድን,እየ,ጽቡቕ,ገና\n",
      "3: ተኻኢሉ,ስለ,ንምፍላጥ,ገዛኢ,ኢህወዴግ,ሕዚ,ተገሊፁ,ዓም,ወሲኹ,ምግላፅ\n",
      "4: ውን,ወይ,ኣይኮነን,ሃገራት,ጥራይ,ነይሩ,ስለዚ,ገለ,ማለት,ከምዘሎ\n",
      "5: ስርዓት,ህዝቢ,ዝኾነ,ነዚ,ሰብ,ሃገርና,ምኽንያት,ቃልሲ,ከምኡ,ፖለቲካዊ\n",
      "6: ኤርትራ,ኤርትራውያን,ህግደፍ,ከኣ,ሃገራዊ,ናጽነት,ለውጢ,ግንባር,ትሕቲ,መንእሰያት\n",
      "7: ድምጺ,መራገፊ,kbps,መጻወቲ,ምሉእ,ምስማዕ,ትሕዝቶ,ይክኣል,ይብል,ኣቶ\n",
      "8: ኩሉ,ዝብል,ዘለዎ,ኣብነት,ደቂ,ልዑል,ምበር,ተስፋ,ላዕሊ,ተራ\n",
      "9: ዘለዉ,ብዘይ,ካልእ,እኳ,ካብቶም,ክንዲ,ዘይኮነ,ዓዲ,ስደት,ምስቲ\n",
      "10: እዚ,እቶም,ግን,ካብቲ,እንተኾነ,በቲ,ቅድሚ,እዮም,ግዜ,ከባቢ\n",
      "11: ኣብ,ካብ,ልዕሊ,Ethiopia,ዓመት,Eritrea,ሎሚ,ኩነታት,ዓመታት,ሰብኣዊ\n",
      "12: ናይቲ,ይኹን,እምበር,ዘይብሉ,ነቶም,ሕጊ,ኢልካ,ህይወት,ሓሳብ,ገበን\n",
      "13: ኢና,ኢሉ,ከምዚ,ንህዝቢ,የለን,ኮይና,ወዲ,ዘለዋ,ደው,እየን\n",
      "14: ዘሎ,ኣሎ,ክብል,ውሽጢ,ስራሕ,መወዳእታ,ናይዚ,ኣዝዩ,ኣለና,ካልኣይ\n",
      "15: ላይ,ነው,እና,የኢትዮጵያ,ጋር,ውስጥ,ቀን,አበባ,ወደ,ህዝብ\n",
      "16: እውን,ህዝብን,ጥራሕ,ገሊፆም,ዘይኮነስ,መሬት,ባዕሉ,ምሕደራ,ማይ,ረብሓ\n",
      "17: ስደተኛታት,ባሕሪ,ማእከላይ,ሊብያ,ቁጽሪ,ገማግም,ኣስታት,ትካል,ኢጣልያ,ዋሕዚ\n",
      "18: ኢሎም,ኣለው,ደቡብ,ሓደጋ,ሱዳን,ተባሂሉ,መራሕቲ,ሓገዝ,ኣብታ,ትካላት\n",
      "19: ከም,ብምባል,በዚ,እዞም,ብዙሕ,ምህላዎም,ምንቅስቓስ,ብናይ,ብተደጋጋሚ,ሓያል\n",
      "20: ድማ,ሓደ,ዝሓለፈ,ካልኦት,ወርሒ,ክልተ,ሰሙን,ሰለስተ,ዝተኻየደ,ኣዋርሕ\n",
      "21: ክልል,ሰባት,ዞባ,ኢትዮጵያ,ተቓውሞ,ኦሮምያ,ነበርቲ,ምዕራብ,ትግራይ,ወረዳ\n",
      "22: ብፍላይ,ብርክት,ብዝብል,ዝበሉ,ገንዘብ,ዜጋ,ካብዚ,ሽግር,ከባብታት,ዝነብር\n",
      "23: ምዃኑ,ስልጣን,ቀንዲ,ሰበ,ዝግበር,ገበናት,ፍትሕን,ኣቦ,ከምኡውን,ሰነ\n",
      "24: ኣሜሪካ,ምርጫ,ኦባማ,ጉዳያት,ፕረዝደንት,ትራምፕ,ኣመሪካ,ዶናልድ,ፕረዚደንት,ዋዕላ\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e-06   1.00000000e-06   1.00000000e-06 ...,   8.49395904e-04\n",
      "    7.96715659e-04   4.14287663e-03]\n",
      " [  1.00000000e-06   1.00000000e-06   1.00000000e-06 ...,   8.50571668e-04\n",
      "    7.98696396e-04   4.15492562e-03]\n",
      " [  1.00000000e-06   1.00000000e-06   1.00000000e-06 ...,   8.49906065e-04\n",
      "    7.97328351e-04   4.14144352e-03]\n",
      " ..., \n",
      " [  9.99999000e-01   9.99999000e-01   9.99999000e-01 ...,   9.99999000e-01\n",
      "    9.99999000e-01   9.99999000e-01]\n",
      " [  9.99999000e-01   9.99999000e-01   9.99999000e-01 ...,   9.99999000e-01\n",
      "    9.99999000e-01   9.99999000e-01]\n",
      " [  9.99999000e-01   9.99999000e-01   9.99999000e-01 ...,   9.99999000e-01\n",
      "    9.99999000e-01   9.99999000e-01]]\n"
     ]
    }
   ],
   "source": [
    "#CorEx estimates the probability a document belongs to a topic given that document's words.\n",
    "\n",
    "#The estimated probabilities of topics for each document can be accessed through p_y_given_x.\n",
    "print(topic_model.p_y_given_x) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " [False False False ..., False False False]\n",
      " ..., \n",
      " [ True  True  True ...,  True  True  True]\n",
      " [ True  True  True ...,  True  True  True]\n",
      " [ True  True  True ...,  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "print(topic_model.labels) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Total Correlation (nats)')"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2887d6b0780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model.tcs.shape[0]), topic_model.tcs, color = '#4e79a7', width = 0.5)\n",
    "plt.xlabel('Topic', fontsize = 16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "#Tigrinya (script is left-to-right):\n",
    "#earthquake: ~ምንቅጥቃጥ መሬት   (Romanized: meneqeteqaate mareete)\n",
    "\n",
    "#drought: ~ነቕጺ            (Romanized: naqhetsi)\n",
    "#flood: ~ዕልቕልቕ            (Romanized: eleqheleqhe)\n",
    "#disaster: መዓት; ~መቕዘፍቲ    (Romanized: maaate; maqhezafeti)\n",
    " \n",
    "anchor_words = [[\"መሬት\"],\n",
    "                [\"መዓት\"]]\n",
    "\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25,max_iter=1500, seed=3192)   \n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: መሬት,እቲ,ከም,ድማ,ግን,ኮይኑ,ሓደ,ሰብ,ኢሉ,ዘለዎ\n",
      "1: ከምቲ,ሕጂ,እንተ,ነታ,ሓንቲ,የብሉን,ዓቢ,ወዲ,መዓት,ዓዲ\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.76272005e-06   1.00000000e-06   8.67954266e-05 ...,   1.00000000e-06\n",
      "    1.00142421e-03   1.26709157e-05]\n",
      " [  4.76389264e-06   1.00000000e-06   8.68176894e-05 ...,   1.00000000e-06\n",
      "    9.63248350e-04   1.26706228e-05]\n",
      " [  4.76389286e-06   1.00000000e-06   8.68110080e-05 ...,   1.00000000e-06\n",
      "    1.00141812e-03   1.26705210e-05]\n",
      " ..., \n",
      " [  9.99999000e-01   9.99999000e-01   9.99999000e-01 ...,   9.99999000e-01\n",
      "    9.99999000e-01   9.99999000e-01]\n",
      " [  9.99999000e-01   9.99999000e-01   9.99999000e-01 ...,   9.99999000e-01\n",
      "    9.99999000e-01   9.99999000e-01]\n",
      " [  9.99999000e-01   9.99999000e-01   9.99999000e-01 ...,   9.99999000e-01\n",
      "    9.99999000e-01   9.99999000e-01]]\n"
     ]
    }
   ],
   "source": [
    "#CorEx estimates the probability a document belongs to a topic given that document's words.\n",
    "\n",
    "print(anchored_topic_model.p_y_given_x) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "topic_classification=anchored_topic_model.labels[:,0:2] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.reset_index()\n",
    "result[\"doc_id\"]=raw_data[\"doc_id\"]\n",
    "result[\"true_class_label\"]=raw_data[\"class_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[(result['topic1']==True) | (result['topic2']==True)  , 'predicted_class_label'] = \"indomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.loc[(result['topic1']==False) & (result['topic2']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (4573, 5)\n",
      "shape after filtering unknown docs (1447, 5)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.true_class_label !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 5)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.loc[(result['true_class_label']==\"indomain\") & (result['predicted_class_label']==\"indomain\")].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted++</th>\n",
       "      <th>Predicted--</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual++</th>\n",
       "      <td>157</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual--</th>\n",
       "      <td>48</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted++  Predicted--\n",
       "Actual++          157          469\n",
       "Actual--           48          773"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.true_class_label, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76585365853658538"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25079872204472842"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37785800240673889"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020508_20151218_H0040MWHT</th>\n",
       "      <td>መንግስቲ ኢትዮጵያ ፣ ኣብ ኣዲስ ኣበባን ከባቢኣን ዝወጠኖ ናይ መሬት ልም...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20160205_H0040ME5F</th>\n",
       "      <td>ኣብ ክልል ጋምቤላ ኣብዚ ሕዚ እዋን ብዝተልዓለ ግጭትን ዕግርግርን ምኽንያ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020060_20160825_G0040KICM</th>\n",
       "      <td>ኣትለት ፈይሳ ለሊሳ ንመግለጺ መንግስቲ ብምእማን ከይምለስ ስድራቤቱ ኣጠን...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20161223_H0040MDS6</th>\n",
       "      <td>ኣብ ጃዊን ከባቢኣን ኣብ ልዕሊ ገዛኢ ስርዓት ኢህወደግ ሓያል ተቃውሞ ህዝ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020508_20160202_H0040MWHU</th>\n",
       "      <td>ኣብ ኢትዮጵያ ፣ ክልል ጋምቤላ ኣብ ዝተኻየደ ቀቢላዊ ግጭት  ሰባት ከምዝ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20160326_H0040ME1K</th>\n",
       "      <td>ኣብ ዞባ ምዕራብ ዝርከቡ ኣመሓደርቲ ብዝፈፀምዎ ግዑዝይ ኣሰራርሓ ፣ ህዝቢ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20170210_H0T0036X1</th>\n",
       "      <td>ህወሓትን ሕቶ መሬትን ህዝቢ ትግራይ ንቓልሲ ክላዓዓልን ኣንፃር ገዛእቲ ደ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20160106_G0T000A4D</th>\n",
       "      <td>ሰሜን ኮርያ ፡ ሃይትሮጅን ቦምብ ፈቲና ሎሚ ኣብ ሰዓታት ቅድሚ ቀትሪ ምን...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20160609_G0040IC0X</th>\n",
       "      <td>ምጽራይ ሕሃ ገበናት ኣንጻር ሰብኣውነት ኣብ ኤርትራ ረኺቡ ጀኔቫ  ሰ ነ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020060_20160628_G0040EYEG</th>\n",
       "      <td>ሓፈሻዊ ምህርቲ ሕርሻ ኢትዮጵያ ከምዘይንኪ ሚንስቴር ሕርሻን ሃፍቲ ተፈጥሮ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL5_NW_020508_20151218_H0040MWHT  መንግስቲ ኢትዮጵያ ፣ ኣብ ኣዲስ ኣበባን ከባቢኣን ዝወጠኖ ናይ መሬት ልም...   \n",
       "IL5_WL_020506_20160205_H0040ME5F  ኣብ ክልል ጋምቤላ ኣብዚ ሕዚ እዋን ብዝተልዓለ ግጭትን ዕግርግርን ምኽንያ...   \n",
       "IL5_NW_020060_20160825_G0040KICM  ኣትለት ፈይሳ ለሊሳ ንመግለጺ መንግስቲ ብምእማን ከይምለስ ስድራቤቱ ኣጠን...   \n",
       "IL5_WL_020506_20161223_H0040MDS6  ኣብ ጃዊን ከባቢኣን ኣብ ልዕሊ ገዛኢ ስርዓት ኢህወደግ ሓያል ተቃውሞ ህዝ...   \n",
       "IL5_NW_020508_20160202_H0040MWHU  ኣብ ኢትዮጵያ ፣ ክልል ጋምቤላ ኣብ ዝተኻየደ ቀቢላዊ ግጭት  ሰባት ከምዝ...   \n",
       "IL5_WL_020506_20160326_H0040ME1K  ኣብ ዞባ ምዕራብ ዝርከቡ ኣመሓደርቲ ብዝፈፀምዎ ግዑዝይ ኣሰራርሓ ፣ ህዝቢ...   \n",
       "IL5_SN_000370_20170210_H0T0036X1  ህወሓትን ሕቶ መሬትን ህዝቢ ትግራይ ንቓልሲ ክላዓዓልን ኣንፃር ገዛእቲ ደ...   \n",
       "IL5_SN_000370_20160106_G0T000A4D  ሰሜን ኮርያ ፡ ሃይትሮጅን ቦምብ ፈቲና ሎሚ ኣብ ሰዓታት ቅድሚ ቀትሪ ምን...   \n",
       "IL5_NW_020079_20160609_G0040IC0X  ምጽራይ ሕሃ ገበናት ኣንጻር ሰብኣውነት ኣብ ኤርትራ ረኺቡ ጀኔቫ  ሰ ነ ...   \n",
       "IL5_NW_020060_20160628_G0040EYEG  ሓፈሻዊ ምህርቲ ሕርሻ ኢትዮጵያ ከምዘይንኪ ሚንስቴር ሕርሻን ሃፍቲ ተፈጥሮ...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL5_NW_020508_20151218_H0040MWHT      5  \n",
       "IL5_WL_020506_20160205_H0040ME5F      5  \n",
       "IL5_NW_020060_20160825_G0040KICM      2  \n",
       "IL5_WL_020506_20161223_H0040MDS6      2  \n",
       "IL5_NW_020508_20160202_H0040MWHU      2  \n",
       "IL5_WL_020506_20160326_H0040ME1K      2  \n",
       "IL5_SN_000370_20170210_H0T0036X1      2  \n",
       "IL5_SN_000370_20160106_G0T000A4D      2  \n",
       "IL5_NW_020079_20160609_G0040IC0X      1  \n",
       "IL5_NW_020060_20160628_G0040EYEG      1  "
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 1\n",
    "\n",
    "\n",
    "topic1_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic1==True),\"doc_id\"]\n",
    "\n",
    "topic1_docs_id.shape\n",
    "\n",
    "topic1_docs =raw_data.loc[raw_data.doc_id.isin(topic1_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic1_docs= topic1_docs.set_index('doc_id')\n",
    "\n",
    " \n",
    "#anchor_words = [[\"መሬት\"],[\"መዓት\"]]\n",
    "\n",
    "topic1_docs['count'] = topic1_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መሬት').sum())\n",
    "\n",
    "topic1_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic1_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20170203_H0T0036QB</th>\n",
       "      <td>መስጊድ ። ታሪኻዊት ከተማ ናቕፋ ። መዓት መዳፍዕን ፣ ተወንጨፍትን ፣ ደ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020490_20160328_H0040MCA6</th>\n",
       "      <td>ሓለዋ ባሕሪ ሊብያ ፥ ሬሳ ናይ ካብ ኤርትራን ኒጀርን ኢዮም ዝበሎም ዓሰር...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20150611_G0040EZAF</th>\n",
       "      <td>ሓርነትን ጭቆናን ኣማኑኤል ሳህለ መብዛሕትኡ ናይ መውጽእ ሓራ ውድብ ወይ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20151124_G0040IC2Q</th>\n",
       "      <td>እዋናዊ ሃለዋት ኤርትራ ዕለት  ሕዳር  ዓም ይ ሓድሽ ሓበሬታ ንእዋናዊ ሃ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20160610_G0040IC0W</th>\n",
       "      <td>ኣገዳሲ መግለጺ ውህደት ደሞክራሲያዊት ኤርትራ ኣብ ኢጣልያ ዕለት  ኣገዳሲ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020490_20160523_H0040MC9X</th>\n",
       "      <td>ንሰብኣዊ መሰላት ዝጣበቕ ጉጅለ ግፍዕታት ዳዒሽ ኣብ ሊብያ ሰኒዱ ሓደ ፍሉ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20160609_G0040IC0X</th>\n",
       "      <td>ምጽራይ ሕሃ ገበናት ኣንጻር ሰብኣውነት ኣብ ኤርትራ ረኺቡ ጀኔቫ  ሰ ነ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20160908_G0040KJ4B</th>\n",
       "      <td>እዋናዊ ዜናታት – ጀርመን ንቅሉዕ ጥሕሰት ሰብኣዊ መሰላት ምልካዊ ስርዓት...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20150613_G0040EZAB</th>\n",
       "      <td>ስርዓት ህግደፍ ካብ ህዝቢ ኣብ ውሽጢን ወጻእን ዝወርዶ ዘሎ ተጽዕኖ መኣዝ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20160904_G0040KJ4K</th>\n",
       "      <td>ዕድመ ተሳታፍነት ናብ ህዝባዊ ሰሚናር ደንቨር ኮሎራዶ ዛዕባ ሰሚናር ፡ ሓ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL5_SN_000370_20170203_H0T0036QB  መስጊድ ። ታሪኻዊት ከተማ ናቕፋ ። መዓት መዳፍዕን ፣ ተወንጨፍትን ፣ ደ...   \n",
       "IL5_NW_020490_20160328_H0040MCA6  ሓለዋ ባሕሪ ሊብያ ፥ ሬሳ ናይ ካብ ኤርትራን ኒጀርን ኢዮም ዝበሎም ዓሰር...   \n",
       "IL5_NW_020062_20150611_G0040EZAF  ሓርነትን ጭቆናን ኣማኑኤል ሳህለ መብዛሕትኡ ናይ መውጽእ ሓራ ውድብ ወይ ...   \n",
       "IL5_NW_020079_20151124_G0040IC2Q  እዋናዊ ሃለዋት ኤርትራ ዕለት  ሕዳር  ዓም ይ ሓድሽ ሓበሬታ ንእዋናዊ ሃ...   \n",
       "IL5_NW_020079_20160610_G0040IC0W  ኣገዳሲ መግለጺ ውህደት ደሞክራሲያዊት ኤርትራ ኣብ ኢጣልያ ዕለት  ኣገዳሲ...   \n",
       "IL5_NW_020490_20160523_H0040MC9X  ንሰብኣዊ መሰላት ዝጣበቕ ጉጅለ ግፍዕታት ዳዒሽ ኣብ ሊብያ ሰኒዱ ሓደ ፍሉ...   \n",
       "IL5_NW_020079_20160609_G0040IC0X  ምጽራይ ሕሃ ገበናት ኣንጻር ሰብኣውነት ኣብ ኤርትራ ረኺቡ ጀኔቫ  ሰ ነ ...   \n",
       "IL5_NW_020062_20160908_G0040KJ4B  እዋናዊ ዜናታት – ጀርመን ንቅሉዕ ጥሕሰት ሰብኣዊ መሰላት ምልካዊ ስርዓት...   \n",
       "IL5_NW_020062_20150613_G0040EZAB  ስርዓት ህግደፍ ካብ ህዝቢ ኣብ ውሽጢን ወጻእን ዝወርዶ ዘሎ ተጽዕኖ መኣዝ...   \n",
       "IL5_NW_020062_20160904_G0040KJ4K  ዕድመ ተሳታፍነት ናብ ህዝባዊ ሰሚናር ደንቨር ኮሎራዶ ዛዕባ ሰሚናር ፡ ሓ...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL5_SN_000370_20170203_H0T0036QB      1  \n",
       "IL5_NW_020490_20160328_H0040MCA6      0  \n",
       "IL5_NW_020062_20150611_G0040EZAF      0  \n",
       "IL5_NW_020079_20151124_G0040IC2Q      0  \n",
       "IL5_NW_020079_20160610_G0040IC0W      0  \n",
       "IL5_NW_020490_20160523_H0040MC9X      0  \n",
       "IL5_NW_020079_20160609_G0040IC0X      0  \n",
       "IL5_NW_020062_20160908_G0040KJ4B      0  \n",
       "IL5_NW_020062_20150613_G0040EZAB      0  \n",
       "IL5_NW_020062_20160904_G0040KJ4K      0  "
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 2\n",
    "\n",
    "\n",
    "topic2_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic2==True),\"doc_id\"]\n",
    "\n",
    "topic2_docs_id.shape\n",
    "\n",
    "topic2_docs =raw_data.loc[raw_data.doc_id.isin(topic2_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic2_docs= topic2_docs.set_index('doc_id')\n",
    "\n",
    " \n",
    "#anchor_words = [[\"መሬት\"],[\"መዓት\"]]\n",
    "\n",
    "topic2_docs['count'] = topic2_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መዓት').sum())\n",
    "\n",
    "topic2_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic2_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach -- guided-LDA\n",
    "# reference links below\n",
    "#1) https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\n",
    "#2)https://github.com/vi3k6i5/GuidedLDA\n",
    "\n",
    "import numpy as np\n",
    "import guidedlda\n",
    "model = guidedlda.GuidedLDA(n_topics=25, n_iter=500, random_state=3192, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4573x6737 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 141253 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document-Term Matrix-(count matrix)\n",
    "vectorizer2 = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = False,lowercase=False)\n",
    "doc_word_mat2 = vectorizer2.fit_transform(raw_data.text_data)\n",
    "doc_word_mat2 = ss.csr_matrix(doc_word_mat2)\n",
    "doc_word_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words2 = list(np.asarray(vectorizer2.get_feature_names()))\n",
    "word2id = dict((v, idx) for idx, v in enumerate(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "seed_topic_list =[[\"መሬት\"],[\"መዓት\"]]\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 4573\n",
      "INFO:guidedlda:vocab_size: 6737\n",
      "INFO:guidedlda:n_words: 223793\n",
      "INFO:guidedlda:n_topics: 25\n",
      "INFO:guidedlda:n_iter: 500\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -2831412\n",
      "INFO:guidedlda:<20> log likelihood: -1856304\n",
      "INFO:guidedlda:<40> log likelihood: -1796577\n",
      "INFO:guidedlda:<60> log likelihood: -1769756\n",
      "INFO:guidedlda:<80> log likelihood: -1755224\n",
      "INFO:guidedlda:<100> log likelihood: -1746401\n",
      "INFO:guidedlda:<120> log likelihood: -1738963\n",
      "INFO:guidedlda:<140> log likelihood: -1733749\n",
      "INFO:guidedlda:<160> log likelihood: -1728650\n",
      "INFO:guidedlda:<180> log likelihood: -1726219\n",
      "INFO:guidedlda:<200> log likelihood: -1724189\n",
      "INFO:guidedlda:<220> log likelihood: -1722245\n",
      "INFO:guidedlda:<240> log likelihood: -1720728\n",
      "INFO:guidedlda:<260> log likelihood: -1720140\n",
      "INFO:guidedlda:<280> log likelihood: -1717706\n",
      "INFO:guidedlda:<300> log likelihood: -1717837\n",
      "INFO:guidedlda:<320> log likelihood: -1715873\n",
      "INFO:guidedlda:<340> log likelihood: -1714592\n",
      "INFO:guidedlda:<360> log likelihood: -1714202\n",
      "INFO:guidedlda:<380> log likelihood: -1714545\n",
      "INFO:guidedlda:<400> log likelihood: -1713448\n",
      "INFO:guidedlda:<420> log likelihood: -1712696\n",
      "INFO:guidedlda:<440> log likelihood: -1713304\n",
      "INFO:guidedlda:<460> log likelihood: -1712584\n",
      "INFO:guidedlda:<480> log likelihood: -1712020\n",
      "INFO:guidedlda:<499> log likelihood: -1711924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x2887d68ad30>"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: the of in to Eritrea and Ethiopia The is on Ethiopian by Eritrean has for from Tigray via are that\n",
      "Topic 1: ናይ እቲ ድማ ከም ወይ ነቲ ግን ሰብ ስለ ማለት ህዝቢ ነዚ እንተ ዝኾነ ጥራይ ምስ ኣብቲ ኩሉ ሓደ ናይቲ\n",
      "Topic 2: ከም ኣብ እቲ ሓበሬታ ናይ ከተማ ድማ ስርዓት ካብ ህዝቢ ዘሎ ተኻኢሉ ተፈሊጡ ንምፍላጥ ከባቢ ዝርከቡ ናብ ዓም ተገሊፁ ነቲ\n",
      "Topic 3: ኣብ ህዝቢ ስርዓት ህግደፍ ካብ ሃገር ቃልሲ ዘሎ ድማ ናይ ግን እቲ ህዝባዊ እዚ ዓመት ህዝብና ዓመታት ሓደ ስለ ሎሚ\n",
      "Topic 4: ኣብ ኢትዮጵያ ካብ እቲ ሓገዝ ማይ ብሰንኪ ከተማ ሓደጋ ጥዕና ድርቂ ሚልዮን ደርቂ ዘሎ እውን መቐለ ሕክምና ሰብ እዋን ትካል\n",
      "Topic 5: ኣብ ናይ ካብ እቲ ናብ ስርዓት ኤርትራ ዘሎ ኤርትራውያን ሓበሬታ ነቲ ምልካዊ ኢሳይያስ እቶም ናይቲ ዑቕባ እዩ ምንጭታት ምዃኑ ከምዘሎ\n",
      "Topic 6: ኣብ እቲ ደቂ ኣንስትዮ ናይ ገለ ከም ወይ እቶም ውን ፊልም ቋንቋ ግዜ ትግርኛ ቤት ሓንቲ ውሽጢ መጽሓፍ ምልክት ሓበሬታ\n",
      "Topic 7: ኣብ ኤርትራ ሃገራት ሰብኣዊ ሕቡራት መሰላት ኤርትራውያን ኢትዮጵያ ህግደፍ መንግስቲ ውድብ ካብ ኮምሽን ምስ ኢሳያስ መንግስታት ኤርትራን ጸብጻብ ኣንጻር ናብ\n",
      "Topic 8: ትግራይ ኣብ ዞባ ካብ ህዝቢ ወረዳ ክልል ጣብያ ኣምሓራ ናብ ሕቶ ዴምህት ፓርቲ ኣቶ ወልቃይት ህወሓት ዝነበረ እዚ መንነት ኢትዮጵያ\n",
      "Topic 9: ኣብ ኤርትራ ምስ ካብ ውን ድማ ናጽነት ወዲ ዝነበረ ሓደ ኢሳይያስ ድሕሪ ከምቲ ሓርነት ተጋደልቲ ተጋዳላይ በቲ ኔሩ ዓዲ ኣብቲ\n",
      "Topic 10: እዩ ኣብ እቲ እዚ እዮም ምስ ካብ ኣሎ ከኣ ከም ግዜ ናብ እያ ሓደ ዘሎ እውን ኣብቲ ኣብዚ ብዙሕ ኸኣ\n",
      "Topic 11: ኣብ ውን ናይ ፖለቲካዊ ዘሎ ካብ ኤርትራ ከም ኣይኮነን እዚ ከኣ ዝብል ሓደ ዘለዎ ምስ መንግስቲ ሃገራዊ ይኹን ድማ ከምኡ\n",
      "Topic 12: ኣብ ኢዩ ከም እዚ ካብ ኢና ህግደፍ ናብ ህዝቢ እቶም ኣሎ እቲ እንታይ ሎሚ ኩሉ ሰባት ዓለም ፍትሒ ምስ ግን\n",
      "Topic 13: ኣብ kbps ድምጺ ምስ ካብ መጻወቲ መራገፊ ኣብዚ ኣቶ ዋሽንግተን ምስማዕ ዶር ይክኣል ትሕዝቶ ናይ ኢትዮጵያ ምሉእ መደብ ኤርትራ ሎሚ\n",
      "Topic 14: ናይ ኣብ ካብ ምስ ክሳብ ዘሎ ዝብል ናብ ድማ እዋን ከም ምበር ግን ዓለም ዝመስል ታሪኽ ውን ሰዓት ሕጂ ደቂ\n",
      "Topic 15: ኣብ ናይ ኤርትራውያን ኤርትራ ባይቶ ሃገራዊ ህዝባዊ ፍትሒ ደለይቲ ኣኼባ ኣባላት ኢዩ ዓለምለኻዊ ሰላማዊ ግንባር ለውጢ ምንቅስቓስ ማሕበር መግለጺ ኤርትራዊ\n",
      "Topic 16: ኣብ ህዝቢ ናይ ካብ ዘሎ እዩ እቲ ድማ ስርዓት ከም ልዕሊ ናብ ኢህወደግ እዋን ዝኾነ ኣብዚ እዚ ሕዚ ኢትዮጵያ ሃገርና\n",
      "Topic 17: ኣብ ስደተኛታት ናብ ባሕሪ ካብ ሊብያ ማእከላይ ናይ ሰባት ኤውሮጳ ዓመት ሃገራት ልዕሊ እቶም ገማግም ኢጣልያ ስደት ትካል ምስ ቁጽሪ\n",
      "Topic 18: ኣብ ኣሜሪካ ኦባማ ፕረዚደንት ኣመሪካ ናይ ምርጫ ትራምፕ Eritrea Ethiopia ዶናልድ ክሊንተን VOATigrigna ዋዕላ ፕረዝደንት ጉዳይ ሰልፊ ፓርቲ ባራክ ምስ\n",
      "Topic 19: ኣብ ሰባት ኢትዮጵያ ሱዳን መንግስቲ ሕማም ናይ ልዕሊ ናብ ከተማ እቲ ሓይልታት ደቡብ መጥቃዕቲ ካብ ኬንያ ዶብ ምስ ዞባ ሰራዊት\n",
      "Topic 20: Eritrea Ethiopia VOATigrigna ኣብ VOA ከተማ መጥቃዕቲ አብ ሶማል VOATigrina ሰባት መጥቓዕቲ ኣሜሪካ Somalia ኢትዮጵያ ነፋሪት ብዘጋጠመ ተገሊጹ ካብ ነፈርቲ\n",
      "Topic 21: Eritrea Ethiopia VOATigrigna ኣብ ቤተ VOA ክርስቲያን ኣቡነ ፓትርያርክ ማቻር ተዋህዶ ደቡብ ቅዱስ VIDEO ፕረዝደንት ኦርቶዶክስ እንጦንዮስ ምስ ዙር ሱዳን\n",
      "Topic 22: ከተማ ነው ላይ ጉዳይ እና አበባ Ethiopia የኢትዮጵያ አዲስ ውስጥ ጋር ወደ ቀን ባንክ አንድ መኪና ኢትዮጵያ ክልል ህዝብ ግድብ\n",
      "Topic 23: ናይ ኣብ እዩ አብ ደቡብ ኢሎም ሃገራት ኣፍሪቃ እቲ ካብ ኣፍሪካ ሕብረት ኣለው እዚ ሱዳን ሰላም እውን እዮም ምርጫ ግን\n",
      "Topic 24: ኣብ ክልል ተቓውሞ ሰልፊ ኦሮምያ ኢትዮጵያ ኣዲስ ናይ ኣበባ መንግስቲ ኦሮሚያ ከተማ ኦሮሞ ተቃውሞ ከተማታት ተማሃሮ ሓደ ስጉምቲ ሰባት ሰላማዊ\n"
     ]
    }
   ],
   "source": [
    "#[[\"መሬት\"],[\"መዓት\"]]\n",
    "n_top_words = 20\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(words2)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
