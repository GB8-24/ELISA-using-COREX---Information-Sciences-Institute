{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CorEx Hierarchical Topic Models\n",
    "Use the principle of Total Cor-relation Explanation (CorEx) to construct\n",
    "hierarchical topic models. This module is specially designed for sparse count\n",
    "data and implements semi-supervision via the information bottleneck.\n",
    "Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical\n",
    "Representations of High-Dimensional Data.\" AISTATS, 2015.\n",
    "Gallagher et al. \"Anchored Correlation Explanation: Topic Modeling with Minimal\n",
    "Domain Knowledge.\" TACL, 2017.\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu)\n",
    "Ryan J. Gallagher\n",
    "David Kale\n",
    "Lily Fierro\n",
    "License: Apache V2\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "#import corex_topic as ct\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data which is in .txt file line by line, split by tab(\\t) to a list\n",
    "list_data=[]\n",
    "with open('C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\il6_original.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the columns of datframe\n",
    "raw_data = pd.DataFrame(list_data,columns=['doc_id','text_data','class_type','additional']) \n",
    "#drop if any additional columns gets created as part of reading process\n",
    "raw_data=raw_data.drop('additional',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip columns for leading and trailing white spaces\n",
    "raw_data['doc_id']=raw_data.doc_id.str.strip()\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n docs x m attributes\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set doc_id as index\n",
    "raw_data= raw_data.set_index('doc_id')\n",
    "# change \"class_type\" column to \"categorical\" datatype\n",
    "raw_data['class_type'] = raw_data['class_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[eval_incident, unk, nondomain, indomain]\n",
       "Categories (4, object): [eval_incident, unk, nondomain, indomain]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['class_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_incident - This should be considered as part of “indomain”\n",
    "\n",
    "raw_data.loc[raw_data['class_type']==\"eval_incident\", 'class_type'] = \"indomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RT (Retweet) keyword removal\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('RT','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punctuation removal on 'text_data' column\n",
    "#print(string.punctuation)\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "#print(punct)\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: regex.sub('', x))\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove @names mentioned as part of tweets\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('\\@[a-zA-Z0-9]+','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word count (length) of the document\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji's from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Removal/ number removal from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\\d]+','',x))\n",
    "\n",
    "# strip whitespaces again \n",
    "\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "\n",
    "# calculate length again\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_SN_000370_20170329_H0T003I57</th>\n",
       "      <td>YUTARARATUNG</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_SN_000370_20170330_H0T003N16</th>\n",
       "      <td>Kiri</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text_data class_type  length\n",
       "doc_id                                                           \n",
       "IL6_SN_000370_20170329_H0T003I57  YUTARARATUNG        unk       1\n",
       "IL6_SN_000370_20170330_H0T003N16          Kiri        unk       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents with no words after pre-processing\n",
    "raw_data = raw_data.loc[raw_data.length>0]\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing documents with less than 5 words\n",
    "raw_data = raw_data.loc[raw_data.length>5]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the letters to lower case\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk          3901\n",
      "indomain     1806\n",
      "nondomain     164\n",
      "Name: class_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAFUFJREFUeJzt3X+wX3V95/Hnq+GHbnUF5OLSJLNhbToWrEZ7jUzddhUVArQNdnEK42jKMJPuDs7oTrsr+EdRlBnpbMuOU6UTltRg3SL1x5LBVJoi6DK7AjeaBgKy3AKV22TgugGUZaUG3/vH95PyJdwf3+/9CZznY+bO95z3+ZxzPic59/v6nh/fe1JVSJK652eWuwOSpOVhAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHWUASBJHXXEcndgJscff3ytWbNmubshSS8qu3bt+kFVjczW7gUdAGvWrGFsbGy5uyFJLypJ/n6QdgOfAkqyIsl3k9zYxk9KcnuS+5N8MclRrX50Gx9v09f0LeOSVr8vyRnDbZIkaSENcw3gQ8C9feNXAFdW1VrgMeDCVr8QeKyqfh64srUjycnAecApwAbgs0lWzK/7kqS5GigAkqwCzgb+axsPcBrwpdZkG3BOG97YxmnT39nabwSuq6qnq+pBYBxYvxAbIUka3qBHAP8F+E/AT9v4q4HHq+pgG58AVrbhlcDDAG36E639P9WnmOefJNmcZCzJ2OTk5BCbIkkaxqwBkOTXgUerald/eYqmNcu0meZ5tlC1papGq2p0ZGTWi9iSpDka5C6gtwG/meQs4GXAP6d3RHBMkiPap/xVwL7WfgJYDUwkOQJ4FXCgr35I/zySpCU26xFAVV1SVauqag29i7jfqKr3AbcA57Zmm4Ab2vD2Nk6b/o3qPXZsO3Beu0voJGAtcMeCbYkkaSjz+R7AR4DrknwS+C5wTatfA3w+yTi9T/7nAVTV3iTXA/cAB4GLquqZeaxfkjQPeSE/E3h0dLT8IpgkDSfJrqoana3dC/qbwJK03NZc/LVlWe9Dnzp70dfhH4OTpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOmrWAEjysiR3JPnbJHuTfLzVP5fkwSS728+6Vk+STycZT7InyZv7lrUpyf3tZ9N065QkLb5Bngj2NHBaVT2Z5EjgtiR/1ab9x6r60mHtz6T3wPe1wFuBq4C3JjkOuBQYBQrYlWR7VT22EBsiSRrOrEcA1fNkGz2y/cz0IOGNwLVtvm8DxyQ5ETgD2FlVB9qb/k5gw/y6L0maq4GuASRZkWQ38Ci9N/Hb26TL22meK5Mc3WorgYf7Zp9otenqkqRlMFAAVNUzVbUOWAWsT/J64BLgdcBbgOOAj7TmmWoRM9SfI8nmJGNJxiYnJwfpniRpDoa6C6iqHgduBTZU1f52mudp4M+A9a3ZBLC6b7ZVwL4Z6oevY0tVjVbV6MjIyDDdkyQNYZC7gEaSHNOGXw68C/heO69PkgDnAHe3WbYDH2h3A50KPFFV+4GbgNOTHJvkWOD0VpMkLYNB7gI6EdiWZAW9wLi+qm5M8o0kI/RO7ewG/l1rvwM4CxgHngIuAKiqA0k+AdzZ2l1WVQcWblMkScOYNQCqag/wpinqp03TvoCLppm2Fdg6ZB8lSYvAbwJLUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FGDPBT+ZUnuSPK3SfYm+Xirn5Tk9iT3J/likqNa/eg2Pt6mr+lb1iWtfl+SMxZroyRJsxvkCOBp4LSqeiOwDtiQ5FTgCuDKqloLPAZc2NpfCDxWVT8PXNnakeRk4DzgFGAD8Nn2oHlJ0jKYNQCq58k2emT7KeA04Eutvg04pw1vbOO06e9Mkla/rqqerqoHgXFg/YJshSRpaANdA0iyIslu4FFgJ/B3wONVdbA1mQBWtuGVwMMAbfoTwKv761PMI0laYgMFQFU9U1XrgFX0PrX/4lTN2mummTZd/TmSbE4ylmRscnJykO5JkuZgqLuAqupx4FbgVOCYJEe0SauAfW14AlgN0Ka/CjjQX59inv51bKmq0aoaHRkZGaZ7kqQhDHIX0EiSY9rwy4F3AfcCtwDntmabgBva8PY2Tpv+jaqqVj+v3SV0ErAWuGOhNkSSNJwjZm/CicC2dsfOzwDXV9WNSe4BrkvySeC7wDWt/TXA55OM0/vkfx5AVe1Ncj1wD3AQuKiqnlnYzZEkDWrWAKiqPcCbpqg/wBR38VTVj4H3TrOsy4HLh++mJGmh+U1gSeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqqEGeCbw6yS1J7k2yN8mHWv1jSf4hye72c1bfPJckGU9yX5Iz+uobWm08ycWLs0mSpEEM8kzgg8DvVdV3krwS2JVkZ5t2ZVX95/7GSU6m9xzgU4CfA/4myS+0yZ8B3g1MAHcm2V5V9yzEhkiShjPIM4H3A/vb8I+S3AusnGGWjcB1VfU08GB7OPyhZwePt2cJk+S61tYAkKRlMNQ1gCRr6D0g/vZW+mCSPUm2Jjm21VYCD/fNNtFq09UlSctg4ABI8grgy8CHq+qHwFXAa4F19I4Q/uhQ0ylmrxnqh69nc5KxJGOTk5ODdk+SNKSBAiDJkfTe/L9QVV8BqKpHquqZqvopcDXPnuaZAFb3zb4K2DdD/TmqaktVjVbV6MjIyLDbI0ka0CB3AQW4Bri3qv64r35iX7P3AHe34e3AeUmOTnISsBa4A7gTWJvkpCRH0btQvH1hNkOSNKxB7gJ6G/B+4K4ku1vto8D5SdbRO43zEPC7AFW1N8n19C7uHgQuqqpnAJJ8ELgJWAFsraq9C7gtkqQhDHIX0G1Mff5+xwzzXA5cPkV9x0zzSZKWjt8ElqSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjBnko/OoktyS5N8neJB9q9eOS7Exyf3s9ttWT5NNJxpPsSfLmvmVtau3vT7Jp8TZLkjSbQY4ADgK/V1W/CJwKXJTkZOBi4OaqWgvc3MYBzgTWtp/NwFXQCwzgUuCtwHrg0kOhIUlaerMGQFXtr6rvtOEfAfcCK4GNwLbWbBtwThveCFxbPd8GjklyInAGsLOqDlTVY8BOYMOCbo0kaWBDXQNIsgZ4E3A78Jqq2g+9kABOaM1WAg/3zTbRatPVJUnLYOAASPIK4MvAh6vqhzM1naJWM9QPX8/mJGNJxiYnJwftniRpSAMFQJIj6b35f6GqvtLKj7RTO7TXR1t9AljdN/sqYN8M9eeoqi1VNVpVoyMjI8NsiyRpCIPcBRTgGuDeqvrjvknbgUN38mwCbuirf6DdDXQq8EQ7RXQTcHqSY9vF39NbTZK0DI4YoM3bgPcDdyXZ3WofBT4FXJ/kQuD7wHvbtB3AWcA48BRwAUBVHUjyCeDO1u6yqjqwIFshSRrarAFQVbcx9fl7gHdO0b6Ai6ZZ1lZg6zAdlCQtDr8JLEkdZQBIUkcZAJLUUQaAJHWUASBJHTXIbaAvWmsu/tqyrPehT529LOuVpGF4BCBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHXUIA+F35rk0SR399U+luQfkuxuP2f1TbskyXiS+5Kc0Vff0GrjSS5e+E2RJA1jkCOAzwEbpqhfWVXr2s8OgCQnA+cBp7R5PptkRZIVwGeAM4GTgfNbW0nSMhnkofDfSrJmwOVtBK6rqqeBB5OMA+vbtPGqegAgyXWt7T1D91iStCDmcw3gg0n2tFNEx7baSuDhvjYTrTZd/XmSbE4ylmRscnJyHt2TJM1krgFwFfBaYB2wH/ijVs8UbWuG+vOLVVuqarSqRkdGRubYPUnSbOb0RLCqeuTQcJKrgRvb6ASwuq/pKmBfG56uLklaBnM6AkhyYt/oe4BDdwhtB85LcnSSk4C1wB3AncDaJCclOYreheLtc++2JGm+Zj0CSPIXwNuB45NMAJcCb0+yjt5pnIeA3wWoqr1Jrqd3cfcgcFFVPdOW80HgJmAFsLWq9i741kiSBjbIXUDnT1G+Zob2lwOXT1HfAewYqneSpEXjN4ElqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjZg2AJFuTPJrk7r7acUl2Jrm/vR7b6kny6STjSfYkeXPfPJta+/uTbFqczZEkDWqQI4DPARsOq10M3FxVa4Gb2zjAmfQeBL8W2AxcBb3AoPcs4bcC64FLD4WGJGl5zBoAVfUt4MBh5Y3Atja8DTinr35t9XwbOCbJicAZwM6qOlBVjwE7eX6oSJKW0FyvAbymqvYDtNcTWn0l8HBfu4lWm64uSVomC30ROFPUaob68xeQbE4ylmRscnJyQTsnSXrWXAPgkXZqh/b6aKtPAKv72q0C9s1Qf56q2lJVo1U1OjIyMsfuSZJmM9cA2A4cupNnE3BDX/0D7W6gU4En2imim4DTkxzbLv6e3mqSpGVyxGwNkvwF8Hbg+CQT9O7m+RRwfZILge8D723NdwBnAePAU8AFAFV1IMkngDtbu8uq6vALy5KkJTRrAFTV+dNMeucUbQu4aJrlbAW2DtU7SdKi8ZvAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHXUvAIgyUNJ7kqyO8lYqx2XZGeS+9vrsa2eJJ9OMp5kT5I3L8QGSJLmZiGOAN5RVeuqarSNXwzcXFVrgZvbOMCZwNr2sxm4agHWLUmao8U4BbQR2NaGtwHn9NWvrZ5vA8ckOXER1i9JGsB8A6CAv06yK8nmVntNVe0HaK8ntPpK4OG+eSda7TmSbE4ylmRscnJynt2TJE3niHnO/7aq2pfkBGBnku/N0DZT1Op5haotwBaA0dHR502XJC2MeR0BVNW+9voo8FVgPfDIoVM77fXR1nwCWN03+ypg33zWL0mauzkHQJKfTfLKQ8PA6cDdwHZgU2u2CbihDW8HPtDuBjoVeOLQqSJJ0tKbzymg1wBfTXJoOf+tqr6e5E7g+iQXAt8H3tva7wDOAsaBp4AL5rFuSdI8zTkAquoB4I1T1P8P8M4p6gVcNNf1SZIWlt8ElqSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOmq+D4SROmvNxV9blvU+9Kmzl2W9eunxCECSOsoAkKSOMgAkqaMMAEnqqCUPgCQbktyXZDzJxUu9fklSz5IGQJIVwGeAM4GTgfOTnLyUfZAk9Sz1EcB6YLyqHqiqfwSuAzYucR8kSSx9AKwEHu4bn2g1SdISW+ovgmWKWj2nQbIZ2NxGn0xy3zzWdzzwg3nMPye5YqnXqGXi/qVFkyvmtX/9y0EaLXUATACr+8ZXAfv6G1TVFmDLQqwsyVhVjS7EsqTDuX9pMS3F/rXUp4DuBNYmOSnJUcB5wPYl7oMkiSU+Aqiqg0k+CNwErAC2VtXepeyDJKlnyf8YXFXtAHYs0eoW5FSSNA33Ly2mRd+/UlWzt5IkveT4pyAkqaOWPQCS/M8h2789yY2L1Z+2jsuSvGsx16EXlyS3Jlm0OzKS/FySLy3W8vXil+R3kvzJQi5z2R8IU1W/stx9OFxV/cFy90HdUlX7gHOXux/qlhfCEcCT7fXt7VPWl5J8L8kXkqRN29BqtwG/1TfvcUn+e5I9Sb6d5A2t/rEk25L8dZKHkvxWkj9McleSryc5srX7gyR3Jrk7yZa+9X0uyblt+KEkH0/ynTb/65b4n0hDSLImyb1Jrk6yt+0DL0+yru0je5J8Ncmxrf2tSa5IckeS/53kV1v95Umua+2/CLy8bx3nt33h7uTZr2UlebIta1eSv0myvi3/gSS/2de//9H2p+8k+ZW++t1t+HeSfKXtq/cn+cMl/CfUEun/P2/jv9/eu6bcJw+b9+wk/yvJ8fPpw7IHwGHeBHyY3h+K+1fA25K8DLga+A3gV4F/0df+48B3q+oNwEeBa/umvRY4m97fGvpz4Jaq+iXg/7U6wJ9U1Vuq6vX0fsF/fZp+/aCq3gxcBfz+vLdSi20t8JmqOgV4HPi39PaNj7R95S7g0r72R1TVenr73qH6vweeau0vB34ZeqdqgCuA04B1wFuSnNPm+Vng1qr6ZeBHwCeBdwPvAS5rbR4F3t32p98GPj3NNqxr038J+O0kq6dpp5emqfZJAJK8B7gYOKuq5vVN9BdaANxRVRNV9VNgN7AGeB3wYFXdX71blv68r/2/Bj4PUFXfAF6d5FVt2l9V1U/o/bKvAL7e6ne15QK8I8ntSe6i9wt9yjT9+kp73dU3r164Hqyq3W14F70PA8dU1TdbbRvwa33tp/r//TXavlZVe4A9rf4Wem/yk1V1EPhC37L+kefuZ9/s2wcPLfdI4Oq2z/0lvQ87U7m5qp6oqh8D9zDgV/v1kjHde847gI8AZ1fVY/NdyQstAJ7uG36GZ69RTHev6kx/W+hpgBYmP6ln73f9KXBEO7L4LHBuOzK4GnjZLP3q75NeuA7fj44ZsP3h/79T7XdT7XOHHL6f9e+Dh5b7H4BHgDcCo8BRs/Rpqn7ppeEgz30P7n//mW6ffAB4JfALC9GBF1oATOV7wElJXtvGz++b9i3gfdC7hkDvVM0PB1zuoX/sHyR5BV6Aeyl7Anis71zq+4FvztAenrtvvR54Q6vfDvybJMen93yL8wdYVr9XAftbKLyf3tGpuukR4IQkr05yNNOfgu739/Sug16bZLozFgN7wX+qqKofp/cXQr+W5AfAbcDr2+SPAX+WZA/wFLBpiOU+nuRqeofnD9H7O0V66doE/GmSf0bvU9QFs7S/imf3rd3AHQBVtT/JJcAt9I4GdlTVDUP047PAl5O8ty3j/w63GXqpqKqfJLmM3oeKB+l92B1kvvuSvA/4yyS/UVV/N9c++E1gSeqoF8MpIEnSIjAAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOur/A33JEfyjQeh8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x200126bbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# document count across 4 class labels (unk,nondomain,indomain,eval_incident)\n",
    "print(raw_data.class_type.value_counts())\n",
    "plt.hist('class_type',data=raw_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the formatted data to a new text file for future reference\n",
    "\n",
    "reset_data=raw_data.reset_index()\n",
    "reset_data.to_csv('C:\\\\Users\\\\User\\\\Desktop\\\\isi\\\\Oromo.txt', header=True, index=False, sep='\\t', mode='w',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indomain</th>\n",
       "      <td>1806.0</td>\n",
       "      <td>35.859911</td>\n",
       "      <td>107.356458</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nondomain</th>\n",
       "      <td>164.0</td>\n",
       "      <td>42.073171</td>\n",
       "      <td>112.330503</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>684.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unk</th>\n",
       "      <td>3901.0</td>\n",
       "      <td>125.700333</td>\n",
       "      <td>344.687689</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6062.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length                                                       \n",
       "             count        mean         std  min   25%   50%   75%     max\n",
       "class_type                                                               \n",
       "indomain    1806.0   35.859911  107.356458  6.0  10.0  12.0  14.0   981.0\n",
       "nondomain    164.0   42.073171  112.330503  6.0  11.0  13.0  14.0   684.0\n",
       "unk         3901.0  125.700333  344.687689  6.0   9.0  14.0  27.0  6062.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic stats to understand document length across 4 class labels\n",
    "raw_data.groupby('class_type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Term matrix (binary matrix) with max_df =0.995, min_df =0.001\n",
    "vectorizer = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=True)\n",
    "doc_word_mat = vectorizer.fit_transform(raw_data.text_data)\n",
    "doc_word_mat = ss.csr_matrix(doc_word_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 8360)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x200126bbc50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the CorEx topic model with 25 topics\n",
    "\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=1500, verbose=False, seed=3192)\n",
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: oromoo,qabsoo,keenya,ummata,maqaa,abbaa,osoo,wayyaanee,ilmaan,bilisummaa\n",
      "1: dhugaa,ifatti,aadaa,gaafa,abdii,qabna,caalaa,lammii,keessan,qooda\n",
      "2: warra,tana,achi,qofaa,fakkaata,kunoo,ofitti,dandaʼama,habashaa,gahuu\n",
      "3: akka,kan,kun,jira,adda,qabu,garuu,waliin,kanaan,bara\n",
      "4: lafa,mirga,aangoo,yeroon,warri,as,murtii,duula,harkaa,qabuu\n",
      "5: guddaa,gadi,ammaa,hojiin,kaayyoo,ija,gahuuf,bittaa,akeeka,tarkaanfiin\n",
      "6: jedhan,washington,dc,obbo,xurree,kuma,marsariitii,dubbatan,taphachisi,has\n",
      "7: isa,miti,dhimma,qofa,hunda,rakkoo,addunyaa,siyaasaa,yaada,biyyoota\n",
      "8: irraa,wal,nama,isaan,karaa,nu,akkuma,amma,jedhanii,biraa\n",
      "9: of,ykn,qabne,biyyaa,garaa,isaaf,hundaan,dantaa,yakka,jiraatu\n",
      "10: irra,jedhu,inni,harka,jedhee,ittiin,keenyaa,jalaa,namaa,sana\n",
      "11: hin,waan,malee,haa,qaba,taane,taʼee,bira,namni,taʼeef\n",
      "12: isaa,keessaa,tokko,yeroo,jechuun,taʼuu,kanaaf,hedduu,walitti,ammoo\n",
      "13: ni,ofii,buʼaa,dhiiga,jireenya,jirru,lafaa,hamaa,akkanaa,mee\n",
      "14: kana,jiru,fi,ture,mootummaa,humna,akkasumas,jiraachuu,uummata,dandeenye\n",
      "15: jiran,yoo,kanneen,oromiyaa,taʼu,sadarkaa,naannoo,waggaa,waraanaa,haaluma\n",
      "16: taʼe,namoota,gama,alaa,namoonni,fedhii,biyyattii,barbaachisu,qaama,dabalatee\n",
      "17: mana,aku,nak,yang,yg,ada,tak,oromo,macam,dia\n",
      "18: keessa,haalli,waraana,tokkoo,mooraa,jiraatan,sochiin,barattoota,barnootaa,xumura\n",
      "19: keessatti,itti,godina,magaalaa,aanaa,bahaa,fxg,lixaa,fufee,qeerroon\n",
      "20: irratti,biyya,kanaa,taʼuun,tarkaanfii,duraa,guddaan,hawaasa,mormii,marii\n",
      "21: isaanii,gara,hojii,jechuu,ffaa,kaasee,darbe,gahaa,isaatti,barnoota\n",
      "22: booda,dura,mootummaan,erga,guutuu,jirtu,kunis,jiruuf,lamaan,ishee\n",
      "23: bakka,jala,nagaa,bulchiinsa,qabeenya,jedhaman,olii,waraanni,qawwee,ganama\n",
      "24: gaaffii,deebii,ibsa,abo,sabaa,ilaalchisee,gaaffiin,kutaa,barbaachisaa,oduu\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e-06 1.00000000e-06 1.00000000e-06 ... 4.71636343e-04\n",
      "  3.74097308e-04 4.58551485e-04]\n",
      " [1.00000000e-06 1.00000000e-06 1.00000000e-06 ... 5.00894996e-04\n",
      "  3.95908511e-04 4.98719191e-04]\n",
      " [1.00000000e-06 1.00000000e-06 1.00000000e-06 ... 4.98006855e-04\n",
      "  3.94037181e-04 4.92243816e-04]\n",
      " ...\n",
      " [9.99999000e-01 9.99999000e-01 9.99999000e-01 ... 9.99999000e-01\n",
      "  9.99999000e-01 9.99999000e-01]\n",
      " [9.99999000e-01 9.99999000e-01 9.99999000e-01 ... 9.99999000e-01\n",
      "  9.99999000e-01 9.99999000e-01]\n",
      " [9.99999000e-01 9.99999000e-01 9.99999000e-01 ... 9.99999000e-01\n",
      "  9.99999000e-01 9.99999000e-01]]\n"
     ]
    }
   ],
   "source": [
    "#The estimated probabilities of topics for each document can be accessed through p_y_given_x.\n",
    "print(topic_model.p_y_given_x) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "print(topic_model.labels) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "\n",
    "#socho始aa socho始an socho始uun  socho始uu  \n",
    "anchor_words = [[\"hoongee\",\"hongee\"],[\"galaana\",\"galaanaa\"],[\"balaa\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=1500,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: oromiyaa,walitti,lafa,qaban,keenyaa,duraa,waggoota,hongee,lammiilee,argachuu\n",
      "1: malee,haa,qaba,gadi,ni,harka,hunda,saba,galaana,ofii\n",
      "2: balaa,barbaachisu,isaatiin,qaamni,barbaadan,faallaa,biyyi,eegama,namaaf,barbaachisa\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to the topics of anchored words\n",
    "#(in this case, we have words seeded for 3 topics, hence checking the classification for those topics) \n",
    "topic_classification=anchored_topic_model.labels[:,0:3] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the classification results for the first 3 topics (seeded)\n",
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.reset_index()\n",
    "result[\"doc_id\"]=raw_data[\"doc_id\"]\n",
    "result[\"true_class_label\"]=raw_data[\"class_type\"]\n",
    "result[\"text_data\"]=raw_data[\"text_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning class lables based on the binary classification result\n",
    "result.loc[(result['topic1']==True) | (result['topic2']==True) |  (result['topic3']==True) , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False) & (result['topic3']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 7)\n",
      "shape after filtering unknown docs (1970, 7)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.true_class_label !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          161         1645\n",
      "Actual--           20          144\n",
      "precision 0.8895027624309392\n",
      "recall 0.08914728682170543\n",
      "F1_score 0.16205334675390037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.true_class_label, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm)\n",
    "\n",
    "\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision)\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall)\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more Anchor words from chineaseroom translator\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "\n",
    "#socho始aa socho始an socho始uun  socho始uu  \n",
    "anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],\n",
    "                [\"balaa\",\"dhibee\",\"irraa\"],\n",
    "                [\"sochii\",\"lafaa\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=1500,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: siyaasaa,hongee,jedhamee,jiraate,akkanaa,lammiilee,hoongee,miliyoona,oolaa,ilma\n",
      "1: guutuu,lolaa,galaana,galaanaa,fakkaatu,hafe,barbaada,takka,fudhatama,qalbii\n",
      "2: irraa,balaa,dhibee,kenname,gosa,deebine,galmee,ilmoo,labsa,ajjeesu\n",
      "3: sochii,lafaa,jirru,sammuu,bilisa,dabran,laga,qabxii,ajjeechaan,yaadu\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to the topics of anchored words\n",
    "#(in this case, we have words seeded for 4 topics, hence checking the classification for those topics) \n",
    "topic_classification=anchored_topic_model.labels[:,0:4] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the classification results for the first 4 topics (seeded)\n",
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\",\"topic4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.reset_index()\n",
    "result[\"doc_id\"]=raw_data[\"doc_id\"]\n",
    "result[\"true_class_label\"]=raw_data[\"class_type\"]\n",
    "result[\"text_data\"]=raw_data[\"text_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning class lables based on the binary classification result\n",
    "result.loc[(result['topic1']==True) | (result['topic2']==True) |  (result['topic3']==True) |(result['topic4']==True) , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False) & (result['topic3']==False) & (result['topic4']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 8)\n",
      "shape after filtering unknown docs (1970, 8)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.true_class_label !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding more words from chinease room translator \n",
      "\n",
      "          Predicted++  Predicted--\n",
      "Actual++          248         1558\n",
      "Actual--           27          137 \n",
      "\n",
      "precision 0.9018181818181819 \n",
      "\n",
      "recall 0.13732004429678848 \n",
      "\n",
      "F1_score 0.2383469485824123 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.true_class_label, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "print(\"After adding more words from chinease room translator \\n\" )\n",
    "print(cm,\"\\n\")\n",
    "\n",
    "\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++            2           16\n",
      "Actual--            0            2 \n",
      "\n",
      "precision 1.0 \n",
      "\n",
      "recall 0.1111111111111111 \n",
      "\n",
      "F1_score 0.19999999999999998 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 20 based anchor word count\n",
    "\n",
    "top_docs = result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መሬት|መዓት').sum())\n",
    "\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "top15 = top_docs.iloc[0:20,:]\n",
    "\n",
    "cm=pd.DataFrame(confusion_matrix(top15.true_class_label, top15.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20151002_H0040LX9Z</th>\n",
       "      <td>itoophiyaa keessatti laakkofsi namoota hongeef...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020537_20161206_H0040MR72</th>\n",
       "      <td>balaa hoongee naannolee shanitti uumameef deeg...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20170426_H0040MPVA</th>\n",
       "      <td>aanaalee godina guraagee balaan hoongee mudate...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020505_20150902_H0040MO8I</th>\n",
       "      <td>ijoo dubbii abo ummatoota beela irraa baraaruu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020535_20151124_H0040MR6K</th>\n",
       "      <td>diiniyaas alamuu laliisaa itiyoopiyaan maalif ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160205_H0040LX8T</th>\n",
       "      <td>ummata beelaye duʼa jalaa baraaruuf gargaarsa ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020412_20150929_H0040MR7V</th>\n",
       "      <td>hoongee fi gogiinsa oromiyaa keessaa lammiilee...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020533_20170318_H0040MO5O</th>\n",
       "      <td>sababa walitti buʼiinsa daangaa oromiyaa fi su...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020533_20170120_H0040MO5P</th>\n",
       "      <td>daangaa naannoo oromiyaa fi sumaalee irratti w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020548_20151216_H0040MR7K</th>\n",
       "      <td>birhanu m lenjiso lolli wayyaaneen oromorratti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_NW_020423_20151002_H0040LX9Z  itoophiyaa keessatti laakkofsi namoota hongeef...   \n",
       "IL6_WL_020537_20161206_H0040MR72  balaa hoongee naannolee shanitti uumameef deeg...   \n",
       "IL6_NW_020532_20170426_H0040MPVA  aanaalee godina guraagee balaan hoongee mudate...   \n",
       "IL6_NW_020505_20150902_H0040MO8I  ijoo dubbii abo ummatoota beela irraa baraaruu...   \n",
       "IL6_WL_020535_20151124_H0040MR6K  diiniyaas alamuu laliisaa itiyoopiyaan maalif ...   \n",
       "IL6_NW_020423_20160205_H0040LX8T  ummata beelaye duʼa jalaa baraaruuf gargaarsa ...   \n",
       "IL6_NW_020412_20150929_H0040MR7V  hoongee fi gogiinsa oromiyaa keessaa lammiilee...   \n",
       "IL6_NW_020533_20170318_H0040MO5O  sababa walitti buʼiinsa daangaa oromiyaa fi su...   \n",
       "IL6_NW_020533_20170120_H0040MO5P  daangaa naannoo oromiyaa fi sumaalee irratti w...   \n",
       "IL6_WL_020548_20151216_H0040MR7K  birhanu m lenjiso lolli wayyaaneen oromorratti...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_NW_020423_20151002_H0040LX9Z      7  \n",
       "IL6_WL_020537_20161206_H0040MR72      7  \n",
       "IL6_NW_020532_20170426_H0040MPVA      5  \n",
       "IL6_NW_020505_20150902_H0040MO8I      4  \n",
       "IL6_WL_020535_20151124_H0040MR6K      4  \n",
       "IL6_NW_020423_20160205_H0040LX8T      4  \n",
       "IL6_NW_020412_20150929_H0040MR7V      3  \n",
       "IL6_NW_020533_20170318_H0040MO5O      3  \n",
       "IL6_NW_020533_20170120_H0040MO5P      2  \n",
       "IL6_WL_020548_20151216_H0040MR7K      2  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 1\n",
    "\n",
    "\n",
    "topic1_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic1==True),\"doc_id\"]\n",
    "\n",
    "topic1_docs_id.shape\n",
    "\n",
    "topic1_docs =raw_data.loc[raw_data.doc_id.isin(topic1_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic1_docs= topic1_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],[\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic1_docs['count'] = topic1_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa').sum())\n",
    "\n",
    "topic1_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic1_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020445_20160426_H0040M4CT</th>\n",
       "      <td>balaa lolaa dirree dawaatti qaqqabeen namoonni...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160419_H0040LX80</th>\n",
       "      <td>biyyatti ajjeefamuu fi biyyoota ollaattii abdi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20160105_H0040MPV6</th>\n",
       "      <td>koomishinichi karoora balaa lolaa ittisuu fi l...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020423_20150814_H0040MR7C</th>\n",
       "      <td>haalli qilleensa el nino cimaan uumamuuf deema...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020445_20160507_H0040M4CF</th>\n",
       "      <td>ministeerichi balaa lolaa mudatu hirʼisuuf hoj...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20170426_H0040MPV3</th>\n",
       "      <td>rooba tibbanaan oromiyaatti namoonni kuma  taʼ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020427_20170210_H0040LYS7</th>\n",
       "      <td>mudannoon weerara maqaa liyyuu polisiin murnii...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020117_20160705_H0040LMLC</th>\n",
       "      <td>bokkaan kalee barraaqa adaamaatti roobe waan h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOD7</th>\n",
       "      <td>karoora mootummaan wayyaanee ergamtuu isaa opd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_NW_020445_20160426_H0040M4CT  balaa lolaa dirree dawaatti qaqqabeen namoonni...   \n",
       "IL6_NW_020423_20160419_H0040LX80  biyyatti ajjeefamuu fi biyyoota ollaattii abdi...   \n",
       "IL6_NW_020532_20160105_H0040MPV6  koomishinichi karoora balaa lolaa ittisuu fi l...   \n",
       "IL6_WL_020423_20150814_H0040MR7C  haalli qilleensa el nino cimaan uumamuuf deema...   \n",
       "IL6_NW_020445_20160507_H0040M4CF  ministeerichi balaa lolaa mudatu hirʼisuuf hoj...   \n",
       "IL6_NW_020532_20170426_H0040MPV3  rooba tibbanaan oromiyaatti namoonni kuma  taʼ...   \n",
       "IL6_NW_020427_20170210_H0040LYS7  mudannoon weerara maqaa liyyuu polisiin murnii...   \n",
       "IL6_NW_020117_20160705_H0040LMLC  bokkaan kalee barraaqa adaamaatti roobe waan h...   \n",
       "IL6_WL_020405_20170307_H0040LOD7  karoora mootummaan wayyaanee ergamtuu isaa opd...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020415_20160225_H0040LT3D     16  \n",
       "IL6_NW_020445_20160426_H0040M4CT      8  \n",
       "IL6_NW_020423_20160419_H0040LX80      7  \n",
       "IL6_NW_020532_20160105_H0040MPV6      7  \n",
       "IL6_WL_020423_20150814_H0040MR7C      6  \n",
       "IL6_NW_020445_20160507_H0040M4CF      6  \n",
       "IL6_NW_020532_20170426_H0040MPV3      6  \n",
       "IL6_NW_020427_20170210_H0040LYS7      5  \n",
       "IL6_NW_020117_20160705_H0040LMLC      5  \n",
       "IL6_WL_020405_20170307_H0040LOD7      4  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 2\n",
    "\n",
    "\n",
    "topic2_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic2==True),\"doc_id\"]\n",
    "\n",
    "topic2_docs_id.shape\n",
    "\n",
    "\n",
    "topic2_docs =raw_data.loc[raw_data.doc_id.isin(topic2_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic2_docs= topic2_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],[\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic2_docs['count'] = topic2_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'galaana|galaanaa|guutuu|lolaa').sum())\n",
    "\n",
    "topic2_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic2_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020539_20160403_H0040MR7Q</th>\n",
       "      <td>bishaantu rasaasa taʼe yooseef hambaa tolaa bi...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOE5</th>\n",
       "      <td>sabummaa mormaa sabummaa deggeruu sbo bitootes...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOFT</th>\n",
       "      <td>shira lukkeelee fi gooftoota isaani gaomee dhu...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20160105_H0040MPV6</th>\n",
       "      <td>koomishinichi karoora balaa lolaa ittisuu fi l...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020411_20170318_H0040LSJJ</th>\n",
       "      <td>sirni tpleprdf wayta ummata wayyaba gara lammi...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020412_20170427_H0040MR6F</th>\n",
       "      <td>ibsa ijjannoo barattoota oromoo yuunibarsiitii...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020117_20160420_H0040LMNX</th>\n",
       "      <td>balaa doonii godaantota  galaafaterraa namni l...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160419_H0040LX80</th>\n",
       "      <td>biyyatti ajjeefamuu fi biyyoota ollaattii abdi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020408_20160402_H0040M6A0</th>\n",
       "      <td>#oromoprotests daily april   sbo ebla   sbovol...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020539_20160403_H0040MR7Q  bishaantu rasaasa taʼe yooseef hambaa tolaa bi...   \n",
       "IL6_WL_020405_20170307_H0040LOE5  sabummaa mormaa sabummaa deggeruu sbo bitootes...   \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_WL_020405_20170307_H0040LOFT  shira lukkeelee fi gooftoota isaani gaomee dhu...   \n",
       "IL6_NW_020532_20160105_H0040MPV6  koomishinichi karoora balaa lolaa ittisuu fi l...   \n",
       "IL6_NW_020411_20170318_H0040LSJJ  sirni tpleprdf wayta ummata wayyaba gara lammi...   \n",
       "IL6_WL_020412_20170427_H0040MR6F  ibsa ijjannoo barattoota oromoo yuunibarsiitii...   \n",
       "IL6_NW_020117_20160420_H0040LMNX  balaa doonii godaantota  galaafaterraa namni l...   \n",
       "IL6_NW_020423_20160419_H0040LX80  biyyatti ajjeefamuu fi biyyoota ollaattii abdi...   \n",
       "IL6_NW_020408_20160402_H0040M6A0  #oromoprotests daily april   sbo ebla   sbovol...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020539_20160403_H0040MR7Q     22  \n",
       "IL6_WL_020405_20170307_H0040LOE5     18  \n",
       "IL6_WL_020415_20160225_H0040LT3D     16  \n",
       "IL6_WL_020405_20170307_H0040LOFT     16  \n",
       "IL6_NW_020532_20160105_H0040MPV6     13  \n",
       "IL6_NW_020411_20170318_H0040LSJJ     12  \n",
       "IL6_WL_020412_20170427_H0040MR6F     11  \n",
       "IL6_NW_020117_20160420_H0040LMNX     11  \n",
       "IL6_NW_020423_20160419_H0040LX80     11  \n",
       "IL6_NW_020408_20160402_H0040M6A0     10  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 3\n",
    "\n",
    "\n",
    "topic3_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic3==True),\"doc_id\"]\n",
    "\n",
    "topic3_docs_id.shape\n",
    "\n",
    "\n",
    "topic3_docs =raw_data.loc[raw_data.doc_id.isin(topic3_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic3_docs= topic3_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic3_docs['count'] = topic3_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'balaa|dhibee|irraa').sum())\n",
    "\n",
    "topic3_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic3_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOET</th>\n",
       "      <td>qeerroo diddaan barattoota oromoo bifa qindaay...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020525_20160831_H0040MNBX</th>\n",
       "      <td>#oromoprotests #dhaamsa qeerroo bilisummaa oro...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOD7</th>\n",
       "      <td>karoora mootummaan wayyaanee ergamtuu isaa opd...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOFT</th>\n",
       "      <td>shira lukkeelee fi gooftoota isaani gaomee dhu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOSG</th>\n",
       "      <td>wayyaaneen shira marsaa ffaa magaalaa amboo ir...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020520_20160716_H0040MMVK</th>\n",
       "      <td>injifannoo uummanni oromoo baatii saddet darba...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20150514_H0040LXAY</th>\n",
       "      <td>sochiin dachii oromiyaa eprdfopdo irratti mana...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020427_20150806_H0040LYRC</th>\n",
       "      <td>uummatni hidhaa fi reebichaan saree iyyuu ni x...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020525_20160218_H0040MNCV</th>\n",
       "      <td>godina arsii lixaatti magaalotaa fi baadiyaan ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_WL_020405_20170307_H0040LOET  qeerroo diddaan barattoota oromoo bifa qindaay...   \n",
       "IL6_WL_020525_20160831_H0040MNBX  #oromoprotests #dhaamsa qeerroo bilisummaa oro...   \n",
       "IL6_WL_020405_20170307_H0040LOD7  karoora mootummaan wayyaanee ergamtuu isaa opd...   \n",
       "IL6_WL_020405_20170307_H0040LOFT  shira lukkeelee fi gooftoota isaani gaomee dhu...   \n",
       "IL6_WL_020405_20170307_H0040LOSG  wayyaaneen shira marsaa ffaa magaalaa amboo ir...   \n",
       "IL6_WL_020520_20160716_H0040MMVK  injifannoo uummanni oromoo baatii saddet darba...   \n",
       "IL6_NW_020423_20150514_H0040LXAY  sochiin dachii oromiyaa eprdfopdo irratti mana...   \n",
       "IL6_NW_020427_20150806_H0040LYRC  uummatni hidhaa fi reebichaan saree iyyuu ni x...   \n",
       "IL6_WL_020525_20160218_H0040MNCV  godina arsii lixaatti magaalotaa fi baadiyaan ...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020415_20160225_H0040LT3D      7  \n",
       "IL6_WL_020405_20170307_H0040LOET      6  \n",
       "IL6_WL_020525_20160831_H0040MNBX      5  \n",
       "IL6_WL_020405_20170307_H0040LOD7      5  \n",
       "IL6_WL_020405_20170307_H0040LOFT      4  \n",
       "IL6_WL_020405_20170307_H0040LOSG      4  \n",
       "IL6_WL_020520_20160716_H0040MMVK      4  \n",
       "IL6_NW_020423_20150514_H0040LXAY      3  \n",
       "IL6_NW_020427_20150806_H0040LYRC      3  \n",
       "IL6_WL_020525_20160218_H0040MNCV      3  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 4\n",
    "\n",
    "\n",
    "topic4_docs_id = result.loc[(result.true_class_label==\"indomain\")&(result.topic4==True),\"doc_id\"]\n",
    "\n",
    "topic4_docs_id.shape\n",
    "\n",
    "\n",
    "topic4_docs =raw_data.loc[raw_data.doc_id.isin(topic4_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic4_docs= topic4_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic4_docs['count'] = topic4_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'sochii|lafaa').sum())\n",
    "\n",
    "topic4_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic4_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach guided-LDA\n",
    "# reference links below\n",
    "#1) https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\n",
    "#2)https://github.com/vi3k6i5/GuidedLDA\n",
    "\n",
    "import numpy as np\n",
    "import guidedlda\n",
    "model = guidedlda.GuidedLDA(n_topics=25, n_iter=1500, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5871x8360 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 247375 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document-Term Matrix-(count matrix)\n",
    "vectorizer2 = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = False,lowercase=True)\n",
    "doc_word_mat2 = vectorizer2.fit_transform(raw_data.text_data)\n",
    "doc_word_mat2 = ss.csr_matrix(doc_word_mat2)\n",
    "doc_word_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words2 = list(np.asarray(vectorizer2.get_feature_names()))\n",
    "word2id = dict((v, idx) for idx, v in enumerate(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "\n",
    "seed_topic_list = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],\n",
    "                [\"balaa\",\"dhibee\",\"irraa\"],\n",
    "                [\"sochii\",\"lafaa\"]]\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 5871\n",
      "INFO:guidedlda:vocab_size: 8360\n",
      "INFO:guidedlda:n_words: 442803\n",
      "INFO:guidedlda:n_topics: 25\n",
      "INFO:guidedlda:n_iter: 1500\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\guidedlda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -5295904\n",
      "INFO:guidedlda:<20> log likelihood: -3738637\n",
      "INFO:guidedlda:<40> log likelihood: -3625005\n",
      "INFO:guidedlda:<60> log likelihood: -3571601\n",
      "INFO:guidedlda:<80> log likelihood: -3544784\n",
      "INFO:guidedlda:<100> log likelihood: -3528939\n",
      "INFO:guidedlda:<120> log likelihood: -3518160\n",
      "INFO:guidedlda:<140> log likelihood: -3507952\n",
      "INFO:guidedlda:<160> log likelihood: -3503433\n",
      "INFO:guidedlda:<180> log likelihood: -3497244\n",
      "INFO:guidedlda:<200> log likelihood: -3493299\n",
      "INFO:guidedlda:<220> log likelihood: -3490710\n",
      "INFO:guidedlda:<240> log likelihood: -3488527\n",
      "INFO:guidedlda:<260> log likelihood: -3485648\n",
      "INFO:guidedlda:<280> log likelihood: -3482464\n",
      "INFO:guidedlda:<300> log likelihood: -3482612\n",
      "INFO:guidedlda:<320> log likelihood: -3481994\n",
      "INFO:guidedlda:<340> log likelihood: -3478770\n",
      "INFO:guidedlda:<360> log likelihood: -3477980\n",
      "INFO:guidedlda:<380> log likelihood: -3476858\n",
      "INFO:guidedlda:<400> log likelihood: -3476305\n",
      "INFO:guidedlda:<420> log likelihood: -3474892\n",
      "INFO:guidedlda:<440> log likelihood: -3473329\n",
      "INFO:guidedlda:<460> log likelihood: -3472098\n",
      "INFO:guidedlda:<480> log likelihood: -3473004\n",
      "INFO:guidedlda:<500> log likelihood: -3471395\n",
      "INFO:guidedlda:<520> log likelihood: -3470699\n",
      "INFO:guidedlda:<540> log likelihood: -3469978\n",
      "INFO:guidedlda:<560> log likelihood: -3468548\n",
      "INFO:guidedlda:<580> log likelihood: -3466309\n",
      "INFO:guidedlda:<600> log likelihood: -3467309\n",
      "INFO:guidedlda:<620> log likelihood: -3464731\n",
      "INFO:guidedlda:<640> log likelihood: -3461233\n",
      "INFO:guidedlda:<660> log likelihood: -3461813\n",
      "INFO:guidedlda:<680> log likelihood: -3460234\n",
      "INFO:guidedlda:<700> log likelihood: -3460575\n",
      "INFO:guidedlda:<720> log likelihood: -3459368\n",
      "INFO:guidedlda:<740> log likelihood: -3458649\n",
      "INFO:guidedlda:<760> log likelihood: -3456979\n",
      "INFO:guidedlda:<780> log likelihood: -3456387\n",
      "INFO:guidedlda:<800> log likelihood: -3457525\n",
      "INFO:guidedlda:<820> log likelihood: -3456615\n",
      "INFO:guidedlda:<840> log likelihood: -3455203\n",
      "INFO:guidedlda:<860> log likelihood: -3454941\n",
      "INFO:guidedlda:<880> log likelihood: -3454730\n",
      "INFO:guidedlda:<900> log likelihood: -3453846\n",
      "INFO:guidedlda:<920> log likelihood: -3455197\n",
      "INFO:guidedlda:<940> log likelihood: -3452683\n",
      "INFO:guidedlda:<960> log likelihood: -3451933\n",
      "INFO:guidedlda:<980> log likelihood: -3449595\n",
      "INFO:guidedlda:<1000> log likelihood: -3451020\n",
      "INFO:guidedlda:<1020> log likelihood: -3451234\n",
      "INFO:guidedlda:<1040> log likelihood: -3449721\n",
      "INFO:guidedlda:<1060> log likelihood: -3448704\n",
      "INFO:guidedlda:<1080> log likelihood: -3447457\n",
      "INFO:guidedlda:<1100> log likelihood: -3448027\n",
      "INFO:guidedlda:<1120> log likelihood: -3447045\n",
      "INFO:guidedlda:<1140> log likelihood: -3447686\n",
      "INFO:guidedlda:<1160> log likelihood: -3445872\n",
      "INFO:guidedlda:<1180> log likelihood: -3445984\n",
      "INFO:guidedlda:<1200> log likelihood: -3446254\n",
      "INFO:guidedlda:<1220> log likelihood: -3446955\n",
      "INFO:guidedlda:<1240> log likelihood: -3447986\n",
      "INFO:guidedlda:<1260> log likelihood: -3446345\n",
      "INFO:guidedlda:<1280> log likelihood: -3444909\n",
      "INFO:guidedlda:<1300> log likelihood: -3446063\n",
      "INFO:guidedlda:<1320> log likelihood: -3446989\n",
      "INFO:guidedlda:<1340> log likelihood: -3444449\n",
      "INFO:guidedlda:<1360> log likelihood: -3444130\n",
      "INFO:guidedlda:<1380> log likelihood: -3444084\n",
      "INFO:guidedlda:<1400> log likelihood: -3443933\n",
      "INFO:guidedlda:<1420> log likelihood: -3441809\n",
      "INFO:guidedlda:<1440> log likelihood: -3442142\n",
      "INFO:guidedlda:<1460> log likelihood: -3441182\n",
      "INFO:guidedlda:<1480> log likelihood: -3440274\n",
      "INFO:guidedlda:<1499> log likelihood: -3441239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x200172e4c88>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: nu na koo keenya ani kan akka si hin kee\n",
      "Topic 1: afaan oromoo gadaa keessatti aadaa sirna bara ilmaan ture itti\n",
      "Topic 2: nu kan kana fi hin akka waan keenya of irratti\n",
      "Topic 3: fi oromoo qabsoo bilisummaa abo ummata irratti jiru akka isaa\n",
      "Topic 4: fi akka nama keessaa hin ka itoophiyaa biyya irratti hiriira\n",
      "Topic 5: keessatti godina itti aanaa magaalaa fxg jira fi wallaggaa oromoo\n",
      "Topic 6: kan fi oromoo ture irraa akka kun yeroo keessa kana\n",
      "Topic 7: akka fi kan finfinnee obbo naannoo oromiyaa hojii irratti itti\n",
      "Topic 8: fi oromoo jiru wayyaanee ummata hin isaa wayyaaneen kan ilmaan\n",
      "Topic 9: mana kan akka hidhaa isaanii obbo fi oromoo hin isaa\n",
      "Topic 10: mana aku yg nak yang ada ni tak di tu\n",
      "Topic 11: hin kan fi akka oromoo dha malee utuu tokko tahu\n",
      "Topic 12: fi oromoo uummata wayyaanee irraa irratti kan hin oromiyaa qeerroo\n",
      "Topic 13: oromo oromoo fi ethiopia qeerroo sbo bilisummaa oduu sagalee qophii\n",
      "Topic 14: fi hin kan akka irraa oromoo irratti ummata isaa ummatni\n",
      "Topic 15: fi akka mootummaan jira jiru itoophiyaa balaa keessatti addunyaa bishaan\n",
      "Topic 16: ganda ang ng mo na ko sa ni ka talaga\n",
      "Topic 17: fii isaa qabsoo kan akka irraa oromoo biyya itti tan\n",
      "Topic 18: fi code jedhan ka hin akka irratti mp kbps isaanii\n",
      "Topic 19: hin yoo fi waan malee kana kan tokko nama isa\n",
      "Topic 20: fi godina oromorevolution wayyaanee waraana irratti kan bahaa waraanaa harargee\n",
      "Topic 21: magaalaa fi jiru keessatti mormii oromiyaa jira akka kan hiriira\n",
      "Topic 22: kan fi kuni irratti ummata isaa ummanni ture yeroo keessatti\n",
      "Topic 23: akka kan kana irratti kun yeroo waan itti hin keessatti\n",
      "Topic 24: mana the to que de in and of ganda eu\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(words2)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
