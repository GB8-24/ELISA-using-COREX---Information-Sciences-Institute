{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     25,
     282,
     312,
     345,
     357,
     385,
     397,
     531,
     599,
     603,
     607,
     617,
     623
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"CorEx Hierarchical Topic Models\n",
    "Use the principle of Total Cor-relation Explanation (CorEx) to construct\n",
    "hierarchical topic models. This module is specially designed for sparse count\n",
    "data and implements semi-supervision via the information bottleneck.\n",
    "Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical\n",
    "Representations of High-Dimensional Data.\" AISTATS, 2015.\n",
    "Gallagher et al. \"Anchored Correlation Explanation: Topic Modeling with Minimal\n",
    "Domain Knowledge.\" TACL, 2017.\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu)\n",
    "Ryan J. Gallagher\n",
    "David Kale\n",
    "Lily Fierro\n",
    "License: Apache V2\n",
    "\"\"\"\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data which is in .txt file line by line, split by tab(\\t) to a list\n",
    "list_data=[]\n",
    "with open('C:\\\\Users\\\\Gaurav\\\\Desktop\\\\ISI forms\\\\Project\\\\il5_uromo.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4969, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the columns of the datframe\n",
    "raw_data = pd.DataFrame(list_data,columns=['doc_id','text_data','class_type']) \n",
    "# strip columns for leading and trailing white spaces\n",
    "raw_data['doc_id']=raw_data.doc_id.str.strip()\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "# n docs x m attributes\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set doc_id as index of the datframe\n",
    "raw_data= raw_data.set_index('doc_id')\n",
    "\n",
    "# change \"class_type\" column to \"categorical\" datatype\n",
    "raw_data['class_type'] = raw_data['class_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eval_incident - This should be considered as part of “indomain”\n",
    "\n",
    "raw_data.loc[raw_data['class_type']==\"eval_incident\", 'class_type'] = \"indomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "\n",
    "# RT (Retweet) keyword removal\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('RT','',x))\n",
    "\n",
    "#punctuation removal on 'text_data' column\n",
    "#print(string.punctuation)\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "#print(punct)\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: regex.sub('', x))\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "\n",
    "# remove @names mentioned as part of tweets\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('\\@[a-zA-Z0-9]+','',x))\n",
    "\n",
    "# remove emoji's from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))\n",
    "\n",
    "\n",
    "# Date Removal/ number removal from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\\d]+','',x))\n",
    "\n",
    "# strip whitespaces again \n",
    "\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "\n",
    "# calculate length again\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "# calculate word count (length) of the document\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by document length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20160501_G0T000FB5</th>\n",
       "      <td>#FeelTheBern</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20160415_G0T000FCN</th>\n",
       "      <td>#DemDebate</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text_data class_type  length\n",
       "doc_id                                                           \n",
       "IL5_SN_000370_20160501_G0T000FB5  #FeelTheBern        unk       1\n",
       "IL5_SN_000370_20160415_G0T000FCN    #DemDebate        unk       1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents with no words after pre-processing\n",
    "raw_data = raw_data.loc[raw_data.length>0]\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4573, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing documents with less than 5 words (very short documents)\n",
    "raw_data = raw_data.loc[raw_data.length>5]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk          3126\n",
      "nondomain     821\n",
      "indomain      626\n",
      "Name: class_type, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f016db471dbf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[0;32m   3079\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3080\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3081\u001b[1;33m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[0;32m   3082\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3083\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1898\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   6178\u001b[0m             \u001b[0mxmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6179\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6180\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6181\u001b[0m                     \u001b[0mxmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6182\u001b[0m                     \u001b[0mxmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "# document count across 4 class labels (unk,nondomain,indomain,eval_incident)\n",
    "\n",
    "print(raw_data.class_type.value_counts())\n",
    "plt.hist('class_type',data=raw_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the formatted data to a new text file for future reference\n",
    "reset_data=raw_data.reset_index()\n",
    "reset_data.to_csv('C:\\\\Users\\\\Gaurav\\\\Desktop\\\\ISI forms\\\\Project\\\\Tigrinya.txt', header=True, index=False, sep='\\t', mode='w',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic stats to understand document length across 3 class labels\n",
    "raw_data.groupby('class_type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Term matrix (binary matrix) with max_df =0.995, min_df =0.001\n",
    "\n",
    "vectorizer = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=False)\n",
    "doc_word_mat = vectorizer.fit_transform(reset_data.text_data)\n",
    "doc_word_mat = ss.csr_matrix(doc_word_mat)\n",
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x211f1479438>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the CorEx topic model with 25 topics\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=1500, verbose=False, seed=3192)\n",
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ወይ,እምበር,ከምቲ,እንተ,እንታይ,ክንዲ,ይግባእ,ኢልካ,ባይታ,ውሑዳት\n",
      "1: ተኻኢሉ,ስለ,ንምፍላጥ,ገዛኢ,ብዘይ,ኢህወዴግ,ተገሊፁ,ሕዚ,ብምባል,ወሲኹ\n",
      "2: ኤርትራ,ኩሉ,ከኣ,ኣይኮነን,ህግደፍ,ኤርትራውያን,ቃልሲ,ስለዚ,ፍትሒ,ለውጢ\n",
      "3: ውን,ሕጂ,ጥራይ,ከምዘሎ,ማለት,ዓለም,ንሱ,ኣገባብ,ኣንጻር,ዕድል\n",
      "4: እቲ,ኣብዚ,ኣብቲ,ምስ,እዩ,ድሕሪ,ክሳብ,ካብቲ,ብመሰረት,ዝርከቡ\n",
      "5: ከም,ድማ,ነቲ,ግን,እቶም,ሓደ,ዝኾነ,እንተኾነ,ስልጣን,በዚ\n",
      "6: ካልእ,ትሕቲ,መንእሰያት,ኩሎም,ሞት,ዘለዋ,ዝተፈላለየ,ገዛ,ተስፋ,ዝኣመሰሉ\n",
      "7: ዘሎ,ሓበሬታ,ናይቲ,በቲ,ምኽንያት,ኣረዲኡ,ወገናት,ምህላዉ,ብዙሓት,ምህላዎም\n",
      "8: መራገፊ,kbps,ድምጺ,መጻወቲ,ምሉእ,ምስማዕ,ኢሎም,ትሕዝቶ,ይክኣል,ዋሽንግተን\n",
      "9: ዘለዎ,ሕቶ,ፖለቲካዊ,ኣለዎ,ዘለዎም,ሓሳብ,መልሲ,ባዕሉ,ዝኽእል,ዘለና\n",
      "10: ምዃኑ,ገለ,ነይሩ,ከሎ,ብዛዕባ,ክኸውን,ሓንቲ,ስደት,ይኽእል,መራሕቲ\n",
      "11: እዚ,ኮይኑ,ነዚ,ሰብ,ይኹን,ግዜ,እኳ,ከምኡ,ብዙሕ,ዓይነት\n",
      "12: ካብ,ኣብ,ናብ,መንግስቲ,ሃገር,ዓመት,Ethiopia,እታ,Eritrea,ዓመታት\n",
      "13: ሓቂ,የለን,ዘይኮነስ,ንሕና,ዘመን,ክፋል,እምበኣር,ጸገም,ንኽእል,ማዕረ\n",
      "14: ስርዓት,ህዝቢ,ሃገርና,ህዝባዊ,ህዝብን,ሃገራዊ,ሃገርን,ሓይሊ,ብምዃኑ,ኣኼባ\n",
      "15: ሃገራት,ሕቡራት,ስደተኛታት,ባሕሪ,ማእከላይ,ወጻኢ,ጉዳያት,ኣሜሪካ,ትማሊ,ኣመሪካ\n",
      "16: ኢዩ,እሞ,ደኣ,ኸኣ,ኣነ,ኢኻ,ኣምላኽ,ቤተ,ዘለኹ,መገዲ\n",
      "17: ላይ,ነው,እና,የኢትዮጵያ,ጋር,አበባ,ውስጥ,ቀን,ወደ,አንድ\n",
      "18: ተፈሊጡ,ካልኦት,ዝርከብ,ከባቢ,ገሊፆም,ክልተ,ሓደጋ,ተባሂሉ,ኣብታ,ሽሕ\n",
      "19: ልዕሊ,ዝሓለፈ,ኣሎ,ዝብል,ኢና,ውሽጢ,ወርሒ,ቀንዲ,መወዳእታ,ዝበለ\n",
      "20: ናይ,እዋን,ብፍላይ,ኩነታት,ስራሕ,ሎሚ,ኣዲስ,ኣበባ,ካልኣይ,ኣዝዩ\n",
      "21: ሰባት,ዞባ,ኢትዮጵያ,ደቡብ,ብሰንኪ,ሱዳን,ሓገዝ,ወረዳ,ሓይልታት,ኣፍሊጡ\n",
      "22: እዮም,ሰብኣዊ,መሰላት,ቅድሚ,ዝበሉ,ብርክት,ውድብ,ኣካል,ካብቶም,መንገዲ\n",
      "23: ትግራይ,ሰናይ,ብህዝቢ,መቐለ,ዕድመ,ሕቶታት,ግርማይ,ሒዙ,ህወሓት,ዝተገብረ\n",
      "24: ክልል,ተቓውሞ,ኦሮምያ,ሰልፊ,ኦሮሚያ,ሰላማዊ,ነበርቲ,ተቃውሞ,ፖሊስ,መጥቃዕቲ\n"
     ]
    }
   ],
   "source": [
    "# Print all 25 topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "#Tigrinya (script is left-to-right):\n",
    "#earthquake: ~ምንቅጥቃጥ መሬት   (Romanized: meneqeteqaate mareete)\n",
    "\n",
    "#drought: ~ነቕጺ            (Romanized: naqhetsi)\n",
    "#flood: ~ዕልቕልቕ            (Romanized: eleqheleqhe)\n",
    "#disaster: መዓት; ~መቕዘፍቲ    (Romanized: maaate; maqhezafeti)\n",
    " \n",
    "anchor_words = [[\"መሬት\"],\n",
    "                [\"መዓት\",\"ሓደጋ\"],\n",
    "                [\"ድርቂ\",\"ድርቅን\"],\n",
    "                [\"ውሕጅ\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25,max_iter=1500, seed=3192)   \n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: መሬት,ከም,ህዝቢ,ስለ,ሰብ,በቲ,ብዘይ,ይኹን,ምኽንያት,እኳ\n",
      "1: ሓደጋ,ብሰንኪ,ሞት,ካብዚ,እኹል,መዓት,ሓገዝ,ትካላት,ገዛ,ንኣብነት\n",
      "2: ድርቂ,ተኻኢሉ,ንምፍላጥ,ገዛኢ,ተገሊፁ,ሕዚ,ኢህወዴግ,ተፈሊጡ,ወሲኹ,ዓም\n",
      "3: እዮም,ኢሉ,ውሕጅ,ብዙሕ,ቀንዲ,ንሱ,ክንዲ,ክኸውን,ደረጃ,ሓንቲ\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to the topics of anchored words\n",
    "#(in this case, we have words seeded for 4 topics, hence checking the classification for those topics) \n",
    "topic_classification=anchored_topic_model.labels[:,0:4] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the classification results for the first 4 topics (seeded)\n",
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\",\"topic4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL5_SN_000370_20161010_H0T00365S</td>\n",
       "      <td>We have Constitution but not Constitutionality</td>\n",
       "      <td>unk</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL5_SN_000370_20150728_G0T000F6B</td>\n",
       "      <td>IMO adetat deqena gidefom yibla alewa</td>\n",
       "      <td>unk</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id  \\\n",
       "0  IL5_SN_000370_20161010_H0T00365S   \n",
       "1  IL5_SN_000370_20150728_G0T000F6B   \n",
       "\n",
       "                                        text_data class_type  length  \n",
       "0  We have Constitution but not Constitutionality        unk       6  \n",
       "1           IMO adetat deqena gidefom yibla alewa        unk       6  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.reset_index(inplace=True)\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL5_SN_000370_20161010_H0T00365S</td>\n",
       "      <td>We have Constitution but not Constitutionality</td>\n",
       "      <td>unk</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL5_SN_000370_20150728_G0T000F6B</td>\n",
       "      <td>IMO adetat deqena gidefom yibla alewa</td>\n",
       "      <td>unk</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id  \\\n",
       "0  IL5_SN_000370_20161010_H0T00365S   \n",
       "1  IL5_SN_000370_20150728_G0T000F6B   \n",
       "\n",
       "                                        text_data class_type  topic1  topic2  \\\n",
       "0  We have Constitution but not Constitutionality        unk   False   False   \n",
       "1           IMO adetat deqena gidefom yibla alewa        unk   False   False   \n",
       "\n",
       "   topic3  topic4  \n",
       "0   False   False  \n",
       "1   False   False  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat( [reset_data.iloc[:,0:3], result], axis=1)\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assigning class lables based on the binary classification result\n",
    "result.loc[(result['topic1']==True) | (result['topic2']==True)  , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (4573, 8)\n",
      "shape after filtering unknown docs (1447, 8)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.class_type !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          180          446\n",
      "Actual--           44          777\n",
      "precision 0.803571428571\n",
      "recall 0.287539936102\n",
      "F1_score 0.423529411765\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for the overall classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.class_type, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm)\n",
    "\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision)\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall)\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           12            0\n",
      "Actual--            8            0\n",
      "precision 0.6\n",
      "recall 1.0\n",
      "F1_score 0.7499999999999999\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 20 based anchor word count\n",
    "\n",
    "top_docs = result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መሬት|መዓት').sum())\n",
    "\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "top15 = top_docs.iloc[0:20,:]\n",
    "\n",
    "cm=pd.DataFrame(confusion_matrix(top15.class_type, top15.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm)\n",
    "\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision)\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall)\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           17            5\n",
      "Actual--           10           18 \n",
      "\n",
      "precision 0.6296296296296297 \n",
      "\n",
      "recall 0.7727272727272727 \n",
      "\n",
      "F1_score 0.6938775510204083 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 50 based anchor word count\n",
    "\n",
    "top_docs = result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መሬት|መዓት').sum())\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "top15 = top_docs.iloc[0:50,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top15.class_type, top15.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020508_20151218_H0040MWHT</th>\n",
       "      <td>መንግስቲ ኢትዮጵያ ፣ ኣብ ኣዲስ ኣበባን ከባቢኣን ዝወጠኖ ናይ መሬት ልም...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20160205_H0040ME5F</th>\n",
       "      <td>ኣብ ክልል ጋምቤላ ኣብዚ ሕዚ እዋን ብዝተልዓለ ግጭትን ዕግርግርን ምኽንያ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20170210_H0T0036X1</th>\n",
       "      <td>ህወሓትን ሕቶ መሬትን ህዝቢ ትግራይ ንቓልሲ ክላዓዓልን ኣንፃር ገዛእቲ ደ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20160106_G0T000A4D</th>\n",
       "      <td>ሰሜን ኮርያ ፡ ሃይትሮጅን ቦምብ ፈቲና ሎሚ ኣብ ሰዓታት ቅድሚ ቀትሪ ምን...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020508_20160202_H0040MWHU</th>\n",
       "      <td>ኣብ ኢትዮጵያ ፣ ክልል ጋምቤላ ኣብ ዝተኻየደ ቀቢላዊ ግጭት  ሰባት ከምዝ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20161223_H0040MDS6</th>\n",
       "      <td>ኣብ ጃዊን ከባቢኣን ኣብ ልዕሊ ገዛኢ ስርዓት ኢህወደግ ሓያል ተቃውሞ ህዝ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20160326_H0040ME1K</th>\n",
       "      <td>ኣብ ዞባ ምዕራብ ዝርከቡ ኣመሓደርቲ ብዝፈፀምዎ ግዑዝይ ኣሰራርሓ ፣ ህዝቢ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020060_20160825_G0040KICM</th>\n",
       "      <td>ኣትለት ፈይሳ ለሊሳ ንመግለጺ መንግስቲ ብምእማን ከይምለስ ስድራቤቱ ኣጠን...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_WL_020506_20160406_H0040ME0W</th>\n",
       "      <td>ኣብ ክልል ትግራይ ዞባ ሰሜናዊ ምዕራብ ዝነብር ሕብረተሰብ ብወተሃደራት ገ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020508_20160418_H0040MWHW</th>\n",
       "      <td>ዕጡቓት ደቡብ ሱዳን ፣ ዶብ ብምጥሓስ ኣብ ዘካየድዎ መጥቃዕቲ  ኢትዮጵያው...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL5_NW_020508_20151218_H0040MWHT  መንግስቲ ኢትዮጵያ ፣ ኣብ ኣዲስ ኣበባን ከባቢኣን ዝወጠኖ ናይ መሬት ልም...   \n",
       "IL5_WL_020506_20160205_H0040ME5F  ኣብ ክልል ጋምቤላ ኣብዚ ሕዚ እዋን ብዝተልዓለ ግጭትን ዕግርግርን ምኽንያ...   \n",
       "IL5_SN_000370_20170210_H0T0036X1  ህወሓትን ሕቶ መሬትን ህዝቢ ትግራይ ንቓልሲ ክላዓዓልን ኣንፃር ገዛእቲ ደ...   \n",
       "IL5_SN_000370_20160106_G0T000A4D  ሰሜን ኮርያ ፡ ሃይትሮጅን ቦምብ ፈቲና ሎሚ ኣብ ሰዓታት ቅድሚ ቀትሪ ምን...   \n",
       "IL5_NW_020508_20160202_H0040MWHU  ኣብ ኢትዮጵያ ፣ ክልል ጋምቤላ ኣብ ዝተኻየደ ቀቢላዊ ግጭት  ሰባት ከምዝ...   \n",
       "IL5_WL_020506_20161223_H0040MDS6  ኣብ ጃዊን ከባቢኣን ኣብ ልዕሊ ገዛኢ ስርዓት ኢህወደግ ሓያል ተቃውሞ ህዝ...   \n",
       "IL5_WL_020506_20160326_H0040ME1K  ኣብ ዞባ ምዕራብ ዝርከቡ ኣመሓደርቲ ብዝፈፀምዎ ግዑዝይ ኣሰራርሓ ፣ ህዝቢ...   \n",
       "IL5_NW_020060_20160825_G0040KICM  ኣትለት ፈይሳ ለሊሳ ንመግለጺ መንግስቲ ብምእማን ከይምለስ ስድራቤቱ ኣጠን...   \n",
       "IL5_WL_020506_20160406_H0040ME0W  ኣብ ክልል ትግራይ ዞባ ሰሜናዊ ምዕራብ ዝነብር ሕብረተሰብ ብወተሃደራት ገ...   \n",
       "IL5_NW_020508_20160418_H0040MWHW  ዕጡቓት ደቡብ ሱዳን ፣ ዶብ ብምጥሓስ ኣብ ዘካየድዎ መጥቃዕቲ  ኢትዮጵያው...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL5_NW_020508_20151218_H0040MWHT      5  \n",
       "IL5_WL_020506_20160205_H0040ME5F      5  \n",
       "IL5_SN_000370_20170210_H0T0036X1      2  \n",
       "IL5_SN_000370_20160106_G0T000A4D      2  \n",
       "IL5_NW_020508_20160202_H0040MWHU      2  \n",
       "IL5_WL_020506_20161223_H0040MDS6      2  \n",
       "IL5_WL_020506_20160326_H0040ME1K      2  \n",
       "IL5_NW_020060_20160825_G0040KICM      2  \n",
       "IL5_WL_020506_20160406_H0040ME0W      1  \n",
       "IL5_NW_020508_20160418_H0040MWHW      1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 1\n",
    "\n",
    "topic1_docs_id = result.loc[(result.class_type==\"indomain\")&(result.topic1==True),\"doc_id\"]\n",
    "topic1_docs_id.shape\n",
    "topic1_docs =raw_data.loc[raw_data.doc_id.isin(topic1_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic1_docs= topic1_docs.set_index('doc_id')\n",
    "topic1_docs['count'] = topic1_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መሬት').sum())\n",
    "topic1_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic1_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL5_SN_000370_20170203_H0T0036QB</th>\n",
       "      <td>መስጊድ ። ታሪኻዊት ከተማ ናቕፋ ። መዓት መዳፍዕን ፣ ተወንጨፍትን ፣ ደ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20160701_G0040IC0L</th>\n",
       "      <td>ኤርትራ ፦ ሕቡራት ሃገራት ንጉዳይ ኤርትራ ተግባራዊ ውሳኔ ክግበረሉ ዘኽእ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20150611_G0040EZAF</th>\n",
       "      <td>ሓርነትን ጭቆናን ኣማኑኤል ሳህለ መብዛሕትኡ ናይ መውጽእ ሓራ ውድብ ወይ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20151124_G0040IC2Q</th>\n",
       "      <td>እዋናዊ ሃለዋት ኤርትራ ዕለት  ሕዳር  ዓም ይ ሓድሽ ሓበሬታ ንእዋናዊ ሃ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20160610_G0040IC0W</th>\n",
       "      <td>ኣገዳሲ መግለጺ ውህደት ደሞክራሲያዊት ኤርትራ ኣብ ኢጣልያ ዕለት  ኣገዳሲ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020490_20160523_H0040MC9X</th>\n",
       "      <td>ንሰብኣዊ መሰላት ዝጣበቕ ጉጅለ ግፍዕታት ዳዒሽ ኣብ ሊብያ ሰኒዱ ሓደ ፍሉ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020079_20160609_G0040IC0X</th>\n",
       "      <td>ምጽራይ ሕሃ ገበናት ኣንጻር ሰብኣውነት ኣብ ኤርትራ ረኺቡ ጀኔቫ  ሰ ነ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20160908_G0040KJ4B</th>\n",
       "      <td>እዋናዊ ዜናታት – ጀርመን ንቅሉዕ ጥሕሰት ሰብኣዊ መሰላት ምልካዊ ስርዓት...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20150613_G0040EZAB</th>\n",
       "      <td>ስርዓት ህግደፍ ካብ ህዝቢ ኣብ ውሽጢን ወጻእን ዝወርዶ ዘሎ ተጽዕኖ መኣዝ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL5_NW_020062_20160904_G0040KJ4K</th>\n",
       "      <td>ዕድመ ተሳታፍነት ናብ ህዝባዊ ሰሚናር ደንቨር ኮሎራዶ ዛዕባ ሰሚናር ፡ ሓ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL5_SN_000370_20170203_H0T0036QB  መስጊድ ። ታሪኻዊት ከተማ ናቕፋ ። መዓት መዳፍዕን ፣ ተወንጨፍትን ፣ ደ...   \n",
       "IL5_NW_020079_20160701_G0040IC0L  ኤርትራ ፦ ሕቡራት ሃገራት ንጉዳይ ኤርትራ ተግባራዊ ውሳኔ ክግበረሉ ዘኽእ...   \n",
       "IL5_NW_020062_20150611_G0040EZAF  ሓርነትን ጭቆናን ኣማኑኤል ሳህለ መብዛሕትኡ ናይ መውጽእ ሓራ ውድብ ወይ ...   \n",
       "IL5_NW_020079_20151124_G0040IC2Q  እዋናዊ ሃለዋት ኤርትራ ዕለት  ሕዳር  ዓም ይ ሓድሽ ሓበሬታ ንእዋናዊ ሃ...   \n",
       "IL5_NW_020079_20160610_G0040IC0W  ኣገዳሲ መግለጺ ውህደት ደሞክራሲያዊት ኤርትራ ኣብ ኢጣልያ ዕለት  ኣገዳሲ...   \n",
       "IL5_NW_020490_20160523_H0040MC9X  ንሰብኣዊ መሰላት ዝጣበቕ ጉጅለ ግፍዕታት ዳዒሽ ኣብ ሊብያ ሰኒዱ ሓደ ፍሉ...   \n",
       "IL5_NW_020079_20160609_G0040IC0X  ምጽራይ ሕሃ ገበናት ኣንጻር ሰብኣውነት ኣብ ኤርትራ ረኺቡ ጀኔቫ  ሰ ነ ...   \n",
       "IL5_NW_020062_20160908_G0040KJ4B  እዋናዊ ዜናታት – ጀርመን ንቅሉዕ ጥሕሰት ሰብኣዊ መሰላት ምልካዊ ስርዓት...   \n",
       "IL5_NW_020062_20150613_G0040EZAB  ስርዓት ህግደፍ ካብ ህዝቢ ኣብ ውሽጢን ወጻእን ዝወርዶ ዘሎ ተጽዕኖ መኣዝ...   \n",
       "IL5_NW_020062_20160904_G0040KJ4K  ዕድመ ተሳታፍነት ናብ ህዝባዊ ሰሚናር ደንቨር ኮሎራዶ ዛዕባ ሰሚናር ፡ ሓ...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL5_SN_000370_20170203_H0T0036QB      1  \n",
       "IL5_NW_020079_20160701_G0040IC0L      0  \n",
       "IL5_NW_020062_20150611_G0040EZAF      0  \n",
       "IL5_NW_020079_20151124_G0040IC2Q      0  \n",
       "IL5_NW_020079_20160610_G0040IC0W      0  \n",
       "IL5_NW_020490_20160523_H0040MC9X      0  \n",
       "IL5_NW_020079_20160609_G0040IC0X      0  \n",
       "IL5_NW_020062_20160908_G0040KJ4B      0  \n",
       "IL5_NW_020062_20150613_G0040EZAB      0  \n",
       "IL5_NW_020062_20160904_G0040KJ4K      0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 2\n",
    "\n",
    "\n",
    "topic2_docs_id = result.loc[(result.class_type==\"indomain\")&(result.topic2==True),\"doc_id\"]\n",
    "topic2_docs =raw_data.loc[raw_data.doc_id.isin(topic2_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic2_docs= topic2_docs.set_index('doc_id')\n",
    "topic2_docs['count'] = topic2_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መዓት').sum())\n",
    "topic2_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic2_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidedlda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-9c2c5b0e4641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mguidedlda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mguidedlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGuidedLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'guidedlda'"
     ]
    }
   ],
   "source": [
    "# Alternative approach guided-LDA\n",
    "# reference links below\n",
    "#1) https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\n",
    "#2)https://github.com/vi3k6i5/GuidedLDA\n",
    "\n",
    "import numpy as np\n",
    "import guidedlda\n",
    "model = guidedlda.GuidedLDA(n_topics=15, n_iter=4000, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document-Term Matrix-(count matrix)\n",
    "vectorizer2 = CountVectorizer(encoding='utf-8',stop_words=None,max_df=0.997, min_df=0.001, binary=False,lowercase=True)\n",
    "doc_word_mat2 = vectorizer2.fit_transform(reset_data.text_data)\n",
    "doc_word_mat2 = doc_word_mat2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words2 = list(np.asarray(vectorizer2.get_feature_names()))\n",
    "word2id = dict((v, idx) for idx, v in enumerate(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "seed_topic_list =[[\"መሬት\"],[\"መዓት\"]]\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 4573\n",
      "INFO:guidedlda:vocab_size: 6768\n",
      "INFO:guidedlda:n_words: 224416\n",
      "INFO:guidedlda:n_topics: 15\n",
      "INFO:guidedlda:n_iter: 4000\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -2655148\n",
      "INFO:guidedlda:<20> log likelihood: -1852300\n",
      "INFO:guidedlda:<40> log likelihood: -1791466\n",
      "INFO:guidedlda:<60> log likelihood: -1767883\n",
      "INFO:guidedlda:<80> log likelihood: -1754333\n",
      "INFO:guidedlda:<100> log likelihood: -1744922\n",
      "INFO:guidedlda:<120> log likelihood: -1740384\n",
      "INFO:guidedlda:<140> log likelihood: -1736346\n",
      "INFO:guidedlda:<160> log likelihood: -1731429\n",
      "INFO:guidedlda:<180> log likelihood: -1728390\n",
      "INFO:guidedlda:<200> log likelihood: -1726258\n",
      "INFO:guidedlda:<220> log likelihood: -1724994\n",
      "INFO:guidedlda:<240> log likelihood: -1724030\n",
      "INFO:guidedlda:<260> log likelihood: -1722795\n",
      "INFO:guidedlda:<280> log likelihood: -1721400\n",
      "INFO:guidedlda:<300> log likelihood: -1721540\n",
      "INFO:guidedlda:<320> log likelihood: -1720325\n",
      "INFO:guidedlda:<340> log likelihood: -1720459\n",
      "INFO:guidedlda:<360> log likelihood: -1719991\n",
      "INFO:guidedlda:<380> log likelihood: -1720104\n",
      "INFO:guidedlda:<400> log likelihood: -1718808\n",
      "INFO:guidedlda:<420> log likelihood: -1718521\n",
      "INFO:guidedlda:<440> log likelihood: -1717907\n",
      "INFO:guidedlda:<460> log likelihood: -1717176\n",
      "INFO:guidedlda:<480> log likelihood: -1717095\n",
      "INFO:guidedlda:<500> log likelihood: -1717052\n",
      "INFO:guidedlda:<520> log likelihood: -1717206\n",
      "INFO:guidedlda:<540> log likelihood: -1716166\n",
      "INFO:guidedlda:<560> log likelihood: -1715565\n",
      "INFO:guidedlda:<580> log likelihood: -1715526\n",
      "INFO:guidedlda:<600> log likelihood: -1716242\n",
      "INFO:guidedlda:<620> log likelihood: -1715923\n",
      "INFO:guidedlda:<640> log likelihood: -1715759\n",
      "INFO:guidedlda:<660> log likelihood: -1715314\n",
      "INFO:guidedlda:<680> log likelihood: -1716004\n",
      "INFO:guidedlda:<700> log likelihood: -1715993\n",
      "INFO:guidedlda:<720> log likelihood: -1715460\n",
      "INFO:guidedlda:<740> log likelihood: -1714770\n",
      "INFO:guidedlda:<760> log likelihood: -1714861\n",
      "INFO:guidedlda:<780> log likelihood: -1713226\n",
      "INFO:guidedlda:<800> log likelihood: -1714182\n",
      "INFO:guidedlda:<820> log likelihood: -1714747\n",
      "INFO:guidedlda:<840> log likelihood: -1714092\n",
      "INFO:guidedlda:<860> log likelihood: -1713886\n",
      "INFO:guidedlda:<880> log likelihood: -1713509\n",
      "INFO:guidedlda:<900> log likelihood: -1713950\n",
      "INFO:guidedlda:<920> log likelihood: -1713357\n",
      "INFO:guidedlda:<940> log likelihood: -1713564\n",
      "INFO:guidedlda:<960> log likelihood: -1713347\n",
      "INFO:guidedlda:<980> log likelihood: -1714158\n",
      "INFO:guidedlda:<1000> log likelihood: -1713276\n",
      "INFO:guidedlda:<1020> log likelihood: -1712826\n",
      "INFO:guidedlda:<1040> log likelihood: -1713012\n",
      "INFO:guidedlda:<1060> log likelihood: -1712801\n",
      "INFO:guidedlda:<1080> log likelihood: -1712350\n",
      "INFO:guidedlda:<1100> log likelihood: -1712419\n",
      "INFO:guidedlda:<1120> log likelihood: -1712975\n",
      "INFO:guidedlda:<1140> log likelihood: -1712654\n",
      "INFO:guidedlda:<1160> log likelihood: -1712725\n",
      "INFO:guidedlda:<1180> log likelihood: -1712727\n",
      "INFO:guidedlda:<1200> log likelihood: -1712124\n",
      "INFO:guidedlda:<1220> log likelihood: -1712853\n",
      "INFO:guidedlda:<1240> log likelihood: -1712038\n",
      "INFO:guidedlda:<1260> log likelihood: -1712226\n",
      "INFO:guidedlda:<1280> log likelihood: -1711467\n",
      "INFO:guidedlda:<1300> log likelihood: -1711860\n",
      "INFO:guidedlda:<1320> log likelihood: -1711421\n",
      "INFO:guidedlda:<1340> log likelihood: -1711893\n",
      "INFO:guidedlda:<1360> log likelihood: -1711808\n",
      "INFO:guidedlda:<1380> log likelihood: -1711552\n",
      "INFO:guidedlda:<1400> log likelihood: -1711298\n",
      "INFO:guidedlda:<1420> log likelihood: -1711890\n",
      "INFO:guidedlda:<1440> log likelihood: -1712605\n",
      "INFO:guidedlda:<1460> log likelihood: -1712415\n",
      "INFO:guidedlda:<1480> log likelihood: -1711137\n",
      "INFO:guidedlda:<1500> log likelihood: -1712012\n",
      "INFO:guidedlda:<1520> log likelihood: -1711374\n",
      "INFO:guidedlda:<1540> log likelihood: -1711787\n",
      "INFO:guidedlda:<1560> log likelihood: -1711738\n",
      "INFO:guidedlda:<1580> log likelihood: -1711463\n",
      "INFO:guidedlda:<1600> log likelihood: -1711287\n",
      "INFO:guidedlda:<1620> log likelihood: -1711235\n",
      "INFO:guidedlda:<1640> log likelihood: -1711348\n",
      "INFO:guidedlda:<1660> log likelihood: -1711202\n",
      "INFO:guidedlda:<1680> log likelihood: -1710892\n",
      "INFO:guidedlda:<1700> log likelihood: -1711416\n",
      "INFO:guidedlda:<1720> log likelihood: -1711579\n",
      "INFO:guidedlda:<1740> log likelihood: -1711403\n",
      "INFO:guidedlda:<1760> log likelihood: -1711414\n",
      "INFO:guidedlda:<1780> log likelihood: -1711740\n",
      "INFO:guidedlda:<1800> log likelihood: -1711274\n",
      "INFO:guidedlda:<1820> log likelihood: -1711521\n",
      "INFO:guidedlda:<1840> log likelihood: -1711208\n",
      "INFO:guidedlda:<1860> log likelihood: -1711273\n",
      "INFO:guidedlda:<1880> log likelihood: -1710962\n",
      "INFO:guidedlda:<1900> log likelihood: -1711374\n",
      "INFO:guidedlda:<1920> log likelihood: -1712119\n",
      "INFO:guidedlda:<1940> log likelihood: -1711560\n",
      "INFO:guidedlda:<1960> log likelihood: -1712735\n",
      "INFO:guidedlda:<1980> log likelihood: -1712103\n",
      "INFO:guidedlda:<2000> log likelihood: -1713174\n",
      "INFO:guidedlda:<2020> log likelihood: -1713301\n",
      "INFO:guidedlda:<2040> log likelihood: -1713132\n",
      "INFO:guidedlda:<2060> log likelihood: -1712554\n",
      "INFO:guidedlda:<2080> log likelihood: -1712177\n",
      "INFO:guidedlda:<2100> log likelihood: -1711679\n",
      "INFO:guidedlda:<2120> log likelihood: -1712336\n",
      "INFO:guidedlda:<2140> log likelihood: -1711952\n",
      "INFO:guidedlda:<2160> log likelihood: -1712002\n",
      "INFO:guidedlda:<2180> log likelihood: -1713007\n",
      "INFO:guidedlda:<2200> log likelihood: -1712059\n",
      "INFO:guidedlda:<2220> log likelihood: -1712482\n",
      "INFO:guidedlda:<2240> log likelihood: -1711561\n",
      "INFO:guidedlda:<2260> log likelihood: -1711962\n",
      "INFO:guidedlda:<2280> log likelihood: -1712067\n",
      "INFO:guidedlda:<2300> log likelihood: -1712328\n",
      "INFO:guidedlda:<2320> log likelihood: -1712161\n",
      "INFO:guidedlda:<2340> log likelihood: -1712610\n",
      "INFO:guidedlda:<2360> log likelihood: -1712242\n",
      "INFO:guidedlda:<2380> log likelihood: -1711946\n",
      "INFO:guidedlda:<2400> log likelihood: -1711654\n",
      "INFO:guidedlda:<2420> log likelihood: -1712428\n",
      "INFO:guidedlda:<2440> log likelihood: -1712325\n",
      "INFO:guidedlda:<2460> log likelihood: -1711279\n",
      "INFO:guidedlda:<2480> log likelihood: -1711233\n",
      "INFO:guidedlda:<2500> log likelihood: -1711678\n",
      "INFO:guidedlda:<2520> log likelihood: -1711216\n",
      "INFO:guidedlda:<2540> log likelihood: -1711316\n",
      "INFO:guidedlda:<2560> log likelihood: -1711230\n",
      "INFO:guidedlda:<2580> log likelihood: -1711671\n",
      "INFO:guidedlda:<2600> log likelihood: -1711630\n",
      "INFO:guidedlda:<2620> log likelihood: -1711786\n",
      "INFO:guidedlda:<2640> log likelihood: -1711959\n",
      "INFO:guidedlda:<2660> log likelihood: -1711479\n",
      "INFO:guidedlda:<2680> log likelihood: -1711998\n",
      "INFO:guidedlda:<2700> log likelihood: -1711461\n",
      "INFO:guidedlda:<2720> log likelihood: -1711720\n",
      "INFO:guidedlda:<2740> log likelihood: -1711264\n",
      "INFO:guidedlda:<2760> log likelihood: -1711250\n",
      "INFO:guidedlda:<2780> log likelihood: -1711527\n",
      "INFO:guidedlda:<2800> log likelihood: -1711557\n",
      "INFO:guidedlda:<2820> log likelihood: -1711572\n",
      "INFO:guidedlda:<2840> log likelihood: -1711855\n",
      "INFO:guidedlda:<2860> log likelihood: -1711257\n",
      "INFO:guidedlda:<2880> log likelihood: -1711822\n",
      "INFO:guidedlda:<2900> log likelihood: -1711360\n",
      "INFO:guidedlda:<2920> log likelihood: -1711481\n",
      "INFO:guidedlda:<2940> log likelihood: -1712262\n",
      "INFO:guidedlda:<2960> log likelihood: -1711846\n",
      "INFO:guidedlda:<2980> log likelihood: -1711504\n",
      "INFO:guidedlda:<3000> log likelihood: -1711019\n",
      "INFO:guidedlda:<3020> log likelihood: -1711332\n",
      "INFO:guidedlda:<3040> log likelihood: -1711740\n",
      "INFO:guidedlda:<3060> log likelihood: -1710980\n",
      "INFO:guidedlda:<3080> log likelihood: -1711555\n",
      "INFO:guidedlda:<3100> log likelihood: -1710996\n",
      "INFO:guidedlda:<3120> log likelihood: -1710185\n",
      "INFO:guidedlda:<3140> log likelihood: -1711469\n",
      "INFO:guidedlda:<3160> log likelihood: -1710884\n",
      "INFO:guidedlda:<3180> log likelihood: -1710978\n",
      "INFO:guidedlda:<3200> log likelihood: -1710589\n",
      "INFO:guidedlda:<3220> log likelihood: -1710620\n",
      "INFO:guidedlda:<3240> log likelihood: -1710589\n",
      "INFO:guidedlda:<3260> log likelihood: -1710453\n",
      "INFO:guidedlda:<3280> log likelihood: -1711177\n",
      "INFO:guidedlda:<3300> log likelihood: -1710693\n",
      "INFO:guidedlda:<3320> log likelihood: -1709960\n",
      "INFO:guidedlda:<3340> log likelihood: -1710598\n",
      "INFO:guidedlda:<3360> log likelihood: -1710097\n",
      "INFO:guidedlda:<3380> log likelihood: -1709927\n",
      "INFO:guidedlda:<3400> log likelihood: -1709878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:<3420> log likelihood: -1710527\n",
      "INFO:guidedlda:<3440> log likelihood: -1710704\n",
      "INFO:guidedlda:<3460> log likelihood: -1710245\n",
      "INFO:guidedlda:<3480> log likelihood: -1710002\n",
      "INFO:guidedlda:<3500> log likelihood: -1710130\n",
      "INFO:guidedlda:<3520> log likelihood: -1710016\n",
      "INFO:guidedlda:<3540> log likelihood: -1709944\n",
      "INFO:guidedlda:<3560> log likelihood: -1709733\n",
      "INFO:guidedlda:<3580> log likelihood: -1709935\n",
      "INFO:guidedlda:<3600> log likelihood: -1709851\n",
      "INFO:guidedlda:<3620> log likelihood: -1709696\n",
      "INFO:guidedlda:<3640> log likelihood: -1709887\n",
      "INFO:guidedlda:<3660> log likelihood: -1709759\n",
      "INFO:guidedlda:<3680> log likelihood: -1709181\n",
      "INFO:guidedlda:<3700> log likelihood: -1709471\n",
      "INFO:guidedlda:<3720> log likelihood: -1709355\n",
      "INFO:guidedlda:<3740> log likelihood: -1709614\n",
      "INFO:guidedlda:<3760> log likelihood: -1709041\n",
      "INFO:guidedlda:<3780> log likelihood: -1708473\n",
      "INFO:guidedlda:<3800> log likelihood: -1708668\n",
      "INFO:guidedlda:<3820> log likelihood: -1708205\n",
      "INFO:guidedlda:<3840> log likelihood: -1708035\n",
      "INFO:guidedlda:<3860> log likelihood: -1707707\n",
      "INFO:guidedlda:<3880> log likelihood: -1708589\n",
      "INFO:guidedlda:<3900> log likelihood: -1708932\n",
      "INFO:guidedlda:<3920> log likelihood: -1709116\n",
      "INFO:guidedlda:<3940> log likelihood: -1709985\n",
      "INFO:guidedlda:<3960> log likelihood: -1709231\n",
      "INFO:guidedlda:<3980> log likelihood: -1709946\n",
      "INFO:guidedlda:<3999> log likelihood: -1709004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x270b71e9d68>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 170\n",
      "1 309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "vlist = []\n",
    "for i in range(len(doc_topic)):\n",
    "    vlist.append(doc_topic[i].argmax())\n",
    "vlist\n",
    "index_list=np.where(np.asarray(vlist) == 0)[0].tolist()\n",
    "print(\"0\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 1)[0].tolist())\n",
    "print(\"1\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 2)[0].tolist())\n",
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1=reset_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1.loc[index_list,'predicted_class_label']= \"indomain\"\n",
    "\n",
    "temp1.loc[(temp1['predicted_class_label']!=\"indomain\") , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (4573, 5)\n",
      "shape after filtering unknown docs (1447, 5)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",temp1.shape )\n",
    "temp1 = temp1.loc[temp1.class_type !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",temp1.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          116          510\n",
      "Actual--          257          564\n",
      "\n",
      "\n",
      "precision 0.3109919571045576 \n",
      "\n",
      "recall 0.1853035143769968 \n",
      "\n",
      "F1_score 0.23223223223223224 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(temp1.class_type, temp1.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\")\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1447, 6)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline\n",
    "\n",
    "#'መሬት|መዓት|ሓደጋ|ድርቂ|ድርቅን|ውሕጅ'\n",
    "            \n",
    "baseline_temp = reset_data\n",
    "baseline_temp['count'] = baseline_temp['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'መሬት|መዓት|ሓደጋ|ድርቂ|ድርቅን|ውሕጅ').sum())\n",
    "\n",
    "# assigning class lables based on the binary classification result\n",
    "baseline_temp.loc[baseline_temp['count'] >0, 'predicted_class_label'] = \"indomain\"\n",
    "baseline_temp.loc[baseline_temp['count'] <=0, 'predicted_class_label'] = \"nondomain\"\n",
    "baseline_temp['class_type'].unique()\n",
    "\n",
    "# filter unknown class\n",
    "\n",
    "baseline_temp = baseline_temp.loc[baseline_temp.class_type !=\"unk\"]\n",
    "\n",
    "baseline_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           98          528\n",
      "Actual--           20          801 \n",
      "\n",
      "precision 0.830508474576 \n",
      "\n",
      "recall 0.156549520767 \n",
      "\n",
      "F1_score 0.263440860215 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf=pd.DataFrame(confusion_matrix(baseline_temp.class_type, baseline_temp.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "print(cf,\"\\n\")\n",
    "\n",
    "\n",
    "precision = cf.loc[\"Actual++\",\"Predicted++\"]/(cf.loc[\"Actual++\",\"Predicted++\"] + cf.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cf.loc[\"Actual++\",\"Predicted++\"]/(cf.loc[\"Actual++\",\"Predicted++\"] + cf.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
