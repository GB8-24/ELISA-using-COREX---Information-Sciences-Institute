{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CorEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "code_folding": [
     25,
     599,
     603,
     607,
     617,
     623
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"CorEx Hierarchical Topic Models\n",
    "Use the principle of Total Cor-relation Explanation (CorEx) to construct\n",
    "hierarchical topic models. This module is specially designed for sparse count\n",
    "data and implements semi-supervision via the information bottleneck.\n",
    "Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical\n",
    "Representations of High-Dimensional Data.\" AISTATS, 2015.\n",
    "Gallagher et al. \"Anchored Correlation Explanation: Topic Modeling with Minimal\n",
    "Domain Knowledge.\" TACL, 2017.\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu)\n",
    "Ryan J. Gallagher\n",
    "David Kale\n",
    "Lily Fierro\n",
    "License: Apache V2\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "code_folding": [
     3
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read input data which is in .txt file line by line, split by tab(\\t) to a list\n",
    "list_data=[]\n",
    "with open('C:\\\\Users\\\\Gaurav\\\\Desktop\\\\ISI forms\\\\Project\\\\il6_original.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the columns of datframe\n",
    "raw_data = pd.DataFrame(list_data,columns=['doc_id','text_data','class_type','additional']) \n",
    "#drop if any additional columns gets created as part of reading process\n",
    "raw_data=raw_data.drop('additional',axis=1)\n",
    "# strip columns for leading and trailing white spaces\n",
    "raw_data['doc_id']=raw_data.doc_id.str.strip()\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "# n docs x m attributes\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[eval_incident, unk, nondomain, indomain]\n",
       "Categories (4, object): [eval_incident, unk, nondomain, indomain]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set doc_id as index\n",
    "raw_data= raw_data.set_index('doc_id')\n",
    "# change \"class_type\" column to \"categorical\" datatype\n",
    "raw_data['class_type'] = raw_data['class_type'].astype('category')\n",
    "raw_data['class_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eval_incident - This should be considered as part of “indomain”\n",
    "raw_data.loc[raw_data['class_type']==\"eval_incident\", 'class_type'] = \"indomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "\n",
    "# RT (Retweet) keyword removal\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('RT','',x))\n",
    "\n",
    "#punctuation removal on 'text_data' column\n",
    "#print(string.punctuation)\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: regex.sub('', x))\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "\n",
    "# remove @names mentioned as part of tweets\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('\\@[a-zA-Z0-9]+','',x))\n",
    "\n",
    "# Remove emoji's from the documents\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))\n",
    "\n",
    "# Date Removal/ number removal from the documents\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\\d]+','',x))\n",
    "# strip whitespaces again \n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "\n",
    "# Convert the letters to lower case\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: x.lower())\n",
    "\n",
    "# calculate length again\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_SN_000370_20170330_H0T003OA1</th>\n",
       "      <td>こっちがウサギタロッ</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_SN_000370_20170329_H0T003FRR</th>\n",
       "      <td>ゴスロリバイブルの方はmana様が載ってるからまだ買ってたんだけど、ケラまでは買ってなかった...</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_SN_000370_20170330_H0T003OA1                                         こっちがウサギタロッ   \n",
       "IL6_SN_000370_20170329_H0T003FRR  ゴスロリバイブルの方はmana様が載ってるからまだ買ってたんだけど、ケラまでは買ってなかった...   \n",
       "\n",
       "                                 class_type  length  \n",
       "doc_id                                               \n",
       "IL6_SN_000370_20170330_H0T003OA1        unk       1  \n",
       "IL6_SN_000370_20170329_H0T003FRR        unk       1  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents with no words after pre-processing\n",
    "raw_data = raw_data.loc[raw_data.length>0]\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 3)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing documents with less than 5 words\n",
    "raw_data = raw_data.loc[raw_data.length>5]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk          3901\n",
      "indomain     1806\n",
      "nondomain     164\n",
      "Name: class_type, dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-573aab8eb9e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Document count across 3 class labels (unk,nondomain,indomain,eval_incident)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'class_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(x, bins, range, normed, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, hold, data, **kwargs)\u001b[0m\n\u001b[0;32m   3079\u001b[0m                       \u001b[0mhisttype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhisttype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3080\u001b[0m                       \u001b[0mrwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3081\u001b[1;33m                       stacked=stacked, data=data, **kwargs)\n\u001b[0m\u001b[0;32m   3082\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3083\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1896\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1897\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1898\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1899\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1900\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   6178\u001b[0m             \u001b[0mxmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6179\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6180\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6181\u001b[0m                     \u001b[0mxmin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6182\u001b[0m                     \u001b[0mxmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "# Document count across 3 class labels (unk,nondomain,indomain,eval_incident)\n",
    "print(raw_data.class_type.value_counts())\n",
    "plt.hist('class_type',data=raw_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the formatted data to a new text file for future reference\n",
    "\n",
    "reset_data=raw_data.reset_index()\n",
    "reset_data.to_csv('C:\\\\Users\\\\Gaurav\\\\Desktop\\\\ISI forms\\\\Project\\\\Oromo.txt', header=True, index=False, sep='\\t', mode='w',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>indomain</th>\n",
       "      <td>1806.0</td>\n",
       "      <td>35.859911</td>\n",
       "      <td>107.356458</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nondomain</th>\n",
       "      <td>164.0</td>\n",
       "      <td>42.073171</td>\n",
       "      <td>112.330503</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>684.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unk</th>\n",
       "      <td>3901.0</td>\n",
       "      <td>125.700333</td>\n",
       "      <td>344.687689</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6062.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length                                                       \n",
       "             count        mean         std  min   25%   50%   75%     max\n",
       "class_type                                                               \n",
       "indomain    1806.0   35.859911  107.356458  6.0  10.0  12.0  14.0   981.0\n",
       "nondomain    164.0   42.073171  112.330503  6.0  11.0  13.0  14.0   684.0\n",
       "unk         3901.0  125.700333  344.687689  6.0   9.0  14.0  27.0  6062.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic stats to understand document length across 4 class labels\n",
    "raw_data.groupby('class_type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 8360)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Term matrix (binary matrix) with max_df =0.995, min_df =0.001\n",
    "vectorizer = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=True)\n",
    "doc_word_mat = vectorizer.fit_transform(reset_data.text_data)\n",
    "doc_word_mat = ss.csr_matrix(doc_word_mat)\n",
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x1e967afeda0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the CorEx topic model with 25 topics\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=4000, verbose=False, seed=3192)\n",
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: qabsoo,ummata,of,ilmaan,oromoon,diina,bilisummaa,seenaa,oromoof,saba\n",
      "1: akka,kana,jiru,isaa,irraa,kun,keessa,isaanii,itti,jira\n",
      "2: garuu,nama,miti,inni,jedhee,harka,jedhanii,ittiin,biraa,jecha\n",
      "3: abbaa,osoo,gadi,qofa,ni,hundi,hunda,ifatti,ammas,garaa\n",
      "4: warra,warri,nutti,beeku,koo,jirti,ani,natti,jechaa,kiyya\n",
      "5: hin,oromoo,wal,keenya,karaa,dha,haa,nu,amma,jiruu\n",
      "6: wayyaanee,humna,wayyaaneen,waraana,irree,tarkaanfii,humni,qeerroon,mooraa,qeerroo\n",
      "7: qabu,biyya,isa,kanaaf,taane,ofii,siyaasaa,biyyaa,yaada,aadaa\n",
      "8: rakkoo,alaa,taʼeef,dandaʼu,biyyattii,tokkummaa,dhabuu,fakkaatu,hawaasaa,akkanaa\n",
      "9: bifa,jalatti,ajjeechaa,qabnu,qabaachuu,bittaa,baasuu,umrii,dimokraasii,bilisa\n",
      "10: mirga,taʼee,aangoo,biyyummaa,mara,ragaa,jirru,harkaa,dhaaba,lafaa\n",
      "11: irra,oromiyaa,turan,kanaa,dhimma,gama,namoota,guutuu,isaaf,as\n",
      "12: dura,sadarkaa,waggaa,waggoota,turee,barnootaa,dhaabbachuun,dhaabbilee,tajaajila,argatan\n",
      "13: fi,irratti,waliin,bara,haala,addaa,gaaffii,akkasumas,deebii,afaan\n",
      "14: sirna,barbaachisu,furmaata,daran,manneen,argamu,daandii,federaalaa,labsii,tarkaanfiin\n",
      "15: waan,malee,isaan,qaba,jedhu,qaban,ykn,hidhaa,namni,haalli\n",
      "16: mana,ganda,ang,oromo,ng,oromorevolution,aku,nak,yang,mo\n",
      "17: yeroo,bakka,akkuma,ammaa,kanatti,sochii,hedduun,guddaan,buʼaa,addaan\n",
      "18: kan,ture,taʼe,ol,erga,taʼuun,ala,darbe,sirni,kaleessa\n",
      "19: yoo,jechuun,maqaa,lafa,bulchiinsa,yakka,haaluma,argachuu,qabuun,darban\n",
      "20: bira,biyyoota,qabeenya,lamaan,qabdu,isaatti,ishee,adeemsa,alatti,lammaffaa\n",
      "21: mootummaa,kunis,waraanaa,jedhuun,hojiin,karoora,galii,gaggeeffamaa,humnoota,fbc\n",
      "22: gara,keessaa,naannoo,addunyaa,duraa,tokkoo,kuma,hamaa,haga,akkasuma\n",
      "23: booda,washington,dc,obbo,abdii,jiraatan,gochaa,dabalatee,miidhaa,jedhamee\n",
      "24: godina,aanaa,magaalaa,bahaa,lixaa,arsii,harargee,fxg,wallaggaa,fufee\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics(n_words=10)\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "#socho始aa socho始an socho始uun  socho始uu  \n",
    "anchor_words = [[\"hoongee\",\"hongee\"],[\"galaana\",\"galaanaa\"],[\"balaa\"]]\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=4000,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: aadaa,isaaniif,jirru,hongee,qofaa,dhuma,caalaatti,adeemsa,qabatanii,hoongee\n",
      "1: malee,of,oromoon,seenaa,gadi,harka,hunda,ni,saba,hundi\n",
      "2: balaa,fakkaatu,isaatiin,cunqursaa,dabree,mootummoota,faallaa,habashaa,odoo,qbo\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to the topics of anchored words\n",
    "#(in this case, we have words seeded for 3 topics, hence checking the classification for those topics) \n",
    "topic_classification=anchored_topic_model.labels[:,0:3] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the classification results for the first 3 topics (seeded)\n",
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL6_SN_000370_20170331_H0T003SSC</td>\n",
       "      <td>ai mana que dor de cabeça</td>\n",
       "      <td>unk</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL6_SN_000370_20160822_H0T0060G0</td>\n",
       "      <td>ilma ummatni oromoo itti boonuu qabu</td>\n",
       "      <td>indomain</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id                             text_data  \\\n",
       "0  IL6_SN_000370_20170331_H0T003SSC             ai mana que dor de cabeça   \n",
       "1  IL6_SN_000370_20160822_H0T0060G0  ilma ummatni oromoo itti boonuu qabu   \n",
       "\n",
       "  class_type  length  \n",
       "0        unk       6  \n",
       "1   indomain       6  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.reset_index(inplace=True)\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL6_SN_000370_20170331_H0T003SSC</td>\n",
       "      <td>ai mana que dor de cabeça</td>\n",
       "      <td>unk</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL6_SN_000370_20160822_H0T0060G0</td>\n",
       "      <td>ilma ummatni oromoo itti boonuu qabu</td>\n",
       "      <td>indomain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id                             text_data  \\\n",
       "0  IL6_SN_000370_20170331_H0T003SSC             ai mana que dor de cabeça   \n",
       "1  IL6_SN_000370_20160822_H0T0060G0  ilma ummatni oromoo itti boonuu qabu   \n",
       "\n",
       "  class_type  topic1  topic2  topic3  \n",
       "0        unk   False   False   False  \n",
       "1   indomain   False   False   False  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat( [reset_data.iloc[:,0:3], result], axis=1)\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assigning class lables based on the binary classification result\n",
    "result.loc[(result['topic1']==True) | (result['topic2']==True) |  (result['topic3']==True) , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False) & (result['topic3']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 7)\n",
      "shape after filtering unknown docs (1970, 7)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.class_type !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          121         1685\n",
      "Actual--           16          148 \n",
      "\n",
      "precision 0.883211678832 \n",
      "\n",
      "recall 0.0669988925803 \n",
      "\n",
      "F1_score 0.124549665466 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.class_type, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more Anchor words from chineaseroom translator\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "#socho始aa socho始an socho始uun  socho始uu  \n",
    "\n",
    "anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],\n",
    "                [\"balaa\",\"dhibee\",\"irraa\"],\n",
    "                [\"sochii\",\"lafaa\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=4000,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: qofa,waggoota,meeshaa,turee,kanas,isaatti,hongee,ammo,jennee,kkf\n",
      "1: guutuu,lolaa,galaana,galaanaa,dhimmi,baasuu,taʼanii,biraatiin,dimokraasii,dhaggeeffadhaa\n",
      "2: irraa,balaa,dhibee,oggaa,kenname,galmee,saʼa,karaan,labsa,jijjiirraa\n",
      "3: sochii,lafaa,barbaachisu,gaggeessaa,dhabuun,barbaada,adeemsa,olaanaa,lammiilee,warraaqsa\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to the topics of anchored words\n",
    "#(in this case, we have words seeded for 4 topics, hence checking the classification for those topics) \n",
    "topic_classification=anchored_topic_model.labels[:,0:4] # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Storing the classification results for the first 4 topics (seeded)\n",
    "result=pd.DataFrame(topic_classification,columns=[\"topic1\",\"topic2\",\"topic3\",\"topic4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL6_SN_000370_20170331_H0T003SSC</td>\n",
       "      <td>ai mana que dor de cabeça</td>\n",
       "      <td>unk</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL6_SN_000370_20160822_H0T0060G0</td>\n",
       "      <td>ilma ummatni oromoo itti boonuu qabu</td>\n",
       "      <td>indomain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id                             text_data  \\\n",
       "0  IL6_SN_000370_20170331_H0T003SSC             ai mana que dor de cabeça   \n",
       "1  IL6_SN_000370_20160822_H0T0060G0  ilma ummatni oromoo itti boonuu qabu   \n",
       "\n",
       "  class_type  topic1  topic2  topic3  topic4  \n",
       "0        unk   False   False   False   False  \n",
       "1   indomain   False   False   False   False  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat( [reset_data.iloc[:,0:3], result], axis=1)\n",
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assigning class lables based on the binary classification result\n",
    "result.loc[(result['topic1']==True) | (result['topic2']==True) |  (result['topic3']==True) |(result['topic4']==True) , 'predicted_class_label'] = \"indomain\"\n",
    "result.loc[(result['topic1']==False) & (result['topic2']==False) & (result['topic3']==False) & (result['topic4']==False)  , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 8)\n",
      "shape after filtering unknown docs (1970, 8)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",result.shape )\n",
    "result = result.loc[result.class_type !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",result.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding more words from chinease room translator \n",
      "\n",
      "          Predicted++  Predicted--\n",
      "Actual++          250         1556\n",
      "Actual--           27          137 \n",
      "\n",
      "precision 0.902527075812 \n",
      "\n",
      "recall 0.138427464009 \n",
      "\n",
      "F1_score 0.240038406145 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(result.class_type, result.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "print(\"After adding more words from chinease room translator \\n\" )\n",
    "print(cm,\"\\n\")\n",
    "\n",
    "\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           19            0\n",
      "Actual--            1            0 \n",
      "\n",
      "precision 0.95 \n",
      "\n",
      "recall 1.0 \n",
      "\n",
      "F1_score 0.974358974359 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 20 based anchor word count\n",
    "\n",
    "top_docs = result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum())\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "top15 = top_docs.iloc[0:20,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top15.class_type, top15.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           46            3\n",
      "Actual--            1            0 \n",
      "\n",
      "precision 0.978723404255 \n",
      "\n",
      "recall 0.938775510204 \n",
      "\n",
      "F1_score 0.958333333333 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 50 based anchor word count\n",
    "\n",
    "top_docs = result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum())\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "top15 = top_docs.iloc[0:50,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top15.class_type, top15.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20151002_H0040LX9Z</th>\n",
       "      <td>itoophiyaa keessatti laakkofsi namoota hongeef...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020537_20161206_H0040MR72</th>\n",
       "      <td>balaa hoongee naannolee shanitti uumameef deeg...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20170426_H0040MPVA</th>\n",
       "      <td>aanaalee godina guraagee balaan hoongee mudate...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020505_20150902_H0040MO8I</th>\n",
       "      <td>ijoo dubbii abo ummatoota beela irraa baraaruu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160205_H0040LX8T</th>\n",
       "      <td>ummata beelaye duʼa jalaa baraaruuf gargaarsa ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020535_20151124_H0040MR6K</th>\n",
       "      <td>diiniyaas alamuu laliisaa itiyoopiyaan maalif ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020412_20150929_H0040MR7V</th>\n",
       "      <td>hoongee fi gogiinsa oromiyaa keessaa lammiilee...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020533_20170318_H0040MO5O</th>\n",
       "      <td>sababa walitti buʼiinsa daangaa oromiyaa fi su...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020533_20170120_H0040MO5P</th>\n",
       "      <td>daangaa naannoo oromiyaa fi sumaalee irratti w...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020548_20151216_H0040MR7K</th>\n",
       "      <td>birhanu m lenjiso lolli wayyaaneen oromorratti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_NW_020423_20151002_H0040LX9Z  itoophiyaa keessatti laakkofsi namoota hongeef...   \n",
       "IL6_WL_020537_20161206_H0040MR72  balaa hoongee naannolee shanitti uumameef deeg...   \n",
       "IL6_NW_020532_20170426_H0040MPVA  aanaalee godina guraagee balaan hoongee mudate...   \n",
       "IL6_NW_020505_20150902_H0040MO8I  ijoo dubbii abo ummatoota beela irraa baraaruu...   \n",
       "IL6_NW_020423_20160205_H0040LX8T  ummata beelaye duʼa jalaa baraaruuf gargaarsa ...   \n",
       "IL6_WL_020535_20151124_H0040MR6K  diiniyaas alamuu laliisaa itiyoopiyaan maalif ...   \n",
       "IL6_NW_020412_20150929_H0040MR7V  hoongee fi gogiinsa oromiyaa keessaa lammiilee...   \n",
       "IL6_NW_020533_20170318_H0040MO5O  sababa walitti buʼiinsa daangaa oromiyaa fi su...   \n",
       "IL6_NW_020533_20170120_H0040MO5P  daangaa naannoo oromiyaa fi sumaalee irratti w...   \n",
       "IL6_WL_020548_20151216_H0040MR7K  birhanu m lenjiso lolli wayyaaneen oromorratti...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_NW_020423_20151002_H0040LX9Z      7  \n",
       "IL6_WL_020537_20161206_H0040MR72      7  \n",
       "IL6_NW_020532_20170426_H0040MPVA      5  \n",
       "IL6_NW_020505_20150902_H0040MO8I      4  \n",
       "IL6_NW_020423_20160205_H0040LX8T      4  \n",
       "IL6_WL_020535_20151124_H0040MR6K      4  \n",
       "IL6_NW_020412_20150929_H0040MR7V      3  \n",
       "IL6_NW_020533_20170318_H0040MO5O      3  \n",
       "IL6_NW_020533_20170120_H0040MO5P      2  \n",
       "IL6_WL_020548_20151216_H0040MR7K      2  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 1\n",
    "\n",
    "\n",
    "topic1_docs_id = result.loc[(result.class_type==\"indomain\")&(result.topic1==True),\"doc_id\"]\n",
    "\n",
    "topic1_docs_id.shape\n",
    "\n",
    "topic1_docs =raw_data.loc[raw_data.doc_id.isin(topic1_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic1_docs= topic1_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],[\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic1_docs['count'] = topic1_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa').sum())\n",
    "\n",
    "topic1_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic1_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020445_20160426_H0040M4CT</th>\n",
       "      <td>balaa lolaa dirree dawaatti qaqqabeen namoonni...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160419_H0040LX80</th>\n",
       "      <td>biyyatti ajjeefamuu fi biyyoota ollaattii abdi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20160105_H0040MPV6</th>\n",
       "      <td>koomishinichi karoora balaa lolaa ittisuu fi l...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20170426_H0040MPV3</th>\n",
       "      <td>rooba tibbanaan oromiyaatti namoonni kuma  taʼ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020423_20150814_H0040MR7C</th>\n",
       "      <td>haalli qilleensa el nino cimaan uumamuuf deema...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020445_20160507_H0040M4CF</th>\n",
       "      <td>ministeerichi balaa lolaa mudatu hirʼisuuf hoj...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020117_20160705_H0040LMLC</th>\n",
       "      <td>bokkaan kalee barraaqa adaamaatti roobe waan h...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020427_20170210_H0040LYS7</th>\n",
       "      <td>mudannoon weerara maqaa liyyuu polisiin murnii...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOD7</th>\n",
       "      <td>karoora mootummaan wayyaanee ergamtuu isaa opd...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_NW_020445_20160426_H0040M4CT  balaa lolaa dirree dawaatti qaqqabeen namoonni...   \n",
       "IL6_NW_020423_20160419_H0040LX80  biyyatti ajjeefamuu fi biyyoota ollaattii abdi...   \n",
       "IL6_NW_020532_20160105_H0040MPV6  koomishinichi karoora balaa lolaa ittisuu fi l...   \n",
       "IL6_NW_020532_20170426_H0040MPV3  rooba tibbanaan oromiyaatti namoonni kuma  taʼ...   \n",
       "IL6_WL_020423_20150814_H0040MR7C  haalli qilleensa el nino cimaan uumamuuf deema...   \n",
       "IL6_NW_020445_20160507_H0040M4CF  ministeerichi balaa lolaa mudatu hirʼisuuf hoj...   \n",
       "IL6_NW_020117_20160705_H0040LMLC  bokkaan kalee barraaqa adaamaatti roobe waan h...   \n",
       "IL6_NW_020427_20170210_H0040LYS7  mudannoon weerara maqaa liyyuu polisiin murnii...   \n",
       "IL6_WL_020405_20170307_H0040LOD7  karoora mootummaan wayyaanee ergamtuu isaa opd...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020415_20160225_H0040LT3D     16  \n",
       "IL6_NW_020445_20160426_H0040M4CT      8  \n",
       "IL6_NW_020423_20160419_H0040LX80      7  \n",
       "IL6_NW_020532_20160105_H0040MPV6      7  \n",
       "IL6_NW_020532_20170426_H0040MPV3      6  \n",
       "IL6_WL_020423_20150814_H0040MR7C      6  \n",
       "IL6_NW_020445_20160507_H0040M4CF      6  \n",
       "IL6_NW_020117_20160705_H0040LMLC      5  \n",
       "IL6_NW_020427_20170210_H0040LYS7      5  \n",
       "IL6_WL_020405_20170307_H0040LOD7      4  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 2\n",
    "\n",
    "\n",
    "topic2_docs_id = result.loc[(result.class_type==\"indomain\")&(result.topic2==True),\"doc_id\"]\n",
    "\n",
    "topic2_docs_id.shape\n",
    "\n",
    "\n",
    "topic2_docs =raw_data.loc[raw_data.doc_id.isin(topic2_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic2_docs= topic2_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],[\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic2_docs['count'] = topic2_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'galaana|galaanaa|guutuu|lolaa').sum())\n",
    "\n",
    "topic2_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "\n",
    "topic2_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020539_20160403_H0040MR7Q</th>\n",
       "      <td>bishaantu rasaasa taʼe yooseef hambaa tolaa bi...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOE5</th>\n",
       "      <td>sabummaa mormaa sabummaa deggeruu sbo bitootes...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOFT</th>\n",
       "      <td>shira lukkeelee fi gooftoota isaani gaomee dhu...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020532_20160105_H0040MPV6</th>\n",
       "      <td>koomishinichi karoora balaa lolaa ittisuu fi l...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020411_20170318_H0040LSJJ</th>\n",
       "      <td>sirni tpleprdf wayta ummata wayyaba gara lammi...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020412_20170427_H0040MR6F</th>\n",
       "      <td>ibsa ijjannoo barattoota oromoo yuunibarsiitii...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020117_20160420_H0040LMNX</th>\n",
       "      <td>balaa doonii godaantota  galaafaterraa namni l...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20160419_H0040LX80</th>\n",
       "      <td>biyyatti ajjeefamuu fi biyyoota ollaattii abdi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020408_20160402_H0040M6A0</th>\n",
       "      <td>#oromoprotests daily april   sbo ebla   sbovol...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020539_20160403_H0040MR7Q  bishaantu rasaasa taʼe yooseef hambaa tolaa bi...   \n",
       "IL6_WL_020405_20170307_H0040LOE5  sabummaa mormaa sabummaa deggeruu sbo bitootes...   \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_WL_020405_20170307_H0040LOFT  shira lukkeelee fi gooftoota isaani gaomee dhu...   \n",
       "IL6_NW_020532_20160105_H0040MPV6  koomishinichi karoora balaa lolaa ittisuu fi l...   \n",
       "IL6_NW_020411_20170318_H0040LSJJ  sirni tpleprdf wayta ummata wayyaba gara lammi...   \n",
       "IL6_WL_020412_20170427_H0040MR6F  ibsa ijjannoo barattoota oromoo yuunibarsiitii...   \n",
       "IL6_NW_020117_20160420_H0040LMNX  balaa doonii godaantota  galaafaterraa namni l...   \n",
       "IL6_NW_020423_20160419_H0040LX80  biyyatti ajjeefamuu fi biyyoota ollaattii abdi...   \n",
       "IL6_NW_020408_20160402_H0040M6A0  #oromoprotests daily april   sbo ebla   sbovol...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020539_20160403_H0040MR7Q     22  \n",
       "IL6_WL_020405_20170307_H0040LOE5     18  \n",
       "IL6_WL_020415_20160225_H0040LT3D     16  \n",
       "IL6_WL_020405_20170307_H0040LOFT     16  \n",
       "IL6_NW_020532_20160105_H0040MPV6     13  \n",
       "IL6_NW_020411_20170318_H0040LSJJ     12  \n",
       "IL6_WL_020412_20170427_H0040MR6F     11  \n",
       "IL6_NW_020117_20160420_H0040LMNX     11  \n",
       "IL6_NW_020423_20160419_H0040LX80     11  \n",
       "IL6_NW_020408_20160402_H0040M6A0     10  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 3\n",
    "\n",
    "\n",
    "topic3_docs_id = result.loc[(result.class_type==\"indomain\")&(result.topic3==True),\"doc_id\"]\n",
    "\n",
    "topic3_docs_id.shape\n",
    "\n",
    "\n",
    "topic3_docs =raw_data.loc[raw_data.doc_id.isin(topic3_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic3_docs= topic3_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic3_docs['count'] = topic3_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'balaa|dhibee|irraa').sum())\n",
    "\n",
    "topic3_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic3_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_data</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020415_20160225_H0040LT3D</th>\n",
       "      <td>warraaqsi bilisummaa oromoo finiinaajiru sochi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOET</th>\n",
       "      <td>qeerroo diddaan barattoota oromoo bifa qindaay...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020525_20160831_H0040MNBX</th>\n",
       "      <td>#oromoprotests #dhaamsa qeerroo bilisummaa oro...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOD7</th>\n",
       "      <td>karoora mootummaan wayyaanee ergamtuu isaa opd...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020520_20160716_H0040MMVK</th>\n",
       "      <td>injifannoo uummanni oromoo baatii saddet darba...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOFT</th>\n",
       "      <td>shira lukkeelee fi gooftoota isaani gaomee dhu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020405_20170307_H0040LOSG</th>\n",
       "      <td>wayyaaneen shira marsaa ffaa magaalaa amboo ir...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020423_20150514_H0040LXAY</th>\n",
       "      <td>sochiin dachii oromiyaa eprdfopdo irratti mana...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_NW_020427_20150806_H0040LYRC</th>\n",
       "      <td>uummatni hidhaa fi reebichaan saree iyyuu ni x...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IL6_WL_020412_20170427_H0040MR6F</th>\n",
       "      <td>ibsa ijjannoo barattoota oromoo yuunibarsiitii...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          text_data  \\\n",
       "doc_id                                                                                \n",
       "IL6_WL_020415_20160225_H0040LT3D  warraaqsi bilisummaa oromoo finiinaajiru sochi...   \n",
       "IL6_WL_020405_20170307_H0040LOET  qeerroo diddaan barattoota oromoo bifa qindaay...   \n",
       "IL6_WL_020525_20160831_H0040MNBX  #oromoprotests #dhaamsa qeerroo bilisummaa oro...   \n",
       "IL6_WL_020405_20170307_H0040LOD7  karoora mootummaan wayyaanee ergamtuu isaa opd...   \n",
       "IL6_WL_020520_20160716_H0040MMVK  injifannoo uummanni oromoo baatii saddet darba...   \n",
       "IL6_WL_020405_20170307_H0040LOFT  shira lukkeelee fi gooftoota isaani gaomee dhu...   \n",
       "IL6_WL_020405_20170307_H0040LOSG  wayyaaneen shira marsaa ffaa magaalaa amboo ir...   \n",
       "IL6_NW_020423_20150514_H0040LXAY  sochiin dachii oromiyaa eprdfopdo irratti mana...   \n",
       "IL6_NW_020427_20150806_H0040LYRC  uummatni hidhaa fi reebichaan saree iyyuu ni x...   \n",
       "IL6_WL_020412_20170427_H0040MR6F  ibsa ijjannoo barattoota oromoo yuunibarsiitii...   \n",
       "\n",
       "                                  count  \n",
       "doc_id                                   \n",
       "IL6_WL_020415_20160225_H0040LT3D      7  \n",
       "IL6_WL_020405_20170307_H0040LOET      6  \n",
       "IL6_WL_020525_20160831_H0040MNBX      5  \n",
       "IL6_WL_020405_20170307_H0040LOD7      5  \n",
       "IL6_WL_020520_20160716_H0040MMVK      4  \n",
       "IL6_WL_020405_20170307_H0040LOFT      4  \n",
       "IL6_WL_020405_20170307_H0040LOSG      4  \n",
       "IL6_NW_020423_20150514_H0040LXAY      3  \n",
       "IL6_NW_020427_20150806_H0040LYRC      3  \n",
       "IL6_WL_020412_20170427_H0040MR6F      3  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 docs for topic 4\n",
    "\n",
    "\n",
    "topic4_docs_id = result.loc[(result.class_type==\"indomain\")&(result.topic4==True),\"doc_id\"]\n",
    "\n",
    "topic4_docs_id.shape\n",
    "\n",
    "\n",
    "topic4_docs =raw_data.loc[raw_data.doc_id.isin(topic4_docs_id),[\"doc_id\",\"text_data\"]]\n",
    "topic4_docs= topic4_docs.set_index('doc_id')\n",
    "\n",
    "#anchor_words = [\"balaa\",\"dhibee\",\"irraa\"],[\"sochii\",\"lafaa\"]]\n",
    "\n",
    "topic4_docs['count'] = topic4_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'sochii|lafaa').sum())\n",
    "\n",
    "topic4_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "topic4_docs.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ranking\n",
    "\n",
    "temp = result.loc[result.class_type ==\"indomain\"]\n",
    "result_nondomain = result.loc[result.class_type ==\"nondomain\"]\n",
    "result_indomain=temp.sample(n=result_nondomain.shape[0],  replace=False, random_state=3192, axis=None)\n",
    "balanced_result= pd.concat( [result_indomain, result_nondomain], axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++            7            1\n",
      "Actual--           12            0 \n",
      "\n",
      "precision 0.368421052632 \n",
      "\n",
      "recall 0.875 \n",
      "\n",
      "F1_score 0.518518518519 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 20 based anchor word count\n",
    "\n",
    "top_docs = balanced_result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum())\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "top20 = top_docs.iloc[0:20,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top20.class_type, top20.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++            6            5\n",
      "Actual--            8            1 \n",
      "\n",
      "precision 0.428571428571 \n",
      "\n",
      "recall 0.545454545455 \n",
      "\n",
      "F1_score 0.48 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##tf \n",
    "\n",
    "# metrics @ top 20 based anchor word count\n",
    "\n",
    "top_docs = balanced_result\n",
    "\n",
    "top_docs['tf'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum()/len(x.split()))\n",
    "\n",
    "top_docs.sort_values(by=['tf'],ascending=False,inplace=True)\n",
    "top20 = top_docs.iloc[0:20,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top20.class_type, top20.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           16            9\n",
      "Actual--           21            4 \n",
      "\n",
      "precision 0.432432432432 \n",
      "\n",
      "recall 0.64 \n",
      "\n",
      "F1_score 0.516129032258 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##tf sum\n",
    "\n",
    "# metrics @ top 20 based anchor word count\n",
    "\n",
    "top_docs = balanced_result\n",
    "\n",
    "top_docs['tf'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum()/len(x.split()))\n",
    "\n",
    "top_docs.sort_values(by=['tf'],ascending=False,inplace=True)\n",
    "top50 = top_docs.iloc[0:50,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top50.class_type, top50.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++           15            8\n",
      "Actual--           24            3 \n",
      "\n",
      "precision 0.384615384615 \n",
      "\n",
      "recall 0.652173913043 \n",
      "\n",
      "F1_score 0.483870967742 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metrics @ top 50 based anchor word count\n",
    "\n",
    "top_docs = balanced_result\n",
    "top_docs['count'] = top_docs['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum())\n",
    "top_docs.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "top50 = top_docs.iloc[0:50,:]\n",
    "cm=pd.DataFrame(confusion_matrix(top50.class_type, top50.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm,\"\\n\")\n",
    "# precision\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 4786)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(encoding='utf-8',stop_words=None, max_df =0.997, min_df =1,binary =False,\n",
    "                             lowercase=True, use_idf=True,smooth_idf=True)\n",
    "\n",
    "tf_idf_mat = vectorizer.fit_transform(balanced_result.text_data)\n",
    "tf_idf_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_matrix=tf_idf_mat.todense()\n",
    "keyword_mat=dense_matrix[:,[2184,1680,1681,1911,2950,433,2353,3966,2894]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd2=pd.DataFrame(keyword_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate word count (length) of the document\n",
    "pd2['sum'] = pd2.apply(sum, axis=1)\n",
    "\n",
    "# sort by highest td-idfscores\n",
    "pd2=pd2.sort_values(by='sum', axis=0, ascending=False, inplace=False, kind='quicksort', na_position='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[144,\n",
       " 224,\n",
       " 320,\n",
       " 117,\n",
       " 195,\n",
       " 265,\n",
       " 31,\n",
       " 11,\n",
       " 27,\n",
       " 4,\n",
       " 223,\n",
       " 222,\n",
       " 221,\n",
       " 220,\n",
       " 219,\n",
       " 0,\n",
       " 218,\n",
       " 217,\n",
       " 215,\n",
       " 214]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top 20\n",
    "top20_index=pd2[0:20].index.tolist()\n",
    "top20_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++            0            2\n",
      "Actual--            5           13 \n",
      "\n",
      "precision 0.0 \n",
      "\n",
      "recall 0.0 \n",
      "\n",
      "F1_score nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "top20 = balanced_result.iloc[top20_index]\n",
    "type(top20)\n",
    "#top20.shape\n",
    "\n",
    "top20_df=pd.DataFrame(confusion_matrix(top20.class_type, top20.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "print(top20_df,\"\\n\")\n",
    "# precision\n",
    "precision = top20_df.loc[\"Actual++\",\"Predicted++\"]/(top20_df.loc[\"Actual++\",\"Predicted++\"] + top20_df.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  top20_df.loc[\"Actual++\",\"Predicted++\"]/(top20_df.loc[\"Actual++\",\"Predicted++\"] + top20_df.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++            0            4\n",
      "Actual--            6           40 \n",
      "\n",
      "precision 0.0 \n",
      "\n",
      "recall 0.0 \n",
      "\n",
      "F1_score nan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top 20\n",
    "top50_index=pd2[0:50].index.tolist()\n",
    "top50_index\n",
    "top50 = balanced_result.iloc[top50_index]\n",
    "type(top20)\n",
    "#top20.shape\n",
    "\n",
    "top50_df=pd.DataFrame(confusion_matrix(top50.class_type, top50.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "print(top50_df,\"\\n\")\n",
    "# precision\n",
    "precision = top50_df.loc[\"Actual++\",\"Predicted++\"]/(top50_df.loc[\"Actual++\",\"Predicted++\"] + top50_df.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "#recall\n",
    "recall =  top50_df.loc[\"Actual++\",\"Predicted++\"]/(top50_df.loc[\"Actual++\",\"Predicted++\"] + top50_df.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "# F1-Score\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd=pd2.iloc[:,0:9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.371842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0    1    2         3    4    5    6    7    8  product\n",
       "144  0.000000  0.0  0.0  0.413487  0.0  0.0  0.0  0.0  0.0      0.0\n",
       "224  0.371842  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0      0.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate word count (length) of the document\n",
    "cd['product'] = cd.prod(axis=1)\n",
    "cd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guidedlda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-9c2c5b0e4641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mguidedlda\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mguidedlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGuidedLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'guidedlda'"
     ]
    }
   ],
   "source": [
    "# Alternative approach guided-LDA\n",
    "# reference links below\n",
    "#1) https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\n",
    "#2)https://github.com/vi3k6i5/GuidedLDA\n",
    "\n",
    "import numpy as np\n",
    "import guidedlda\n",
    "model = guidedlda.GuidedLDA(n_topics=15, n_iter=4000, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Document-Term Matrix-(count matrix)\n",
    "vectorizer2 = CountVectorizer(encoding='utf-8',stop_words=None,max_df=0.997, min_df=0.001, binary=False,lowercase=True)\n",
    "doc_word_mat2 = vectorizer2.fit_transform(reset_data.text_data)\n",
    "doc_word_mat2 = doc_word_mat2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words2 = list(np.asarray(vectorizer2.get_feature_names()))\n",
    "word2id = dict((v, idx) for idx, v in enumerate(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "seed_topic_list = [[\"hoongee\",\"hongee\",\"oolaa\"],[\"galaana\",\"galaanaa\",\"guutuu\",\"lolaa\"],\n",
    "                [\"balaa\",\"dhibee\",\"irraa\"],\n",
    "                [\"sochii\",\"lafaa\"]]\n",
    "\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 5871\n",
      "INFO:guidedlda:vocab_size: 8360\n",
      "INFO:guidedlda:n_words: 442803\n",
      "INFO:guidedlda:n_topics: 15\n",
      "INFO:guidedlda:n_iter: 4000\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -4970386\n",
      "INFO:guidedlda:<20> log likelihood: -3695558\n",
      "INFO:guidedlda:<40> log likelihood: -3595373\n",
      "INFO:guidedlda:<60> log likelihood: -3555486\n",
      "INFO:guidedlda:<80> log likelihood: -3535340\n",
      "INFO:guidedlda:<100> log likelihood: -3521209\n",
      "INFO:guidedlda:<120> log likelihood: -3513010\n",
      "INFO:guidedlda:<140> log likelihood: -3504078\n",
      "INFO:guidedlda:<160> log likelihood: -3496875\n",
      "INFO:guidedlda:<180> log likelihood: -3493097\n",
      "INFO:guidedlda:<200> log likelihood: -3488878\n",
      "INFO:guidedlda:<220> log likelihood: -3485970\n",
      "INFO:guidedlda:<240> log likelihood: -3486597\n",
      "INFO:guidedlda:<260> log likelihood: -3481151\n",
      "INFO:guidedlda:<280> log likelihood: -3480183\n",
      "INFO:guidedlda:<300> log likelihood: -3479413\n",
      "INFO:guidedlda:<320> log likelihood: -3478216\n",
      "INFO:guidedlda:<340> log likelihood: -3476232\n",
      "INFO:guidedlda:<360> log likelihood: -3476661\n",
      "INFO:guidedlda:<380> log likelihood: -3473757\n",
      "INFO:guidedlda:<400> log likelihood: -3472523\n",
      "INFO:guidedlda:<420> log likelihood: -3471114\n",
      "INFO:guidedlda:<440> log likelihood: -3470598\n",
      "INFO:guidedlda:<460> log likelihood: -3467916\n",
      "INFO:guidedlda:<480> log likelihood: -3467632\n",
      "INFO:guidedlda:<500> log likelihood: -3466311\n",
      "INFO:guidedlda:<520> log likelihood: -3465247\n",
      "INFO:guidedlda:<540> log likelihood: -3465994\n",
      "INFO:guidedlda:<560> log likelihood: -3464069\n",
      "INFO:guidedlda:<580> log likelihood: -3463706\n",
      "INFO:guidedlda:<600> log likelihood: -3462602\n",
      "INFO:guidedlda:<620> log likelihood: -3462320\n",
      "INFO:guidedlda:<640> log likelihood: -3462299\n",
      "INFO:guidedlda:<660> log likelihood: -3461928\n",
      "INFO:guidedlda:<680> log likelihood: -3461220\n",
      "INFO:guidedlda:<700> log likelihood: -3461354\n",
      "INFO:guidedlda:<720> log likelihood: -3459948\n",
      "INFO:guidedlda:<740> log likelihood: -3460675\n",
      "INFO:guidedlda:<760> log likelihood: -3459078\n",
      "INFO:guidedlda:<780> log likelihood: -3460914\n",
      "INFO:guidedlda:<800> log likelihood: -3458706\n",
      "INFO:guidedlda:<820> log likelihood: -3458520\n",
      "INFO:guidedlda:<840> log likelihood: -3456549\n",
      "INFO:guidedlda:<860> log likelihood: -3456538\n",
      "INFO:guidedlda:<880> log likelihood: -3457337\n",
      "INFO:guidedlda:<900> log likelihood: -3456222\n",
      "INFO:guidedlda:<920> log likelihood: -3457391\n",
      "INFO:guidedlda:<940> log likelihood: -3457131\n",
      "INFO:guidedlda:<960> log likelihood: -3458700\n",
      "INFO:guidedlda:<980> log likelihood: -3457844\n",
      "INFO:guidedlda:<1000> log likelihood: -3457692\n",
      "INFO:guidedlda:<1020> log likelihood: -3457213\n",
      "INFO:guidedlda:<1040> log likelihood: -3455386\n",
      "INFO:guidedlda:<1060> log likelihood: -3457185\n",
      "INFO:guidedlda:<1080> log likelihood: -3457323\n",
      "INFO:guidedlda:<1100> log likelihood: -3454343\n",
      "INFO:guidedlda:<1120> log likelihood: -3455300\n",
      "INFO:guidedlda:<1140> log likelihood: -3455106\n",
      "INFO:guidedlda:<1160> log likelihood: -3455082\n",
      "INFO:guidedlda:<1180> log likelihood: -3454411\n",
      "INFO:guidedlda:<1200> log likelihood: -3454313\n",
      "INFO:guidedlda:<1220> log likelihood: -3453663\n",
      "INFO:guidedlda:<1240> log likelihood: -3454985\n",
      "INFO:guidedlda:<1260> log likelihood: -3453642\n",
      "INFO:guidedlda:<1280> log likelihood: -3452468\n",
      "INFO:guidedlda:<1300> log likelihood: -3453386\n",
      "INFO:guidedlda:<1320> log likelihood: -3451504\n",
      "INFO:guidedlda:<1340> log likelihood: -3451391\n",
      "INFO:guidedlda:<1360> log likelihood: -3451051\n",
      "INFO:guidedlda:<1380> log likelihood: -3450956\n",
      "INFO:guidedlda:<1400> log likelihood: -3450748\n",
      "INFO:guidedlda:<1420> log likelihood: -3452223\n",
      "INFO:guidedlda:<1440> log likelihood: -3452083\n",
      "INFO:guidedlda:<1460> log likelihood: -3452237\n",
      "INFO:guidedlda:<1480> log likelihood: -3451877\n",
      "INFO:guidedlda:<1500> log likelihood: -3451436\n",
      "INFO:guidedlda:<1520> log likelihood: -3450673\n",
      "INFO:guidedlda:<1540> log likelihood: -3451242\n",
      "INFO:guidedlda:<1560> log likelihood: -3450370\n",
      "INFO:guidedlda:<1580> log likelihood: -3450788\n",
      "INFO:guidedlda:<1600> log likelihood: -3450257\n",
      "INFO:guidedlda:<1620> log likelihood: -3449614\n",
      "INFO:guidedlda:<1640> log likelihood: -3449610\n",
      "INFO:guidedlda:<1660> log likelihood: -3449738\n",
      "INFO:guidedlda:<1680> log likelihood: -3450029\n",
      "INFO:guidedlda:<1700> log likelihood: -3450389\n",
      "INFO:guidedlda:<1720> log likelihood: -3450165\n",
      "INFO:guidedlda:<1740> log likelihood: -3449568\n",
      "INFO:guidedlda:<1760> log likelihood: -3450783\n",
      "INFO:guidedlda:<1780> log likelihood: -3450306\n",
      "INFO:guidedlda:<1800> log likelihood: -3450965\n",
      "INFO:guidedlda:<1820> log likelihood: -3450006\n",
      "INFO:guidedlda:<1840> log likelihood: -3450180\n",
      "INFO:guidedlda:<1860> log likelihood: -3450586\n",
      "INFO:guidedlda:<1880> log likelihood: -3450295\n",
      "INFO:guidedlda:<1900> log likelihood: -3449658\n",
      "INFO:guidedlda:<1920> log likelihood: -3449018\n",
      "INFO:guidedlda:<1940> log likelihood: -3449433\n",
      "INFO:guidedlda:<1960> log likelihood: -3448673\n",
      "INFO:guidedlda:<1980> log likelihood: -3449062\n",
      "INFO:guidedlda:<2000> log likelihood: -3450387\n",
      "INFO:guidedlda:<2020> log likelihood: -3449500\n",
      "INFO:guidedlda:<2040> log likelihood: -3449688\n",
      "INFO:guidedlda:<2060> log likelihood: -3448197\n",
      "INFO:guidedlda:<2080> log likelihood: -3448541\n",
      "INFO:guidedlda:<2100> log likelihood: -3446223\n",
      "INFO:guidedlda:<2120> log likelihood: -3448254\n",
      "INFO:guidedlda:<2140> log likelihood: -3447432\n",
      "INFO:guidedlda:<2160> log likelihood: -3447305\n",
      "INFO:guidedlda:<2180> log likelihood: -3446848\n",
      "INFO:guidedlda:<2200> log likelihood: -3447239\n",
      "INFO:guidedlda:<2220> log likelihood: -3446165\n",
      "INFO:guidedlda:<2240> log likelihood: -3447577\n",
      "INFO:guidedlda:<2260> log likelihood: -3446797\n",
      "INFO:guidedlda:<2280> log likelihood: -3447482\n",
      "INFO:guidedlda:<2300> log likelihood: -3447637\n",
      "INFO:guidedlda:<2320> log likelihood: -3445891\n",
      "INFO:guidedlda:<2340> log likelihood: -3446890\n",
      "INFO:guidedlda:<2360> log likelihood: -3446658\n",
      "INFO:guidedlda:<2380> log likelihood: -3444398\n",
      "INFO:guidedlda:<2400> log likelihood: -3444902\n",
      "INFO:guidedlda:<2420> log likelihood: -3444795\n",
      "INFO:guidedlda:<2440> log likelihood: -3444518\n",
      "INFO:guidedlda:<2460> log likelihood: -3442819\n",
      "INFO:guidedlda:<2480> log likelihood: -3444577\n",
      "INFO:guidedlda:<2500> log likelihood: -3444094\n",
      "INFO:guidedlda:<2520> log likelihood: -3443561\n",
      "INFO:guidedlda:<2540> log likelihood: -3445272\n",
      "INFO:guidedlda:<2560> log likelihood: -3445910\n",
      "INFO:guidedlda:<2580> log likelihood: -3443737\n",
      "INFO:guidedlda:<2600> log likelihood: -3444604\n",
      "INFO:guidedlda:<2620> log likelihood: -3443404\n",
      "INFO:guidedlda:<2640> log likelihood: -3443460\n",
      "INFO:guidedlda:<2660> log likelihood: -3443872\n",
      "INFO:guidedlda:<2680> log likelihood: -3444062\n",
      "INFO:guidedlda:<2700> log likelihood: -3443333\n",
      "INFO:guidedlda:<2720> log likelihood: -3443372\n",
      "INFO:guidedlda:<2740> log likelihood: -3442256\n",
      "INFO:guidedlda:<2760> log likelihood: -3443202\n",
      "INFO:guidedlda:<2780> log likelihood: -3444464\n",
      "INFO:guidedlda:<2800> log likelihood: -3442678\n",
      "INFO:guidedlda:<2820> log likelihood: -3443642\n",
      "INFO:guidedlda:<2840> log likelihood: -3442765\n",
      "INFO:guidedlda:<2860> log likelihood: -3442491\n",
      "INFO:guidedlda:<2880> log likelihood: -3442018\n",
      "INFO:guidedlda:<2900> log likelihood: -3442941\n",
      "INFO:guidedlda:<2920> log likelihood: -3442151\n",
      "INFO:guidedlda:<2940> log likelihood: -3442849\n",
      "INFO:guidedlda:<2960> log likelihood: -3442710\n",
      "INFO:guidedlda:<2980> log likelihood: -3442705\n",
      "INFO:guidedlda:<3000> log likelihood: -3442891\n",
      "INFO:guidedlda:<3020> log likelihood: -3443741\n",
      "INFO:guidedlda:<3040> log likelihood: -3442261\n",
      "INFO:guidedlda:<3060> log likelihood: -3442912\n",
      "INFO:guidedlda:<3080> log likelihood: -3442942\n",
      "INFO:guidedlda:<3100> log likelihood: -3442656\n",
      "INFO:guidedlda:<3120> log likelihood: -3441543\n",
      "INFO:guidedlda:<3140> log likelihood: -3443406\n",
      "INFO:guidedlda:<3160> log likelihood: -3441118\n",
      "INFO:guidedlda:<3180> log likelihood: -3442138\n",
      "INFO:guidedlda:<3200> log likelihood: -3443332\n",
      "INFO:guidedlda:<3220> log likelihood: -3441427\n",
      "INFO:guidedlda:<3240> log likelihood: -3441564\n",
      "INFO:guidedlda:<3260> log likelihood: -3441870\n",
      "INFO:guidedlda:<3280> log likelihood: -3441411\n",
      "INFO:guidedlda:<3300> log likelihood: -3440082\n",
      "INFO:guidedlda:<3320> log likelihood: -3440253\n",
      "INFO:guidedlda:<3340> log likelihood: -3441109\n",
      "INFO:guidedlda:<3360> log likelihood: -3441556\n",
      "INFO:guidedlda:<3380> log likelihood: -3440825\n",
      "INFO:guidedlda:<3400> log likelihood: -3440986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:<3420> log likelihood: -3441214\n",
      "INFO:guidedlda:<3440> log likelihood: -3439796\n",
      "INFO:guidedlda:<3460> log likelihood: -3440606\n",
      "INFO:guidedlda:<3480> log likelihood: -3440446\n",
      "INFO:guidedlda:<3500> log likelihood: -3440648\n",
      "INFO:guidedlda:<3520> log likelihood: -3439044\n",
      "INFO:guidedlda:<3540> log likelihood: -3441232\n",
      "INFO:guidedlda:<3560> log likelihood: -3439744\n",
      "INFO:guidedlda:<3580> log likelihood: -3440826\n",
      "INFO:guidedlda:<3600> log likelihood: -3439982\n",
      "INFO:guidedlda:<3620> log likelihood: -3439163\n",
      "INFO:guidedlda:<3640> log likelihood: -3439646\n",
      "INFO:guidedlda:<3660> log likelihood: -3439406\n",
      "INFO:guidedlda:<3680> log likelihood: -3440281\n",
      "INFO:guidedlda:<3700> log likelihood: -3440671\n",
      "INFO:guidedlda:<3720> log likelihood: -3438571\n",
      "INFO:guidedlda:<3740> log likelihood: -3439275\n",
      "INFO:guidedlda:<3760> log likelihood: -3439291\n",
      "INFO:guidedlda:<3780> log likelihood: -3441718\n",
      "INFO:guidedlda:<3800> log likelihood: -3440252\n",
      "INFO:guidedlda:<3820> log likelihood: -3438130\n",
      "INFO:guidedlda:<3840> log likelihood: -3438324\n",
      "INFO:guidedlda:<3860> log likelihood: -3438604\n",
      "INFO:guidedlda:<3880> log likelihood: -3440128\n",
      "INFO:guidedlda:<3900> log likelihood: -3439593\n",
      "INFO:guidedlda:<3920> log likelihood: -3438913\n",
      "INFO:guidedlda:<3940> log likelihood: -3439183\n",
      "INFO:guidedlda:<3960> log likelihood: -3439932\n",
      "INFO:guidedlda:<3980> log likelihood: -3439442\n",
      "INFO:guidedlda:<3999> log likelihood: -3440620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x1adc5af2320>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 116\n",
      "1 164\n",
      "2 826\n",
      "3 1034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "vlist = []\n",
    "for i in range(len(doc_topic)):\n",
    "    vlist.append(doc_topic[i].argmax())\n",
    "vlist\n",
    "index_list=np.where(np.asarray(vlist) == 0)[0].tolist()\n",
    "print(\"0\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 1)[0].tolist())\n",
    "print(\"1\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 2)[0].tolist())\n",
    "print(\"2\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 3)[0].tolist())\n",
    "print(\"3\",len(index_list))\n",
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1=reset_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1.loc[index_list,'predicted_class_label']= \"indomain\"\n",
    "\n",
    "temp1.loc[(temp1['predicted_class_label']!=\"indomain\") , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 5)\n",
      "shape after filtering unknown docs (1970, 5)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",temp1.shape )\n",
    "temp1 = temp1.loc[temp1.class_type !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",temp1.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          417         1389\n",
      "Actual--           47          117\n",
      "\n",
      "\n",
      "precision 0.8987068965517241 \n",
      "\n",
      "recall 0.23089700996677742 \n",
      "\n",
      "F1_score 0.36740088105726876 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(temp1.class_type, temp1.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\")\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "seed_topic_list2 = [[\"hoongee\",\"hongee\"],[\"galaana\",\"galaanaa\",\"guutuu\"],\n",
    "                [\"balaa\"]]\n",
    "\n",
    "seed_topics2 = {}\n",
    "for t_id, st in enumerate(seed_topic_list2):\n",
    "    for word in st:\n",
    "        seed_topics2[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 5871\n",
      "INFO:guidedlda:vocab_size: 8360\n",
      "INFO:guidedlda:n_words: 442803\n",
      "INFO:guidedlda:n_topics: 15\n",
      "INFO:guidedlda:n_iter: 4000\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -4970962\n",
      "INFO:guidedlda:<20> log likelihood: -3709338\n",
      "INFO:guidedlda:<40> log likelihood: -3610146\n",
      "INFO:guidedlda:<60> log likelihood: -3564880\n",
      "INFO:guidedlda:<80> log likelihood: -3543168\n",
      "INFO:guidedlda:<100> log likelihood: -3529117\n",
      "INFO:guidedlda:<120> log likelihood: -3519373\n",
      "INFO:guidedlda:<140> log likelihood: -3513058\n",
      "INFO:guidedlda:<160> log likelihood: -3507347\n",
      "INFO:guidedlda:<180> log likelihood: -3500740\n",
      "INFO:guidedlda:<200> log likelihood: -3498525\n",
      "INFO:guidedlda:<220> log likelihood: -3493081\n",
      "INFO:guidedlda:<240> log likelihood: -3492280\n",
      "INFO:guidedlda:<260> log likelihood: -3489956\n",
      "INFO:guidedlda:<280> log likelihood: -3486265\n",
      "INFO:guidedlda:<300> log likelihood: -3483813\n",
      "INFO:guidedlda:<320> log likelihood: -3481032\n",
      "INFO:guidedlda:<340> log likelihood: -3479070\n",
      "INFO:guidedlda:<360> log likelihood: -3478170\n",
      "INFO:guidedlda:<380> log likelihood: -3476412\n",
      "INFO:guidedlda:<400> log likelihood: -3476049\n",
      "INFO:guidedlda:<420> log likelihood: -3474562\n",
      "INFO:guidedlda:<440> log likelihood: -3473628\n",
      "INFO:guidedlda:<460> log likelihood: -3472795\n",
      "INFO:guidedlda:<480> log likelihood: -3471680\n",
      "INFO:guidedlda:<500> log likelihood: -3470588\n",
      "INFO:guidedlda:<520> log likelihood: -3467632\n",
      "INFO:guidedlda:<540> log likelihood: -3468654\n",
      "INFO:guidedlda:<560> log likelihood: -3467423\n",
      "INFO:guidedlda:<580> log likelihood: -3468130\n",
      "INFO:guidedlda:<600> log likelihood: -3467393\n",
      "INFO:guidedlda:<620> log likelihood: -3466182\n",
      "INFO:guidedlda:<640> log likelihood: -3464904\n",
      "INFO:guidedlda:<660> log likelihood: -3466519\n",
      "INFO:guidedlda:<680> log likelihood: -3465489\n",
      "INFO:guidedlda:<700> log likelihood: -3466582\n",
      "INFO:guidedlda:<720> log likelihood: -3465965\n",
      "INFO:guidedlda:<740> log likelihood: -3466057\n",
      "INFO:guidedlda:<760> log likelihood: -3464036\n",
      "INFO:guidedlda:<780> log likelihood: -3462954\n",
      "INFO:guidedlda:<800> log likelihood: -3464286\n",
      "INFO:guidedlda:<820> log likelihood: -3464336\n",
      "INFO:guidedlda:<840> log likelihood: -3462897\n",
      "INFO:guidedlda:<860> log likelihood: -3462158\n",
      "INFO:guidedlda:<880> log likelihood: -3462139\n",
      "INFO:guidedlda:<900> log likelihood: -3463213\n",
      "INFO:guidedlda:<920> log likelihood: -3461839\n",
      "INFO:guidedlda:<940> log likelihood: -3460910\n",
      "INFO:guidedlda:<960> log likelihood: -3460642\n",
      "INFO:guidedlda:<980> log likelihood: -3461446\n",
      "INFO:guidedlda:<1000> log likelihood: -3460247\n",
      "INFO:guidedlda:<1020> log likelihood: -3462013\n",
      "INFO:guidedlda:<1040> log likelihood: -3460826\n",
      "INFO:guidedlda:<1060> log likelihood: -3460504\n",
      "INFO:guidedlda:<1080> log likelihood: -3460597\n",
      "INFO:guidedlda:<1100> log likelihood: -3460639\n",
      "INFO:guidedlda:<1120> log likelihood: -3459732\n",
      "INFO:guidedlda:<1140> log likelihood: -3458524\n",
      "INFO:guidedlda:<1160> log likelihood: -3458539\n",
      "INFO:guidedlda:<1180> log likelihood: -3458809\n",
      "INFO:guidedlda:<1200> log likelihood: -3458522\n",
      "INFO:guidedlda:<1220> log likelihood: -3458225\n",
      "INFO:guidedlda:<1240> log likelihood: -3459509\n",
      "INFO:guidedlda:<1260> log likelihood: -3458230\n",
      "INFO:guidedlda:<1280> log likelihood: -3459309\n",
      "INFO:guidedlda:<1300> log likelihood: -3456956\n",
      "INFO:guidedlda:<1320> log likelihood: -3457946\n",
      "INFO:guidedlda:<1340> log likelihood: -3457084\n",
      "INFO:guidedlda:<1360> log likelihood: -3456006\n",
      "INFO:guidedlda:<1380> log likelihood: -3454516\n",
      "INFO:guidedlda:<1400> log likelihood: -3455545\n",
      "INFO:guidedlda:<1420> log likelihood: -3456860\n",
      "INFO:guidedlda:<1440> log likelihood: -3457331\n",
      "INFO:guidedlda:<1460> log likelihood: -3457563\n",
      "INFO:guidedlda:<1480> log likelihood: -3455285\n",
      "INFO:guidedlda:<1500> log likelihood: -3454192\n",
      "INFO:guidedlda:<1520> log likelihood: -3455373\n",
      "INFO:guidedlda:<1540> log likelihood: -3455999\n",
      "INFO:guidedlda:<1560> log likelihood: -3453576\n",
      "INFO:guidedlda:<1580> log likelihood: -3455575\n",
      "INFO:guidedlda:<1600> log likelihood: -3454302\n",
      "INFO:guidedlda:<1620> log likelihood: -3452814\n",
      "INFO:guidedlda:<1640> log likelihood: -3453379\n",
      "INFO:guidedlda:<1660> log likelihood: -3454321\n",
      "INFO:guidedlda:<1680> log likelihood: -3454014\n",
      "INFO:guidedlda:<1700> log likelihood: -3452013\n",
      "INFO:guidedlda:<1720> log likelihood: -3453719\n",
      "INFO:guidedlda:<1740> log likelihood: -3452876\n",
      "INFO:guidedlda:<1760> log likelihood: -3452778\n",
      "INFO:guidedlda:<1780> log likelihood: -3454400\n",
      "INFO:guidedlda:<1800> log likelihood: -3452791\n",
      "INFO:guidedlda:<1820> log likelihood: -3452818\n",
      "INFO:guidedlda:<1840> log likelihood: -3452441\n",
      "INFO:guidedlda:<1860> log likelihood: -3453660\n",
      "INFO:guidedlda:<1880> log likelihood: -3453012\n",
      "INFO:guidedlda:<1900> log likelihood: -3452741\n",
      "INFO:guidedlda:<1920> log likelihood: -3452478\n",
      "INFO:guidedlda:<1940> log likelihood: -3451357\n",
      "INFO:guidedlda:<1960> log likelihood: -3450685\n",
      "INFO:guidedlda:<1980> log likelihood: -3452687\n",
      "INFO:guidedlda:<2000> log likelihood: -3452295\n",
      "INFO:guidedlda:<2020> log likelihood: -3452984\n",
      "INFO:guidedlda:<2040> log likelihood: -3452344\n",
      "INFO:guidedlda:<2060> log likelihood: -3451835\n",
      "INFO:guidedlda:<2080> log likelihood: -3452549\n",
      "INFO:guidedlda:<2100> log likelihood: -3452119\n",
      "INFO:guidedlda:<2120> log likelihood: -3451980\n",
      "INFO:guidedlda:<2140> log likelihood: -3451004\n",
      "INFO:guidedlda:<2160> log likelihood: -3451061\n",
      "INFO:guidedlda:<2180> log likelihood: -3450911\n",
      "INFO:guidedlda:<2200> log likelihood: -3450361\n",
      "INFO:guidedlda:<2220> log likelihood: -3450586\n",
      "INFO:guidedlda:<2240> log likelihood: -3450572\n",
      "INFO:guidedlda:<2260> log likelihood: -3449615\n",
      "INFO:guidedlda:<2280> log likelihood: -3449704\n",
      "INFO:guidedlda:<2300> log likelihood: -3449427\n",
      "INFO:guidedlda:<2320> log likelihood: -3449114\n",
      "INFO:guidedlda:<2340> log likelihood: -3449630\n",
      "INFO:guidedlda:<2360> log likelihood: -3449081\n",
      "INFO:guidedlda:<2380> log likelihood: -3449940\n",
      "INFO:guidedlda:<2400> log likelihood: -3450943\n",
      "INFO:guidedlda:<2420> log likelihood: -3450191\n",
      "INFO:guidedlda:<2440> log likelihood: -3450118\n",
      "INFO:guidedlda:<2460> log likelihood: -3449136\n",
      "INFO:guidedlda:<2480> log likelihood: -3450015\n",
      "INFO:guidedlda:<2500> log likelihood: -3448580\n",
      "INFO:guidedlda:<2520> log likelihood: -3448921\n",
      "INFO:guidedlda:<2540> log likelihood: -3448701\n",
      "INFO:guidedlda:<2560> log likelihood: -3448317\n",
      "INFO:guidedlda:<2580> log likelihood: -3447204\n",
      "INFO:guidedlda:<2600> log likelihood: -3448262\n",
      "INFO:guidedlda:<2620> log likelihood: -3448935\n",
      "INFO:guidedlda:<2640> log likelihood: -3449752\n",
      "INFO:guidedlda:<2660> log likelihood: -3450003\n",
      "INFO:guidedlda:<2680> log likelihood: -3449490\n",
      "INFO:guidedlda:<2700> log likelihood: -3449046\n",
      "INFO:guidedlda:<2720> log likelihood: -3446484\n",
      "INFO:guidedlda:<2740> log likelihood: -3447056\n",
      "INFO:guidedlda:<2760> log likelihood: -3448024\n",
      "INFO:guidedlda:<2780> log likelihood: -3448963\n",
      "INFO:guidedlda:<2800> log likelihood: -3449375\n",
      "INFO:guidedlda:<2820> log likelihood: -3447686\n",
      "INFO:guidedlda:<2840> log likelihood: -3448518\n",
      "INFO:guidedlda:<2860> log likelihood: -3449115\n",
      "INFO:guidedlda:<2880> log likelihood: -3448308\n",
      "INFO:guidedlda:<2900> log likelihood: -3447371\n",
      "INFO:guidedlda:<2920> log likelihood: -3447682\n",
      "INFO:guidedlda:<2940> log likelihood: -3448878\n",
      "INFO:guidedlda:<2960> log likelihood: -3448141\n",
      "INFO:guidedlda:<2980> log likelihood: -3447027\n",
      "INFO:guidedlda:<3000> log likelihood: -3447860\n",
      "INFO:guidedlda:<3020> log likelihood: -3446530\n",
      "INFO:guidedlda:<3040> log likelihood: -3447397\n",
      "INFO:guidedlda:<3060> log likelihood: -3447994\n",
      "INFO:guidedlda:<3080> log likelihood: -3448023\n",
      "INFO:guidedlda:<3100> log likelihood: -3447177\n",
      "INFO:guidedlda:<3120> log likelihood: -3448162\n",
      "INFO:guidedlda:<3140> log likelihood: -3448258\n",
      "INFO:guidedlda:<3160> log likelihood: -3446155\n",
      "INFO:guidedlda:<3180> log likelihood: -3448506\n",
      "INFO:guidedlda:<3200> log likelihood: -3448979\n",
      "INFO:guidedlda:<3220> log likelihood: -3449159\n",
      "INFO:guidedlda:<3240> log likelihood: -3447059\n",
      "INFO:guidedlda:<3260> log likelihood: -3446571\n",
      "INFO:guidedlda:<3280> log likelihood: -3447848\n",
      "INFO:guidedlda:<3300> log likelihood: -3449091\n",
      "INFO:guidedlda:<3320> log likelihood: -3448327\n",
      "INFO:guidedlda:<3340> log likelihood: -3446986\n",
      "INFO:guidedlda:<3360> log likelihood: -3448334\n",
      "INFO:guidedlda:<3380> log likelihood: -3447855\n",
      "INFO:guidedlda:<3400> log likelihood: -3448124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:<3420> log likelihood: -3448966\n",
      "INFO:guidedlda:<3440> log likelihood: -3447994\n",
      "INFO:guidedlda:<3460> log likelihood: -3448664\n",
      "INFO:guidedlda:<3480> log likelihood: -3448371\n",
      "INFO:guidedlda:<3500> log likelihood: -3446983\n",
      "INFO:guidedlda:<3520> log likelihood: -3447237\n",
      "INFO:guidedlda:<3540> log likelihood: -3448123\n",
      "INFO:guidedlda:<3560> log likelihood: -3448172\n",
      "INFO:guidedlda:<3580> log likelihood: -3447439\n",
      "INFO:guidedlda:<3600> log likelihood: -3447539\n",
      "INFO:guidedlda:<3620> log likelihood: -3447245\n",
      "INFO:guidedlda:<3640> log likelihood: -3446818\n",
      "INFO:guidedlda:<3660> log likelihood: -3446796\n",
      "INFO:guidedlda:<3680> log likelihood: -3446901\n",
      "INFO:guidedlda:<3700> log likelihood: -3447022\n",
      "INFO:guidedlda:<3720> log likelihood: -3445407\n",
      "INFO:guidedlda:<3740> log likelihood: -3445882\n",
      "INFO:guidedlda:<3760> log likelihood: -3446438\n",
      "INFO:guidedlda:<3780> log likelihood: -3446785\n",
      "INFO:guidedlda:<3800> log likelihood: -3444795\n",
      "INFO:guidedlda:<3820> log likelihood: -3446196\n",
      "INFO:guidedlda:<3840> log likelihood: -3446057\n",
      "INFO:guidedlda:<3860> log likelihood: -3446068\n",
      "INFO:guidedlda:<3880> log likelihood: -3446209\n",
      "INFO:guidedlda:<3900> log likelihood: -3445930\n",
      "INFO:guidedlda:<3920> log likelihood: -3447876\n",
      "INFO:guidedlda:<3940> log likelihood: -3446470\n",
      "INFO:guidedlda:<3960> log likelihood: -3445647\n",
      "INFO:guidedlda:<3980> log likelihood: -3445173\n",
      "INFO:guidedlda:<3999> log likelihood: -3444435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x1adc5af2320>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics2, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 105\n",
      "1 179\n",
      "2 333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "vlist = []\n",
    "for i in range(len(doc_topic)):\n",
    "    vlist.append(doc_topic[i].argmax())\n",
    "vlist\n",
    "index_list=np.where(np.asarray(vlist) == 0)[0].tolist()\n",
    "print(\"0\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 1)[0].tolist())\n",
    "print(\"1\",len(index_list))\n",
    "index_list.extend(np.where(np.asarray(vlist) == 2)[0].tolist())\n",
    "print(\"2\",len(index_list))\n",
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2=reset_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp2.loc[index_list,'predicted_class_label']= \"indomain\"\n",
    "\n",
    "temp2.loc[(temp2['predicted_class_label']!=\"indomain\") , 'predicted_class_label'] = \"nondomain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before filtering unknown docs (5871, 5)\n",
      "shape after filtering unknown docs (1970, 5)\n"
     ]
    }
   ],
   "source": [
    "# filter unknown class\n",
    "print(\"shape before filtering unknown docs\",temp2.shape )\n",
    "temp2 = temp2.loc[temp2.class_type !=\"unk\"]\n",
    "print(\"shape after filtering unknown docs\",temp2.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          108         1698\n",
      "Actual--           13          151\n",
      "\n",
      "\n",
      "precision 0.8925619834710744 \n",
      "\n",
      "recall 0.059800664451827246 \n",
      "\n",
      "F1_score 0.11209133367929423 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=pd.DataFrame(confusion_matrix(temp2.class_type, temp2.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "print(cm)\n",
    "\n",
    "print(\"\\n\")\n",
    "precision = cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cm.loc[\"Actual++\",\"Predicted++\"]/(cm.loc[\"Actual++\",\"Predicted++\"] + cm.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline 1\n",
    "\n",
    "baseline_temp = reset_data\n",
    "baseline_temp['count'] = baseline_temp['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|oolaa|galaana|galaanaa|guutuu|lolaa|balaa|dhibee|irraa|sochii|lafaa').sum())\n",
    "\n",
    "# assigning class lables based on the binary classification result\n",
    "baseline_temp.loc[baseline_temp['count'] >0, 'predicted_class_label'] = \"indomain\"\n",
    "baseline_temp.loc[baseline_temp['count'] <=0, 'predicted_class_label'] = \"nondomain\"\n",
    "baseline_temp['class_type'].unique()\n",
    "\n",
    "# filter unknown class\n",
    "\n",
    "baseline_temp = baseline_temp.loc[baseline_temp.class_type !=\"unk\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          292         1514\n",
      "Actual--           29          135 \n",
      "\n",
      "precision 0.9096573208722741 \n",
      "\n",
      "recall 0.16168327796234774 \n",
      "\n",
      "F1_score 0.27456511518570764 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf=pd.DataFrame(confusion_matrix(baseline_temp.class_type, baseline_temp.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "\n",
    "print(cf,\"\\n\")\n",
    "\n",
    "\n",
    "precision = cf.loc[\"Actual++\",\"Predicted++\"]/(cf.loc[\"Actual++\",\"Predicted++\"] + cf.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cf.loc[\"Actual++\",\"Predicted++\"]/(cf.loc[\"Actual++\",\"Predicted++\"] + cf.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline 2\n",
    "\n",
    "baseline_temp2 = reset_data\n",
    "baseline_temp2['count'] = baseline_temp2['text_data'].apply(lambda x: pd.Series(x.split()).str.contains(r'hoongee|hongee|galaana|galaanaa|guutuu|balaa').sum())\n",
    "\n",
    "# assigning class lables based on the binary classification result\n",
    "baseline_temp2.loc[baseline_temp2['count'] >0, 'predicted_class_label'] = \"indomain\"\n",
    "baseline_temp2.loc[baseline_temp2['count'] <=0, 'predicted_class_label'] = \"nondomain\"\n",
    "baseline_temp2['class_type'].unique()\n",
    "\n",
    "# filter unknown class\n",
    "\n",
    "baseline_temp2 = baseline_temp2.loc[baseline_temp2.class_type !=\"unk\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted++  Predicted--\n",
      "Actual++          109         1697\n",
      "Actual--           11          153 \n",
      "\n",
      "precision 0.9083333333333333 \n",
      "\n",
      "recall 0.06035437430786268 \n",
      "\n",
      "F1_score 0.11318795430944964 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf=pd.DataFrame(confusion_matrix(baseline_temp2.class_type, baseline_temp2.predicted_class_label),columns=[\"Predicted++\",\"Predicted--\"],index=[\"Actual++\",\"Actual--\"])\n",
    "\n",
    "\n",
    "print(cf,\"\\n\")\n",
    "\n",
    "\n",
    "precision = cf.loc[\"Actual++\",\"Predicted++\"]/(cf.loc[\"Actual++\",\"Predicted++\"] + cf.loc[\"Actual--\",\"Predicted++\"])\n",
    "print(\"precision\",precision,\"\\n\")\n",
    "\n",
    "recall =  cf.loc[\"Actual++\",\"Predicted++\"]/(cf.loc[\"Actual++\",\"Predicted++\"] + cf.loc[\"Actual++\",\"Predicted--\"] )\n",
    "print(\"recall\",recall,\"\\n\")\n",
    "\n",
    "F1_score = 2*(precision * recall)/(precision + recall)\n",
    "print(\"F1_score\",F1_score,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
