{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "code_folding": [
     25,
     599,
     603,
     607,
     612,
     617,
     623
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"CorEx Hierarchical Topic Models\n",
    "Use the principle of Total Cor-relation Explanation (CorEx) to construct\n",
    "hierarchical topic models. This module is specially designed for sparse count\n",
    "data and implements semi-supervision via the information bottleneck.\n",
    "Greg Ver Steeg and Aram Galstyan. \"Maximally Informative Hierarchical\n",
    "Representations of High-Dimensional Data.\" AISTATS, 2015.\n",
    "Gallagher et al. \"Anchored Correlation Explanation: Topic Modeling with Minimal\n",
    "Domain Knowledge.\" TACL, 2017.\n",
    "Code below written by:\n",
    "Greg Ver Steeg (gregv@isi.edu)\n",
    "Ryan J. Gallagher\n",
    "David Kale\n",
    "Lily Fierro\n",
    "License: Apache V2\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse XML input data & save a local copy of parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "code_folding": [
     5,
     6,
     9,
     11
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import etree\n",
    "xml_data='C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\New folder\\\\tamil_data_cp.xml'\n",
    "file =open('C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\New folder\\\\new_processed_copy10.txt', 'a',encoding=\"utf-8\")\n",
    "i=1\n",
    "for event, elem in etree.iterparse(xml_data,tag=('FULL_ID_SOURCE', 'LRLP_TOKENIZED_RAW_SOURCE')):\n",
    "    if(i%100000000==0):\n",
    "        print(\"iiiiiiiiiii\",i)\n",
    "        i=1\n",
    "    if elem.tag == 'FULL_ID_SOURCE':\n",
    "            id1 = elem.text\n",
    "    if elem.tag == 'LRLP_TOKENIZED_RAW_SOURCE':\n",
    "            txt=elem.text\n",
    "            temp=id1+\"\\t\"+txt \n",
    "            file.write(temp+\"\\n\")\n",
    "            i=i+1\n",
    "    elem.clear()\n",
    "    elem.getparent().remove(elem)\n",
    "            \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [
     1,
     4,
     6
    ],
    "scrolled": true
   },
   "source": [
    "### Read the parsed data from local directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data=[]\n",
    "with open('C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\New folder\\\\processed_tamil_data.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join segements into a single document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.DataFrame(list_data) \n",
    "raw_data.head(2)\n",
    "new =raw_data.groupby(0)[1].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAM_NW_002923_20170316_H0023Y4BL</td>\n",
       "      <td>தமிழகத்தின் கடன் 3,14,366 கோடி : பட்ஜெட்டில் த...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAM_NW_002923_20170316_H0023Y4BL</td>\n",
       "      <td>2016-17ஆம் நிதியாண்டில் தமிழக பட்ஜெட் பற்றாக்க...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0  \\\n",
       "0  TAM_NW_002923_20170316_H0023Y4BL   \n",
       "1  TAM_NW_002923_20170316_H0023Y4BL   \n",
       "\n",
       "                                                   1  \n",
       "0  தமிழகத்தின் கடன் 3,14,366 கோடி : பட்ஜெட்டில் த...  \n",
       "1  2016-17ஆம் நிதியாண்டில் தமிழக பட்ஜெட் பற்றாக்க...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = new.to_frame().reset_index()\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns= {0:'doc_id',1:\"raw_text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepocessing\n",
    "-  strip additional white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip columns for leading and trailing white spaces\n",
    "data['doc_id']=data.doc_id.str.strip()\n",
    "data['raw_text']=data.raw_text.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  remove any http links\n",
    "-  remove retweets\n",
    "-  remove @user_names of social mediasites\n",
    "-  remove #tags\n",
    "-  remove emoji's\n",
    "-  remove dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191997, 2)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "data['raw_text'] = data['raw_text'].apply(lambda x: re.sub('/(^\\w+:|^)\\/\\// | https?:\\/\\// | http?:\\/\\// | ((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "\n",
    "# RT (Retweet) keyword removal\n",
    "\n",
    "data['raw_text'] = data['raw_text'].apply(lambda x: re.sub('RT','',x))\n",
    "\n",
    "# remove @names mentioned as part of tweets\n",
    "data['raw_text']= data['raw_text'].apply(lambda x: re.sub('\\@[a-zA-Z0-9_]+','',x))\n",
    "\n",
    "# remove #tags mentioned as part of tweets\n",
    "data['raw_text']= data['raw_text'].apply(lambda x: re.sub('\\#[a-zA-Z0-9_]+','',x))\n",
    "\n",
    "# Remove emoji's from the documents\n",
    "data['raw_text']= data['raw_text'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))\n",
    "\n",
    "# Date Removal/ number removal from the documents\n",
    "data['raw_text'] = data['raw_text'].apply(lambda x: re.sub('[\\d]+',' ',x))\n",
    "\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184152, 3)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punctuation removal on 'text_data' column\n",
    "\n",
    "import re\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "data['raw_text'] = data['raw_text'].apply(lambda x: regex.sub(' ', x))\n",
    "data['raw_text']=data.raw_text.str.strip()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  remove any other english text from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove english letters or words\n",
    "data['raw_text'] = data['raw_text'].apply(lambda x: ' '.join([line.strip() for line in x.strip().splitlines()]))\n",
    "data['raw_text']= data['raw_text'].apply(lambda x: re.sub('[a-zA-Z0-9_]+','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the length of each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word count (length) of the document\n",
    "data['length'] = data['raw_text'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "data=data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove documents with less than 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184152, 3)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = data.loc[data.length>5]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189516    இலக்கியம்   ஒரு அப்பாவிப் பெண் அச்சத்தை துறக்க...\n",
       "Name: raw_text, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.doc_id==\"TAM_WL_004645_20160826_H0024KGMX\",\"raw_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\New folder\\\\r1.txt\", header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184152, 3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110888</th>\n",
       "      <td>TAM_WL_003129_20101226_H0024EUL2</td>\n",
       "      <td>இந்தியாவில் இருந்த   மதங்களில் எது இந்து மதம்</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139765</th>\n",
       "      <td>TAM_WL_004634_20150924_H0024KWON</td>\n",
       "      <td>ஜித்தா அதிரையர்களின் ஹஜ்ஜுப் பெருநாள் கொண்டாட்...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  doc_id  \\\n",
       "110888  TAM_WL_003129_20101226_H0024EUL2   \n",
       "139765  TAM_WL_004634_20150924_H0024KWON   \n",
       "\n",
       "                                                 raw_text  length  \n",
       "110888      இந்தியாவில் இருந்த   மதங்களில் எது இந்து மதம்       6  \n",
       "139765  ஜித்தா அதிரையர்களின் ஹஜ்ஜுப் பெருநாள் கொண்டாட்...       6  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAM_WL_003129_20101226_H0024EUL2</td>\n",
       "      <td>இந்தியாவில் இருந்த   மதங்களில் எது இந்து மதம்</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAM_WL_004634_20150924_H0024KWON</td>\n",
       "      <td>ஜித்தா அதிரையர்களின் ஹஜ்ஜுப் பெருநாள் கொண்டாட்...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id  \\\n",
       "0  TAM_WL_003129_20101226_H0024EUL2   \n",
       "1  TAM_WL_004634_20150924_H0024KWON   \n",
       "\n",
       "                                            raw_text  length  \n",
       "0      இந்தியாவில் இருந்த   மதங்களில் எது இந்து மதம்       6  \n",
       "1  ஜித்தா அதிரையர்களின் ஹஜ்ஜுப் பெருநாள் கொண்டாட்...       6  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index(drop=True,inplace=True)\n",
    "data.head(2)\n",
    "# reset index data fed into TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Binary Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184152, 103785)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse as ss\n",
    "#encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=True)\n",
    "# Document Term matrix (binary matrix) with max_df =0.997, min_df =1\n",
    "vectorizer = TfidfVectorizer(encoding='utf-8',stop_words=None, max_df =0.997, min_df =20,binary = True,lowercase=False,tokenizer=lambda x: x.split())\n",
    "doc_word_mat = vectorizer.fit_transform(data.raw_text)\n",
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corex with no anchroing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x1babf429a58>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the CorEx topic model with 25 topics\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=500, verbose=False, seed=3192)\n",
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: என்ன,என்றால்,எல்லாம்,எப்படி,நான்,நீங்கள்,ஏன்,கூட,இப்படி,நாம்\n",
      "1: தொடர்பில்,அரசாங்கம்,ஜனாதிபதி,மாகாண,எதிர்வரும்,இலங்கை,மைத்திரிபால,சிறிலங்கா,சென்னை,கூட்டமைப்பின்\n",
      "2: ரிஷபம்,மகரம்,விருச்சிகம்,சிம்மம்,மிதுனம்,கடகம்,துலாம்,மேஷம்,மீனம்,தனுசு\n",
      "3: இருக்கு,இப்ப,நீங்க,ஆனா,அப்ப,எங்க,எண்டு,நம்ம,இங்க,மாதிரி\n",
      "4: சாப்பிட்டால்,உடலில்,வைட்டமின்,அரைத்து,சாறு,உணவில்,தடவி,எலுமிச்சை,வந்தால்,சத்து\n",
      "5: அலுவலர்கள்,கலெக்டர்,ஜெயலலிதா,அலுவலர்,தமிழ்நாடு,ஊராட்சி,வழங்கினார்,ஊரக,சட்டமன்ற,முதலமைச்சர்\n",
      "6: போட்டியில்,அணி,கிரிக்கெட்,ரன்கள்,அணியின்,டெஸ்ட்,அணிக்கு,ஆட்டத்தில்,போட்டி,விக்கெட்\n",
      "7: பொலிஸார்,வைத்தியசாலையில்,விசாரணைகளை,சடலம்,சம்பவம்,உயிரிழந்துள்ளார்,மேலதிக,வயதுடைய,காவற்துறை,பிரதேசத்தில்\n",
      "8: ஆலய,ஸ்ரீ,பக்தர்கள்,திருவிழா,வருடாந்த,அம்மன்,இலைஹி,ராஜிவூன்,காரைதீவு,இறப்புச்\n",
      "9: தெரி­வித்தார்,தற்­போது,ஜனா­தி­பதி,இவ்­வாறு,இடம்­பெற்ற,சர்­வ­தேச,அர­சாங்கம்,உள்­ளது,எதிர்­வரும்,தொடர்­பாக\n",
      "10: அகவணக்கம்,கேணல்,வணக்க,மலர்வணக்கம்,தமிழீழத்,மாவீரர்,நினைவு,நம்புங்கள்,லெப்,தமிழீழ\n",
      "11: பின்தொடர,இன்ஸ்டாகிராமில்,இன்ஸ்டாகிராம்,காணொளிகளை,டியூப்,டிவிட்டரில்,முகநூல்,ட்விட்டர்,தமிழை,பிபிசி\n",
      "12: விண்வெளி,நாசா,விண்கலம்,விஞ்ஞானிகள்,ஜிபி,விண்ணில்,ஆன்ட்ராய்டு,கிரகத்தில்,ராக்கெட்,கிரகத்தின்\n",
      "13: அகதிகளாகச்,நெடுஞ்சாலைக்கு,நெருக்கிய,பணவீக்க,பணியாற்றுபவர்,பதிலளிப்பது,பதிவிட்டுள்ள,பதிவிற்கு,பரவலைத்,பாங்கொக்கில்\n",
      "14: அக்கட்சியைச்,படுத்தப்பட்டுள்ளது,பதவிநீக்கம்,பயணிக்கக்,பயணிக்கின்றனர்,பயணியாக,பலகட்ட,பல்கலைக்கழகத்திலிருந்து,பல்கலைக்கழகத்துடன்,பங்களாதேசில்\n",
      "15: அகதிகளுக்கும்,தொடர்பாளரான,தொடுத்துள்ளது,தொழிற்சங்கங்களை,தொழில்நுட்பங்களைப்,தோட்டாக்களை,நகராக,நடத்துவேன்,நடமாட்டமும்,நிபுணர்களைக்\n",
      "16: அங்கீகரிக்கப்பட்டுள்ள,பிடித்தவாறு,பாதுக்காப்பு,பாதுகாக்கப்பட்டது,பயணிக்கலாம்,பத்,பதிவாகியுள்ள,பணியாற்றுவதாக,படைகளுக்கும்,பங்கேற்றவர்களில்\n",
      "17: அச்சமடைந்து,பங்கேற்பார்கள்,பட்டதாரியான,பதாக,பயணத்திற்குத்,பயன்படுத்தியதால்,பரிதவிப்பு,பலிக்குமா,பலியாகும்,பல்கலைகழகத்தில்\n",
      "18: அகஸ்டஸ்,பயங்கரவாதிகளுக்கும்,பயணித்தனர்,பயன்படுத்தப்படாமல்,பரப்பளவுடைய,பரப்பியதாக,பரிந்துரைத்துள்ள,பதிவுக்கான,பர்மாவில்,பார்ப்பதாக\n",
      "19: அகற்றப்பட்டன,பணிக்கும்,பதிவாகியிருந்தன,பத்திரிகையாளரும்,பயங்கரவாதிகளால்,பயணமாகும்,பயன்படுத்தியதாகவும்,பட்டவர்களை,பயன்படுத்தியுள்ளது,பற்றாக்குறையாக\n",
      "20: அடைக்கப்பட்டிருந்தார்,நிறுத்தத்தின்,நிலையினால்,நோயாளிகளையும்,பங்களிப்பில்,பசுமாடுகள்,பணிகளுக்காகவும்,நிர்வாணப்படுத்தி,பணியாற்றியபோது,பத்திரங்களில்\n",
      "21: அகதியின்,நிர்வாகங்களுக்கு,நிர்வாகியும்,நீக்கக்கோரி,நீடித்துவரும்,நெதர்லாந்தைச்,படகையும்,படியை,நினைவூட்டல்,படுத்தவேண்டும்\n",
      "22: அங்காடிக்கு,நிறுவனத்துக்கும்,பங்கேற்கவும்,பட்டின,பட்டிமன்ற,பதிலளித்தது,பரப்பினால்,பராமரிப்பின்றி,பள்ளிக்கூடங்களை,பாகத்திலும்\n",
      "23: அகதிகளான,பண்டிகையில்,பதியப்பட்டுள்ளது,பதிலளிப்பார்,பயன்படுத்தப்படுவதை,பரப்பப்படுகிறது,பரவாது,பரிந்துரைத்தார்,நீதிபதியிடம்,பலப்படுத்தப்பட்டிருந்தது\n",
      "24: அங்காங்கே,பதவியேற்றது,பனாமா,பயங்கரவாதிகளும்,பயன்படுத்தியுள்ளார்,பரவுவதாக,பல்கலையின்,பாகிஸ்தானிலிருந்து,பாதைகளிலும்,பாலமும்\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.88029205e-03 6.22480363e-02 1.00000000e-06 ... 4.62060790e-01\n",
      "  1.74177411e-01 2.67973447e-01]\n",
      " [2.31883624e-04 7.03238488e-03 1.00000000e-06 ... 4.62555788e-01\n",
      "  1.73867518e-01 2.67634597e-01]\n",
      " [7.85545498e-04 2.00478268e-02 1.00000000e-06 ... 4.62160145e-01\n",
      "  1.74169280e-01 2.67998034e-01]\n",
      " ...\n",
      " [9.99999000e-01 9.99999000e-01 1.61170886e-05 ... 4.57670581e-01\n",
      "  1.76742657e-01 2.70702235e-01]\n",
      " [9.99999000e-01 1.00000000e-06 5.72757090e-05 ... 4.60503697e-01\n",
      "  1.74977988e-01 2.68776003e-01]\n",
      " [9.99999000e-01 4.04975499e-01 5.95242386e-06 ... 4.59533281e-01\n",
      "  1.75567617e-01 2.69431765e-01]]\n"
     ]
    }
   ],
   "source": [
    "#The estimated probabilities of topics for each document can be accessed through p_y_given_x.\n",
    "print(topic_model.p_y_given_x) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [ True  True False ... False False False]\n",
      " [ True False False ... False False False]\n",
      " [ True False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "print(topic_model.labels) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corex with anchoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "anchor_words = [[\"வெள்ளப்பெருக்கு\",\"பெருவெள்ளம்\"  , \"வெள்ளத்தில்\" ,\n",
    "  \"வெள்ளம்\" , \"பெருக்கெடுத்து\" , \"பலமான\" , \"மழை\", \"கனமழை\" , \"புயல்\" , \"சூறாவளி\" ,\n",
    " \"காற்று\" , \"மண்டலம்\",\n",
    " \"ஆலங்கட்டி\" , \"இடியுடன்\" ]]\n",
    "anchored_topic_model = Corex(n_hidden=25, max_iter=500,seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: மழை,வெள்ளம்,கனமழை,இடியுடன்,புயல்,சூறாவளி,காற்று,வெள்ளப்பெருக்கு,வெள்ளத்தில்,பெருக்கெடுத்து\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax binary classification of which documents belong to the topics of anchored words\n",
    "#(in this case, we have words seeded for 1 topic, hence checking the classification for that topics) \n",
    "topic_classification=anchored_topic_model.labels[:,0:1] # n_docs x k_topics\n",
    "# Storing the classification results for the first 3 topics (seeded)\n",
    "result=pd.DataFrame(topic_classification,columns=[\"predicted_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAM_WL_003129_20101226_H0024EUL2</td>\n",
       "      <td>இந்தியாவில் இருந்த   மதங்களில் எது இந்து மதம்</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAM_WL_004634_20150924_H0024KWON</td>\n",
       "      <td>ஜித்தா அதிரையர்களின் ஹஜ்ஜுப் பெருநாள் கொண்டாட்...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id  \\\n",
       "0  TAM_WL_003129_20101226_H0024EUL2   \n",
       "1  TAM_WL_004634_20150924_H0024KWON   \n",
       "\n",
       "                                            raw_text  predicted_label  \n",
       "0      இந்தியாவில் இருந்த   மதங்களில் எது இந்து மதம்            False  \n",
       "1  ஜித்தா அதிரையர்களின் ஹஜ்ஜுப் பெருநாள் கொண்டாட்...            False  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.concat( [data.iloc[:,0:2], result], axis=1)\n",
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=df_data.loc[(df_data['predicted_label']==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.to_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\New folder\\\\rc2.txt\", header=True, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>TAM_SN_000370_20170513_H0T10FQ4H</td>\n",
       "      <td>மாரி மழை பெய்யாதோ மக்கள் பஞ்சம் தீர</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               doc_id  \\\n",
       "147  TAM_SN_000370_20170513_H0T10FQ4H   \n",
       "\n",
       "                                       raw_text  predicted_label  \n",
       "147         மாரி மழை பெய்யாதோ மக்கள் பஞ்சம் தீர             True  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 documents related to flood and disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\user\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183654</th>\n",
       "      <td>TAM_WL_003129_20110215_H0024E45N</td>\n",
       "      <td>மழைவேண்டிப் பிரார்த்தித்தல் அத்தியாயம் பாடம் ம...</td>\n",
       "      <td>True</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176658</th>\n",
       "      <td>TAM_DF_003507_20050504_H00242EKZ</td>\n",
       "      <td>கிராமியம் மழை சார்ந்த பழமொழிகள் கிராமியம் மழை ...</td>\n",
       "      <td>True</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173106</th>\n",
       "      <td>TAM_NW_003461_20130214_H0023V69V</td>\n",
       "      <td>மதுரை   ராமநாதபுரம் மாவட்டங்களில் சூறாவளி காற்...</td>\n",
       "      <td>True</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175945</th>\n",
       "      <td>TAM_WL_004635_20151205_H0024KBXY</td>\n",
       "      <td>ஆண்டு பார்த்திராத சோகம்       நாட்டின் நான்காவ...</td>\n",
       "      <td>True</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183161</th>\n",
       "      <td>TAM_WL_004645_20160102_H0024KRT5</td>\n",
       "      <td>சென்னை   இயற்கையை அழித்த குற்றத்தின் தண்டனை   ...</td>\n",
       "      <td>True</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173017</th>\n",
       "      <td>TAM_NW_003461_20130305_H0023UOW1</td>\n",
       "      <td>வங்கக்கடலில் குறைந்த காற்றழுத்த தாழ்வுநிலை தஞ்...</td>\n",
       "      <td>True</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174264</th>\n",
       "      <td>TAM_NW_003461_20130306_H0023UOVX</td>\n",
       "      <td>தமிழகத்தில்   மாவட்டங்களில் தொடர் மழை சென்னை  ...</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173019</th>\n",
       "      <td>TAM_NW_003461_20150528_H0023V6UT</td>\n",
       "      <td>தமிழகத்தில் பல மாவட்டங்களில் சூறாவளி காற்று   ...</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172967</th>\n",
       "      <td>TAM_NW_003461_20150513_H0023V6VL</td>\n",
       "      <td>தமிழ்நாடு முழுவதும் பலத்த மழை   அக்னி வெயிலின்...</td>\n",
       "      <td>True</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180486</th>\n",
       "      <td>TAM_NW_003461_20141020_H0023V7C3</td>\n",
       "      <td>தமிழகம் முழுவதும் தொடர் அடை மழை சென்னை அக்   –...</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166551</th>\n",
       "      <td>TAM_WL_004108_20091028_H0024HLFH</td>\n",
       "      <td>காலதாமதமான வடகிழக்கு பருவ மழை துவங்கியது தமிழக...</td>\n",
       "      <td>True</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175371</th>\n",
       "      <td>TAM_NW_003461_20150623_H0023UPP8</td>\n",
       "      <td>நீலகிரி மாவட்டத்தில் நீடிக்கும் கனமழை பள்ளி   ...</td>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175960</th>\n",
       "      <td>TAM_WL_003123_20081126_G0023GED2</td>\n",
       "      <td>புயல் தமிழகக் கடற்கரை ஓரங்களில் புயல் அடிப்பது...</td>\n",
       "      <td>True</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182491</th>\n",
       "      <td>TAM_DF_004640_20151223_H0024J8KU</td>\n",
       "      <td>தமிழகத்தினை உலுக்கிய பெரு வெள்ள   ஆழி பேரழிவு ...</td>\n",
       "      <td>True</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165678</th>\n",
       "      <td>TAM_NW_003461_20141006_H0023V7CZ</td>\n",
       "      <td>சேலத்தில் திடீர் ஆலங்கட்டி மழை சென்னை   அக்   ...</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165292</th>\n",
       "      <td>TAM_NW_003461_20131125_H0023VEN7</td>\n",
       "      <td>வங்கக் கடலில் உருவான புதிய ‘ லெஹர் ’ புயல் மசூ...</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183573</th>\n",
       "      <td>TAM_NW_003461_20160121_H0023UEEH</td>\n",
       "      <td>செம்பரம்பாக்கம் ஏரி திறப்பு   எதிர்க்கட்சி குற...</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158350</th>\n",
       "      <td>TAM_NW_003461_20160607_H0023UEFU</td>\n",
       "      <td>கேரளாவில் தென்மேற்கு பருவமழை துவங்கியது சென்னை...</td>\n",
       "      <td>True</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162280</th>\n",
       "      <td>TAM_WL_003111_20131213_G0023GGIC</td>\n",
       "      <td>கடலூர் மாவட்டத்தில் பரவலாக மழை   இன்று பள்ளிகள...</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172427</th>\n",
       "      <td>TAM_NW_003461_20141121_H0023V7AM</td>\n",
       "      <td>தென் மாவட்டங்களில் கன மழை   பள்ளி   கல்லூரிகளு...</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163261</th>\n",
       "      <td>TAM_NW_003461_20161103_H0023U6XZ</td>\n",
       "      <td>தமிழகத்தில் இடியுடன் கனமழை பெய்யும் காற்றழுத்த...</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169448</th>\n",
       "      <td>TAM_WL_004626_20161208_H0024HSIB</td>\n",
       "      <td>சென்னையை அச்சுறுத்தும் புயல்   பலத்த மழை பெய்ய...</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182649</th>\n",
       "      <td>TAM_WL_004626_20110111_H0024HS8P</td>\n",
       "      <td>கிழக்கில் ஓயாத மழை – மக்கள் அவதி –   பேர் பலி ...</td>\n",
       "      <td>True</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179428</th>\n",
       "      <td>TAM_NW_003461_20141028_H0023V7BM</td>\n",
       "      <td>கோவை   ஈரோடு   திருப்பூரில் வெள்ள அபாய எச்சரிக...</td>\n",
       "      <td>True</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167637</th>\n",
       "      <td>TAM_WL_004635_20161106_H0024JQWF</td>\n",
       "      <td>காலம் தவறி பெய்யும் பருவமழையால் பாதிப்பு   இனி...</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170734</th>\n",
       "      <td>TAM_NW_003461_20141023_H0023V7BW</td>\n",
       "      <td>கனமழை   முழு வீச்சில் நிவாரணப் பணிகள்   ஆண்டுக...</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170092</th>\n",
       "      <td>TAM_NW_003460_20001130_70R002PEX</td>\n",
       "      <td>கடலூர் புதுவையில்   கி மீட்டர் வேகத்தில் சூறைக...</td>\n",
       "      <td>True</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182050</th>\n",
       "      <td>TAM_NW_003461_20151207_H0023UPDS</td>\n",
       "      <td>குடிசைகளை இழந்தவருக்கு ரூ   ஆயிரம்   பாதிக்கப்...</td>\n",
       "      <td>True</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181449</th>\n",
       "      <td>TAM_WL_004636_20141115_H0024I4FR</td>\n",
       "      <td>மேகங்கள்   பற்றி குர்ஆன் கூறும் உண்மைகள் இஸ்லா...</td>\n",
       "      <td>True</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182769</th>\n",
       "      <td>TAM_WL_004645_20160106_H0024KRSZ</td>\n",
       "      <td>கடலூர் மாவட்டம்   பட்ட காலிலே பட்ட துயரம்     ...</td>\n",
       "      <td>True</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156608</th>\n",
       "      <td>TAM_WL_004638_20121018_H0024IHGP</td>\n",
       "      <td>வலுக்கிறது காற்றழுத்தம் தமிழகத்தில் மழை நீடிக்...</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167663</th>\n",
       "      <td>TAM_WL_004635_20151130_H0024KBZ4</td>\n",
       "      <td>நாட்களுக்கு அடித்துக் கொளுத்தப் போகும் கனமழை  ...</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175911</th>\n",
       "      <td>TAM_NW_003523_20151205_H0023VB0Z</td>\n",
       "      <td>உங்கள் கருத்து ம ழையால் ஏற்பட்ட வெள்ளத்தைவிட ஏ...</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140172</th>\n",
       "      <td>TAM_NW_003226_20060501_70R002D2S</td>\n",
       "      <td>செயற்கை மழை சில உண்மைகள் இயற்கையின் சீற்றம் இந...</td>\n",
       "      <td>True</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148411</th>\n",
       "      <td>TAM_NW_003461_20141017_H0023V7C9</td>\n",
       "      <td>வடகிழக்கு பருவமழை அறிகுறி துவங்கியது தமிழகத்தி...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172632</th>\n",
       "      <td>TAM_NW_003461_20151117_H0023UEP3</td>\n",
       "      <td>வெள்ள நிவாரணத்துக்கு ரூ   கோடி   ஜெயலலிதா உத்...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143038</th>\n",
       "      <td>TAM_WL_004635_20151229_H0024JO4K</td>\n",
       "      <td>வரலாறு காணாத மழை இங்கிலாந்தில் வெள்ளத்தில் மூழ...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160659</th>\n",
       "      <td>TAM_NW_003461_20131011_H0023VERG</td>\n",
       "      <td>ஒரிசா   ஆந்திராவுக்கு புயல் ஆபத்து   மீட்பு பண...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176263</th>\n",
       "      <td>TAM_WL_004645_20151208_H0024KRU5</td>\n",
       "      <td>சென்னை மழைக்கு எல் நினோ மட்டும்தான் காரணமா    ...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152220</th>\n",
       "      <td>TAM_WL_004636_20151114_H0024I4EN</td>\n",
       "      <td>புதிய காற்றழுத்த தாழ்வு நிலையால் மாவட்டத்தில் ...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157929</th>\n",
       "      <td>TAM_WL_004636_20141012_H0024I4G3</td>\n",
       "      <td>ஹூட் ஹூட் புயல் விசாகப்பட்டினத்தில் கரையை கடந்...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148887</th>\n",
       "      <td>TAM_NW_003461_20151229_H0023UEM2</td>\n",
       "      <td>தென்மாவட்ட கடலோர பகுதிகளில் இன்றும் மழை பெய்யு...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148571</th>\n",
       "      <td>TAM_NW_002923_20170710_H0024BSFB</td>\n",
       "      <td>பாரிஸில்   நாள் மழை இரண்டே மணி நேரத்தில் பிரான...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146983</th>\n",
       "      <td>TAM_NW_003461_20141020_H0023V7C4</td>\n",
       "      <td>சென்னையில் தேங்கிய மழைநீர்   மோட்டார் பம்ப் மூ...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144052</th>\n",
       "      <td>TAM_NW_003461_20151003_H0023UPIF</td>\n",
       "      <td>சென்னையில் மழை நீடிக்கும் சென்னை   அக் – தமிழக...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168797</th>\n",
       "      <td>TAM_DF_003507_20050922_H00244XR1</td>\n",
       "      <td>டெக்ஸாஸ் நோக்கி ரீட்டா சூறாவளி ரீட்டா சூறாவளி ...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156483</th>\n",
       "      <td>TAM_NW_003461_20131120_H0023VENP</td>\n",
       "      <td>வங்கக் கடலில் உருவான காற்றழுத்த தாழ்வு மண்டலம்...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180849</th>\n",
       "      <td>TAM_WL_004616_20161017_H0024HS2X</td>\n",
       "      <td>அமெரிக்க நகரங்களை ஆண்டுதோறும் நரகம் ஆக்கும் அச...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158256</th>\n",
       "      <td>TAM_WL_003111_20161019_G0023GBSN</td>\n",
       "      <td>தொடரும் கன மழை மாவட்டத்தில் அதிக பட்சமாக பரங்க...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176021</th>\n",
       "      <td>TAM_WL_004626_20141230_H0024IFZX</td>\n",
       "      <td>இயற்­கை அனர்த்­தங்கள் கார­ண­மாக   குடும்­பங்­க...</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157978</th>\n",
       "      <td>TAM_NW_003461_20170925_H0024T1AJ</td>\n",
       "      <td>சென்னை செப்   – வளிமண்டல கீழ் அடுக்கு சுழற்சி ...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176472</th>\n",
       "      <td>TAM_NW_003461_20151223_H0023UDTD</td>\n",
       "      <td>பிரதமருக்கு முதல்வர் ஜெயலலிதா கடிதம் சென்னை   ...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160272</th>\n",
       "      <td>TAM_WL_004635_20121030_H0024K4M9</td>\n",
       "      <td>அமெரிக்காவின் கிழக்கு கடற்கரைப் பகுதியை சாண்டி...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145649</th>\n",
       "      <td>TAM_NW_003461_20150620_H0023UPUZ</td>\n",
       "      <td>மேற்கு தொடர்ச்சி மலையோர மாவட்டங்களில் பரவலாக ம...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179314</th>\n",
       "      <td>TAM_WL_004645_20151212_H0024KRTU</td>\n",
       "      <td>மழை வெள்ளம்   தமிழக அரசுதான் குற்றவாளி – மக்கள...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162326</th>\n",
       "      <td>TAM_NW_003461_20131122_H0023VENI</td>\n",
       "      <td>ஆந்திராவில்   மாவட்டத்துக்கு எச்சரிக்கை மசூலிப...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162644</th>\n",
       "      <td>TAM_NW_003461_20161130_H0023U6WA</td>\n",
       "      <td>வங்கக்கடலில் ‘ நாடா ’ புயல்   கன மழை எச்சரிக்க...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153450</th>\n",
       "      <td>TAM_WL_004635_20151208_H0024KBXD</td>\n",
       "      <td>கொட்டித்தீர்த்த கனமழையே வெள்ளப்பெருக்கு காரணம்...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162801</th>\n",
       "      <td>TAM_NW_003461_20140502_H0023UD5T</td>\n",
       "      <td>ஊட்டியில் இடி மின்னலுடன்   செ மீ   மழை ஊட்டி  ...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175698</th>\n",
       "      <td>TAM_NW_003461_20141028_H0023VE85</td>\n",
       "      <td>வடகிழக்கு பருவமழையினால் பாதிக்கப்பட்ட சாலை   க...</td>\n",
       "      <td>True</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  doc_id  \\\n",
       "183654  TAM_WL_003129_20110215_H0024E45N   \n",
       "176658  TAM_DF_003507_20050504_H00242EKZ   \n",
       "173106  TAM_NW_003461_20130214_H0023V69V   \n",
       "175945  TAM_WL_004635_20151205_H0024KBXY   \n",
       "183161  TAM_WL_004645_20160102_H0024KRT5   \n",
       "173017  TAM_NW_003461_20130305_H0023UOW1   \n",
       "174264  TAM_NW_003461_20130306_H0023UOVX   \n",
       "173019  TAM_NW_003461_20150528_H0023V6UT   \n",
       "172967  TAM_NW_003461_20150513_H0023V6VL   \n",
       "180486  TAM_NW_003461_20141020_H0023V7C3   \n",
       "166551  TAM_WL_004108_20091028_H0024HLFH   \n",
       "175371  TAM_NW_003461_20150623_H0023UPP8   \n",
       "175960  TAM_WL_003123_20081126_G0023GED2   \n",
       "182491  TAM_DF_004640_20151223_H0024J8KU   \n",
       "165678  TAM_NW_003461_20141006_H0023V7CZ   \n",
       "165292  TAM_NW_003461_20131125_H0023VEN7   \n",
       "183573  TAM_NW_003461_20160121_H0023UEEH   \n",
       "158350  TAM_NW_003461_20160607_H0023UEFU   \n",
       "162280  TAM_WL_003111_20131213_G0023GGIC   \n",
       "172427  TAM_NW_003461_20141121_H0023V7AM   \n",
       "163261  TAM_NW_003461_20161103_H0023U6XZ   \n",
       "169448  TAM_WL_004626_20161208_H0024HSIB   \n",
       "182649  TAM_WL_004626_20110111_H0024HS8P   \n",
       "179428  TAM_NW_003461_20141028_H0023V7BM   \n",
       "167637  TAM_WL_004635_20161106_H0024JQWF   \n",
       "170734  TAM_NW_003461_20141023_H0023V7BW   \n",
       "170092  TAM_NW_003460_20001130_70R002PEX   \n",
       "182050  TAM_NW_003461_20151207_H0023UPDS   \n",
       "181449  TAM_WL_004636_20141115_H0024I4FR   \n",
       "182769  TAM_WL_004645_20160106_H0024KRSZ   \n",
       "...                                  ...   \n",
       "156608  TAM_WL_004638_20121018_H0024IHGP   \n",
       "167663  TAM_WL_004635_20151130_H0024KBZ4   \n",
       "175911  TAM_NW_003523_20151205_H0023VB0Z   \n",
       "140172  TAM_NW_003226_20060501_70R002D2S   \n",
       "148411  TAM_NW_003461_20141017_H0023V7C9   \n",
       "172632  TAM_NW_003461_20151117_H0023UEP3   \n",
       "143038  TAM_WL_004635_20151229_H0024JO4K   \n",
       "160659  TAM_NW_003461_20131011_H0023VERG   \n",
       "176263  TAM_WL_004645_20151208_H0024KRU5   \n",
       "152220  TAM_WL_004636_20151114_H0024I4EN   \n",
       "157929  TAM_WL_004636_20141012_H0024I4G3   \n",
       "148887  TAM_NW_003461_20151229_H0023UEM2   \n",
       "148571  TAM_NW_002923_20170710_H0024BSFB   \n",
       "146983  TAM_NW_003461_20141020_H0023V7C4   \n",
       "144052  TAM_NW_003461_20151003_H0023UPIF   \n",
       "168797  TAM_DF_003507_20050922_H00244XR1   \n",
       "156483  TAM_NW_003461_20131120_H0023VENP   \n",
       "180849  TAM_WL_004616_20161017_H0024HS2X   \n",
       "158256  TAM_WL_003111_20161019_G0023GBSN   \n",
       "176021  TAM_WL_004626_20141230_H0024IFZX   \n",
       "157978  TAM_NW_003461_20170925_H0024T1AJ   \n",
       "176472  TAM_NW_003461_20151223_H0023UDTD   \n",
       "160272  TAM_WL_004635_20121030_H0024K4M9   \n",
       "145649  TAM_NW_003461_20150620_H0023UPUZ   \n",
       "179314  TAM_WL_004645_20151212_H0024KRTU   \n",
       "162326  TAM_NW_003461_20131122_H0023VENI   \n",
       "162644  TAM_NW_003461_20161130_H0023U6WA   \n",
       "153450  TAM_WL_004635_20151208_H0024KBXD   \n",
       "162801  TAM_NW_003461_20140502_H0023UD5T   \n",
       "175698  TAM_NW_003461_20141028_H0023VE85   \n",
       "\n",
       "                                                 raw_text  predicted_label  \\\n",
       "183654  மழைவேண்டிப் பிரார்த்தித்தல் அத்தியாயம் பாடம் ம...             True   \n",
       "176658  கிராமியம் மழை சார்ந்த பழமொழிகள் கிராமியம் மழை ...             True   \n",
       "173106  மதுரை   ராமநாதபுரம் மாவட்டங்களில் சூறாவளி காற்...             True   \n",
       "175945  ஆண்டு பார்த்திராத சோகம்       நாட்டின் நான்காவ...             True   \n",
       "183161  சென்னை   இயற்கையை அழித்த குற்றத்தின் தண்டனை   ...             True   \n",
       "173017  வங்கக்கடலில் குறைந்த காற்றழுத்த தாழ்வுநிலை தஞ்...             True   \n",
       "174264  தமிழகத்தில்   மாவட்டங்களில் தொடர் மழை சென்னை  ...             True   \n",
       "173019  தமிழகத்தில் பல மாவட்டங்களில் சூறாவளி காற்று   ...             True   \n",
       "172967  தமிழ்நாடு முழுவதும் பலத்த மழை   அக்னி வெயிலின்...             True   \n",
       "180486  தமிழகம் முழுவதும் தொடர் அடை மழை சென்னை அக்   –...             True   \n",
       "166551  காலதாமதமான வடகிழக்கு பருவ மழை துவங்கியது தமிழக...             True   \n",
       "175371  நீலகிரி மாவட்டத்தில் நீடிக்கும் கனமழை பள்ளி   ...             True   \n",
       "175960  புயல் தமிழகக் கடற்கரை ஓரங்களில் புயல் அடிப்பது...             True   \n",
       "182491  தமிழகத்தினை உலுக்கிய பெரு வெள்ள   ஆழி பேரழிவு ...             True   \n",
       "165678  சேலத்தில் திடீர் ஆலங்கட்டி மழை சென்னை   அக்   ...             True   \n",
       "165292  வங்கக் கடலில் உருவான புதிய ‘ லெஹர் ’ புயல் மசூ...             True   \n",
       "183573  செம்பரம்பாக்கம் ஏரி திறப்பு   எதிர்க்கட்சி குற...             True   \n",
       "158350  கேரளாவில் தென்மேற்கு பருவமழை துவங்கியது சென்னை...             True   \n",
       "162280  கடலூர் மாவட்டத்தில் பரவலாக மழை   இன்று பள்ளிகள...             True   \n",
       "172427  தென் மாவட்டங்களில் கன மழை   பள்ளி   கல்லூரிகளு...             True   \n",
       "163261  தமிழகத்தில் இடியுடன் கனமழை பெய்யும் காற்றழுத்த...             True   \n",
       "169448  சென்னையை அச்சுறுத்தும் புயல்   பலத்த மழை பெய்ய...             True   \n",
       "182649  கிழக்கில் ஓயாத மழை – மக்கள் அவதி –   பேர் பலி ...             True   \n",
       "179428  கோவை   ஈரோடு   திருப்பூரில் வெள்ள அபாய எச்சரிக...             True   \n",
       "167637  காலம் தவறி பெய்யும் பருவமழையால் பாதிப்பு   இனி...             True   \n",
       "170734  கனமழை   முழு வீச்சில் நிவாரணப் பணிகள்   ஆண்டுக...             True   \n",
       "170092  கடலூர் புதுவையில்   கி மீட்டர் வேகத்தில் சூறைக...             True   \n",
       "182050  குடிசைகளை இழந்தவருக்கு ரூ   ஆயிரம்   பாதிக்கப்...             True   \n",
       "181449  மேகங்கள்   பற்றி குர்ஆன் கூறும் உண்மைகள் இஸ்லா...             True   \n",
       "182769  கடலூர் மாவட்டம்   பட்ட காலிலே பட்ட துயரம்     ...             True   \n",
       "...                                                   ...              ...   \n",
       "156608  வலுக்கிறது காற்றழுத்தம் தமிழகத்தில் மழை நீடிக்...             True   \n",
       "167663  நாட்களுக்கு அடித்துக் கொளுத்தப் போகும் கனமழை  ...             True   \n",
       "175911  உங்கள் கருத்து ம ழையால் ஏற்பட்ட வெள்ளத்தைவிட ஏ...             True   \n",
       "140172  செயற்கை மழை சில உண்மைகள் இயற்கையின் சீற்றம் இந...             True   \n",
       "148411  வடகிழக்கு பருவமழை அறிகுறி துவங்கியது தமிழகத்தி...             True   \n",
       "172632  வெள்ள நிவாரணத்துக்கு ரூ   கோடி   ஜெயலலிதா உத்...             True   \n",
       "143038  வரலாறு காணாத மழை இங்கிலாந்தில் வெள்ளத்தில் மூழ...             True   \n",
       "160659  ஒரிசா   ஆந்திராவுக்கு புயல் ஆபத்து   மீட்பு பண...             True   \n",
       "176263  சென்னை மழைக்கு எல் நினோ மட்டும்தான் காரணமா    ...             True   \n",
       "152220  புதிய காற்றழுத்த தாழ்வு நிலையால் மாவட்டத்தில் ...             True   \n",
       "157929  ஹூட் ஹூட் புயல் விசாகப்பட்டினத்தில் கரையை கடந்...             True   \n",
       "148887  தென்மாவட்ட கடலோர பகுதிகளில் இன்றும் மழை பெய்யு...             True   \n",
       "148571  பாரிஸில்   நாள் மழை இரண்டே மணி நேரத்தில் பிரான...             True   \n",
       "146983  சென்னையில் தேங்கிய மழைநீர்   மோட்டார் பம்ப் மூ...             True   \n",
       "144052  சென்னையில் மழை நீடிக்கும் சென்னை   அக் – தமிழக...             True   \n",
       "168797  டெக்ஸாஸ் நோக்கி ரீட்டா சூறாவளி ரீட்டா சூறாவளி ...             True   \n",
       "156483  வங்கக் கடலில் உருவான காற்றழுத்த தாழ்வு மண்டலம்...             True   \n",
       "180849  அமெரிக்க நகரங்களை ஆண்டுதோறும் நரகம் ஆக்கும் அச...             True   \n",
       "158256  தொடரும் கன மழை மாவட்டத்தில் அதிக பட்சமாக பரங்க...             True   \n",
       "176021  இயற்­கை அனர்த்­தங்கள் கார­ண­மாக   குடும்­பங்­க...             True   \n",
       "157978  சென்னை செப்   – வளிமண்டல கீழ் அடுக்கு சுழற்சி ...             True   \n",
       "176472  பிரதமருக்கு முதல்வர் ஜெயலலிதா கடிதம் சென்னை   ...             True   \n",
       "160272  அமெரிக்காவின் கிழக்கு கடற்கரைப் பகுதியை சாண்டி...             True   \n",
       "145649  மேற்கு தொடர்ச்சி மலையோர மாவட்டங்களில் பரவலாக ம...             True   \n",
       "179314  மழை வெள்ளம்   தமிழக அரசுதான் குற்றவாளி – மக்கள...             True   \n",
       "162326  ஆந்திராவில்   மாவட்டத்துக்கு எச்சரிக்கை மசூலிப...             True   \n",
       "162644  வங்கக்கடலில் ‘ நாடா ’ புயல்   கன மழை எச்சரிக்க...             True   \n",
       "153450  கொட்டித்தீர்த்த கனமழையே வெள்ளப்பெருக்கு காரணம்...             True   \n",
       "162801  ஊட்டியில் இடி மின்னலுடன்   செ மீ   மழை ஊட்டி  ...             True   \n",
       "175698  வடகிழக்கு பருவமழையினால் பாதிக்கப்பட்ட சாலை   க...             True   \n",
       "\n",
       "        count  \n",
       "183654    123  \n",
       "176658     68  \n",
       "173106     63  \n",
       "175945     61  \n",
       "183161     56  \n",
       "173017     55  \n",
       "174264     50  \n",
       "173019     49  \n",
       "172967     49  \n",
       "180486     47  \n",
       "166551     47  \n",
       "175371     46  \n",
       "175960     46  \n",
       "182491     44  \n",
       "165678     43  \n",
       "165292     43  \n",
       "183573     43  \n",
       "158350     41  \n",
       "162280     40  \n",
       "172427     40  \n",
       "163261     40  \n",
       "169448     40  \n",
       "182649     40  \n",
       "179428     39  \n",
       "167637     38  \n",
       "170734     38  \n",
       "170092     38  \n",
       "182050     37  \n",
       "181449     36  \n",
       "182769     36  \n",
       "...       ...  \n",
       "156608     26  \n",
       "167663     26  \n",
       "175911     26  \n",
       "140172     26  \n",
       "148411     25  \n",
       "172632     25  \n",
       "143038     25  \n",
       "160659     25  \n",
       "176263     25  \n",
       "152220     25  \n",
       "157929     25  \n",
       "148887     25  \n",
       "148571     25  \n",
       "146983     25  \n",
       "144052     25  \n",
       "168797     25  \n",
       "156483     25  \n",
       "180849     25  \n",
       "158256     25  \n",
       "176021     25  \n",
       "157978     24  \n",
       "176472     24  \n",
       "160272     24  \n",
       "145649     24  \n",
       "179314     24  \n",
       "162326     24  \n",
       "162644     24  \n",
       "153450     24  \n",
       "162801     24  \n",
       "175698     24  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 100 docs for topic \n",
    "r1['count'] = r1['raw_text'].apply(lambda x: pd.Series(x.split()).str.contains(r'வெள்ளப்பெருக்கு|பெருவெள்ளம்|வெள்ளத்தில்|வெள்ளம்|பெருக்கெடுத்து|பலமான|மழை|கனமழை|புயல்|சூறாவளி|காற்று|மண்டலம்|ஆலங்கட்டி|இடியுடன்').sum())\n",
    "r1.sort_values(by=['count'],ascending=False,inplace=True)\n",
    "r1.iloc[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.doc_id.to_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\Data\\\\New folder\\\\top100.csv\", header=None, index=None, sep=' ', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
