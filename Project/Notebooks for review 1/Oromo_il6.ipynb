{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corex code copied from https://github.com/gregversteeg/corex_topic/blob/master/corex_topic.py\n",
    "\n",
    "import numpy as np  # Tested with 1.8.0\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from scipy.special import logsumexp  # Tested with 0.13.0\n",
    "import scipy.sparse as ss\n",
    "from six import string_types # For Python 2&3 compatible string checking\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "class Corex(object):\n",
    "    \"\"\"\n",
    "    Anchored CorEx hierarchical topic models\n",
    "    Code follows sklearn naming/style (e.g. fit(X) to train)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_hidden : int, optional, default=2\n",
    "        Number of hidden units.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations before ending.\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode. 1 outputs TC(X;Y) as you go\n",
    "        2 output alpha matrix and MIs as you go.\n",
    "    tree : bool, default=True\n",
    "        In a tree model, each word can only appear in one topic. tree=False is not yet implemented.\n",
    "    count : string, {'binarize', 'fraction'}\n",
    "        Whether to treat counts (>1) by directly binarizing them, or by constructing a fractional count in [0,1].\n",
    "    seed : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    labels : array, [n_samples, n_hidden]\n",
    "        Label for each hidden unit for each sample.\n",
    "    clusters : array, [n_visible]\n",
    "        Cluster label for each input variable.\n",
    "    p_y_given_x : array, [n_samples, n_hidden]\n",
    "        p(y_j=1|x) for each sample.\n",
    "    alpha : array-like, shape [n_hidden, n_visible]\n",
    "        Adjacency matrix between input variables and hidden units. In range [0,1].\n",
    "    mis : array, [n_hidden, n_visible]\n",
    "        Mutual information between each (visible/observed) variable and hidden unit\n",
    "    tcs : array, [n_hidden]\n",
    "        TC(X_Gj;Y_j) for each hidden unit\n",
    "    tc : float\n",
    "        Convenience variable = Sum_j tcs[j]\n",
    "    tc_history : array\n",
    "        Shows value of TC over the course of learning. Hopefully, it is converging.\n",
    "    words : list of strings\n",
    "        Feature names that label the corresponding columns of X\n",
    "    References\n",
    "    ----------\n",
    "    [1]     Greg Ver Steeg and Aram Galstyan. \"Discovering Structure in\n",
    "            High-Dimensional Data Through Correlation Explanation.\"\n",
    "            NIPS, 2014. arXiv preprint arXiv:1406.1222.\n",
    "    [2]     Greg Ver Steeg and Aram Galstyan. \"Maximally Informative\n",
    "            Hierarchical Representations of High-Dimensional Data\"\n",
    "            AISTATS, 2015. arXiv preprint arXiv:1410.7404.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_hidden=2, max_iter=200, eps=1e-5, seed=None, verbose=False, count='binarize',\n",
    "                 tree=True, **kwargs):\n",
    "        self.n_hidden = n_hidden  # Number of hidden factors to use (Y_1,...Y_m) in paper\n",
    "        self.max_iter = max_iter  # Maximum number of updates to run, regardless of convergence\n",
    "        self.eps = eps  # Change to signal convergence\n",
    "        self.tree = tree\n",
    "        np.random.seed(seed)  # Set seed for deterministic results\n",
    "        self.verbose = verbose\n",
    "        self.t = 20  # Initial softness of the soft-max function for alpha (see NIPS paper [1])\n",
    "        self.count = count  # Which strategy, if necessary, for binarizing count data\n",
    "        if verbose > 0:\n",
    "            np.set_printoptions(precision=3, suppress=True, linewidth=200)\n",
    "            print('corex, rep size:', n_hidden)\n",
    "        if verbose:\n",
    "            np.seterr(all='warn')\n",
    "            # Can change to 'raise' if you are worried to see where the errors are\n",
    "            # Locally, I \"ignore\" underflow errors in logsumexp that appear innocuous (probabilities near 0)\n",
    "        else:\n",
    "            np.seterr(all='ignore')\n",
    "\n",
    "    def label(self, p_y_given_x):\n",
    "        \"\"\"Maximum likelihood labels for some distribution over y's\"\"\"\n",
    "        return (p_y_given_x > 0.5).astype(bool)\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        \"\"\"Maximum likelihood labels for training data. Can access with self.labels (no parens needed)\"\"\"\n",
    "        return self.label(self.p_y_given_x)\n",
    "\n",
    "    @property\n",
    "    def clusters(self):\n",
    "        \"\"\"Return cluster labels for variables\"\"\"\n",
    "        return np.argmax(self.alpha, axis=0)\n",
    "\n",
    "    @property\n",
    "    def sign(self):\n",
    "        \"\"\"Return the direction of correlation, positive or negative, for each variable-latent factor.\"\"\"\n",
    "        return np.sign(self.theta[3] - self.theta[2]).T\n",
    "\n",
    "    @property\n",
    "    def tc(self):\n",
    "        \"\"\"The total correlation explained by all the Y's.\n",
    "        \"\"\"\n",
    "        return np.sum(self.tcs)\n",
    "\n",
    "    def fit(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"\n",
    "        Fit CorEx on the data X. See fit_transform.\n",
    "        \"\"\"\n",
    "        self.fit_transform(X, anchors=anchors, anchor_strength=anchor_strength, words=words, docs=docs)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, anchors=None, anchor_strength=1, words=None, docs=None):\n",
    "        \"\"\"Fit CorEx on the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse CSR or a numpy matrix, shape = [n_samples, n_visible]\n",
    "            Count data or some other sparse binary data.\n",
    "        anchors : A list of variables anchor each corresponding latent factor to.\n",
    "        anchor_strength : How strongly to weight the anchors.\n",
    "        words : list of strings that label the corresponding columns of X\n",
    "        docs : list of strings that label the corresponding rows of X\n",
    "        Returns\n",
    "        -------\n",
    "        Y: array-like, shape = [n_samples, n_hidden]\n",
    "           Learned values for each latent factor for each sample.\n",
    "           Y's are sorted so that Y_1 explains most correlation, etc.\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        self.initialize_parameters(X, words, docs)\n",
    "        if anchors is not None:\n",
    "            anchors = self.preprocess_anchors(list(anchors))\n",
    "        p_y_given_x = np.random.random((self.n_samples, self.n_hidden))\n",
    "        if anchors is not None:\n",
    "            for j, a in enumerate(anchors):\n",
    "                p_y_given_x[:, j] = 0.5 * p_y_given_x[:, j] + 0.5 * X[:, a].mean(axis=1).A1  # Assumes X is a binary matrix\n",
    "\n",
    "        for nloop in range(self.max_iter):\n",
    "            if nloop > 1:\n",
    "                for j in range(self.n_hidden):\n",
    "                    if self.sign[j, np.argmax(self.mis[j])] < 0:\n",
    "                        # Switch label for Y_j so that it is correlated with the top word\n",
    "                        p_y_given_x[:, j] = 1. - p_y_given_x[:, j]\n",
    "            self.log_p_y = self.calculate_p_y(p_y_given_x)\n",
    "            self.theta = self.calculate_theta(X, p_y_given_x, self.log_p_y)  # log p(x_i=1|y)  nv by m by k\n",
    "\n",
    "            if nloop > 0:  # Structure learning step\n",
    "                self.alpha = self.calculate_alpha(X, p_y_given_x, self.theta, self.log_p_y, self.tcs)\n",
    "            if anchors is not None:\n",
    "                for a in flatten(anchors):\n",
    "                    self.alpha[:, a] = 0\n",
    "                for ia, a in enumerate(anchors):\n",
    "                    self.alpha[ia, a] = anchor_strength\n",
    "\n",
    "            p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "\n",
    "            self.update_tc(log_z)  # Calculate TC and record history to check convergence\n",
    "            self.print_verbose()\n",
    "            if self.convergence():\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Overall tc:', self.tc)\n",
    "\n",
    "        if anchors is None:\n",
    "            self.sort_and_output(X)\n",
    "        self.p_y_given_x, self.log_p_y_given_x, self.log_z = self.calculate_latent(X, self.theta)  # Needed to output labels\n",
    "        self.mis = self.calculate_mis(self.theta, self.log_p_y)  # / self.h_x  # could normalize MIs\n",
    "        return self.labels\n",
    "\n",
    "    def transform(self, X, details=False):\n",
    "        \"\"\"\n",
    "        Label hidden factors for (possibly previously unseen) samples of data.\n",
    "        Parameters: samples of data, X, shape = [n_samples, n_visible]\n",
    "        Returns: , shape = [n_samples, n_hidden]\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        p_y_given_x, _, log_z = self.calculate_latent(X, self.theta)\n",
    "        labels = self.label(p_y_given_x)\n",
    "        if details == 'surprise':\n",
    "            # TODO: update\n",
    "            # Totally experimental\n",
    "            n_samples = X.shape[0]\n",
    "            alpha = np.zeros((self.n_hidden, self.n_visible))\n",
    "            for i in range(self.n_visible):\n",
    "                alpha[np.argmax(self.alpha[:, i]), i] = 1\n",
    "            log_p = np.empty((2, n_samples, self.n_hidden))\n",
    "            c0 = np.einsum('ji,ij->j', alpha, self.theta[0])\n",
    "            c1 = np.einsum('ji,ij->j', alpha, self.theta[1])  # length n_hidden\n",
    "            info0 = np.einsum('ji,ij->ij', alpha, self.theta[2] - self.theta[0])\n",
    "            info1 = np.einsum('ji,ij->ij', alpha, self.theta[3] - self.theta[1])\n",
    "            log_p[1] = c1 + X.dot(info1)  # sum_i log p(xi=xi^l|y_j=1)  # Shape is 2 by l by j\n",
    "            log_p[0] = c0 + X.dot(info0)  # sum_i log p(xi=xi^l|y_j=0)\n",
    "            surprise = [-np.sum([log_p[labels[l, j], l, j] for j in range(self.n_hidden)]) for l in range(n_samples)]\n",
    "            return p_y_given_x, log_z, np.array(surprise)\n",
    "        elif details:\n",
    "            return p_y_given_x, log_z\n",
    "        else:\n",
    "            return labels\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.transform(X, details=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.transform(X, details=False)\n",
    "\n",
    "    def preprocess(self, X):\n",
    "        \"\"\"Data can be binary or can be in the range [0,1], where that is interpreted as the probability to\n",
    "        see this variable in a given sample\"\"\"\n",
    "        if X.max() > 1:\n",
    "            if self.count == 'binarize':\n",
    "                X = (X > 0)\n",
    "            elif self.count == 'fraction':\n",
    "                X = X.astype(float)\n",
    "                count = np.array(X.sum(axis=0), dtype=float).ravel()\n",
    "                length = np.array(X.sum(axis=1)).ravel().clip(1)\n",
    "                bg_rate = ss.diags(float(X.sum()) / count, 0)\n",
    "                doc_length = ss.diags(1. / length, 0)\n",
    "                # max_counts = ss.diags(1. / X.max(axis=1).A.ravel(), 0)\n",
    "                X = doc_length * X * bg_rate\n",
    "                X.data = np.clip(X.data, 0, 1)  # np.log(X.data) / (np.log(X.data) + 1)\n",
    "        return X\n",
    "\n",
    "    def initialize_parameters(self, X, words, docs):\n",
    "        \"\"\"Store some statistics about X for future use, and initialize alpha, tc\"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        if self.n_hidden > 1:\n",
    "            self.alpha = np.random.random((self.n_hidden, self.n_visible))\n",
    "            # self.alpha /= np.sum(self.alpha, axis=0, keepdims=True)\n",
    "        else:\n",
    "            self.alpha = np.ones((self.n_hidden, self.n_visible), dtype=float)\n",
    "        self.tc_history = []\n",
    "        self.tcs = np.zeros(self.n_hidden)\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        # Set column labels\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "        # Set row labels\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != X.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and X.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "        else:\n",
    "            self.row_index2doc = None\n",
    "\n",
    "    def update_word_parameters(self, X, words):\n",
    "        \"\"\"\n",
    "        updates parameters that need to be changed for each new model update\n",
    "        specifically, this re-calculates word count related parameters to be based on X,\n",
    "        where X is a batch of new data\n",
    "        \"\"\"\n",
    "        self.n_samples, self.n_visible = X.shape\n",
    "        self.word_counts = np.array(\n",
    "            X.sum(axis=0)).ravel()  # 1-d array of total word occurrences. (Probably slow for CSR)\n",
    "        if np.any(self.word_counts == 0) or np.any(self.word_counts == self.n_samples):\n",
    "            print('WARNING: Some words never appear (or always appear)')\n",
    "            self.word_counts = self.word_counts.clip(0.01, self.n_samples - 0.01)\n",
    "        self.word_freq = (self.word_counts).astype(float) / self.n_samples\n",
    "        self.px_frac = (np.log1p(-self.word_freq) - np.log(self.word_freq)).reshape((-1, 1))  # nv by 1\n",
    "        self.lp0 = np.log1p(-self.word_freq).reshape((-1, 1))  # log p(x_i=0)\n",
    "        self.h_x = binary_entropy(self.word_freq)\n",
    "        if self.verbose:\n",
    "            print('word counts', self.word_counts)\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != X.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and X.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "        else:\n",
    "            self.col_index2word = None\n",
    "            self.word2col_index = None\n",
    "\n",
    "    def preprocess_anchors(self, anchors):\n",
    "        \"\"\"Preprocess anchors so that it is a list of column indices if not already\"\"\"\n",
    "        if anchors is not None:\n",
    "            for n, anchor_list in enumerate(anchors):\n",
    "                # Check if list of anchors or a single str or int anchor\n",
    "                if type(anchor_list) is not list:\n",
    "                    anchor_list = [anchor_list]\n",
    "                # Convert list of anchors to list of anchor indices\n",
    "                new_anchor_list = []\n",
    "                for anchor in anchor_list:\n",
    "                    # Turn string anchors into index anchors\n",
    "                    if isinstance(anchor, string_types):\n",
    "                        if self.words is not None:\n",
    "                            if anchor in self.word2col_index:\n",
    "                                new_anchor_list.append(self.word2col_index[anchor])\n",
    "                            else:\n",
    "                                raise KeyError('Anchor word not in word column labels provided to CorEx: {}'.format(anchor))\n",
    "                        else:\n",
    "                                raise NameError(\"Provided non-index anchors to CorEx without also providing 'words'\")\n",
    "                    else:\n",
    "                        new_anchor_list.append(anchor)\n",
    "                # Update anchors with new anchor list\n",
    "                if len(new_anchor_list) == 1:\n",
    "                    anchors[n] = new_anchor_list[0]\n",
    "                else:\n",
    "                    anchors[n] = new_anchor_list\n",
    "\n",
    "        return anchors\n",
    "\n",
    "    def calculate_p_y(self, p_y_given_x):\n",
    "        \"\"\"Estimate log p(y_j=1).\"\"\"\n",
    "        return np.log(np.mean(p_y_given_x, axis=0))  # n_hidden, log p(y_j=1)\n",
    "\n",
    "    def calculate_theta(self, X, p_y_given_x, log_p_y):\n",
    "        \"\"\"Estimate marginal parameters from data and expected latent labels.\"\"\"\n",
    "        # log p(x_i=1|y)\n",
    "        n_samples = X.shape[0]\n",
    "        p_dot_y = X.T.dot(p_y_given_x).clip(0.01 * np.exp(log_p_y), (n_samples - 0.01) * np.exp(\n",
    "            log_p_y))  # nv by ns dot ns by m -> nv by m  # TODO: Change to CSC for speed?\n",
    "        lp_1g1 = np.log(p_dot_y) - np.log(n_samples) - log_p_y\n",
    "        lp_1g0 = np.log(self.word_counts[:, np.newaxis] - p_dot_y) - np.log(n_samples) - log_1mp(log_p_y)\n",
    "        lp_0g0 = log_1mp(lp_1g0)\n",
    "        lp_0g1 = log_1mp(lp_1g1)\n",
    "        return np.array([lp_0g0, lp_0g1, lp_1g0, lp_1g1])  # 4 by nv by m\n",
    "\n",
    "    def calculate_alpha(self, X, p_y_given_x, theta, log_p_y, tcs):\n",
    "        \"\"\"A rule for non-tree CorEx structure.\"\"\"\n",
    "        # TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\n",
    "        mis = self.calculate_mis(theta, log_p_y)\n",
    "        if self.n_hidden == 1:\n",
    "            alphaopt = np.ones((1, self.n_visible))\n",
    "        elif self.tree:\n",
    "            # sa = np.sum(self.alpha, axis=0)\n",
    "            tc_oom = 1. / self.n_samples\n",
    "            sa = np.sum(self.alpha[tcs > tc_oom], axis=0)\n",
    "            self.t = np.where(sa > 1.1, 1.3 * self.t, self.t)\n",
    "            # tc_oom = np.median(self.h_x)  # \\propto TC of a small group of corr. variables w/median entropy...\n",
    "            # t = 20 + (20 * np.abs(tcs) / tc_oom).reshape((self.n_hidden, 1))  # worked well in many tests\n",
    "            t = (1 + self.t * np.abs(tcs).reshape((self.n_hidden, 1)))\n",
    "            maxmis = np.max(mis, axis=0)\n",
    "            for i in np.where((mis == maxmis).sum(axis=0))[0]:  # Break ties for the largest MI\n",
    "                mis[:, i] += 1e-10 * np.random.random(self.n_hidden)\n",
    "                maxmis[i] = np.max(mis[:, i])\n",
    "            with np.errstate(under='ignore'):\n",
    "                alphaopt = np.exp(t * (mis - maxmis) / self.h_x)\n",
    "        else:\n",
    "            # TODO: Can we make a fast non-tree version of update in the AISTATS paper?\n",
    "            alphaopt = np.zeros((self.n_hidden, self.n_visible))\n",
    "            top_ys = np.argsort(-mis, axis=0)[:self.tree]\n",
    "            raise NotImplementedError\n",
    "        self.mis = mis  # So we don't have to recalculate it when used later\n",
    "        return alphaopt\n",
    "\n",
    "    def calculate_latent(self, X, theta):\n",
    "        \"\"\"\"Calculate the probability distribution for hidden factors for each sample.\"\"\"\n",
    "        ns, nv = X.shape\n",
    "        log_pygx_unnorm = np.empty((2, ns, self.n_hidden))\n",
    "        c0 = np.einsum('ji,ij->j', self.alpha, theta[0] - self.lp0)\n",
    "        c1 = np.einsum('ji,ij->j', self.alpha, theta[1] - self.lp0)  # length n_hidden\n",
    "        info0 = np.einsum('ji,ij->ij', self.alpha, theta[2] - theta[0] + self.px_frac)\n",
    "        info1 = np.einsum('ji,ij->ij', self.alpha, theta[3] - theta[1] + self.px_frac)\n",
    "        log_pygx_unnorm[1] = self.log_p_y + c1 + X.dot(info1)\n",
    "        log_pygx_unnorm[0] = log_1mp(self.log_p_y) + c0 + X.dot(info0)\n",
    "        return self.normalize_latent(log_pygx_unnorm)\n",
    "\n",
    "    def normalize_latent(self, log_pygx_unnorm):\n",
    "        \"\"\"Normalize the latent variable distribution\n",
    "        For each sample in the training set, we estimate a probability distribution\n",
    "        over y_j, each hidden factor. Here we normalize it. (Eq. 7 in paper.)\n",
    "        This normalization factor is used for estimating TC.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Unnormalized distribution of hidden factors for each training sample.\n",
    "        Returns\n",
    "        -------\n",
    "        p_y_given_x : 3D array, shape (n_hidden, n_samples)\n",
    "            p(y_j|x^l), the probability distribution over all hidden factors,\n",
    "            for data samples l = 1...n_samples\n",
    "        log_z : 2D array, shape (n_hidden, n_samples)\n",
    "            Point-wise estimate of total correlation explained by each Y_j for each sample,\n",
    "            used to estimate overall total correlation.\n",
    "        \"\"\"\n",
    "        with np.errstate(under='ignore'):\n",
    "            log_z = logsumexp(log_pygx_unnorm, axis=0)  # Essential to maintain precision.\n",
    "            log_pygx = log_pygx_unnorm[1] - log_z\n",
    "            p_norm = np.exp(log_pygx)\n",
    "        return p_norm.clip(1e-6, 1 - 1e-6), log_pygx, log_z  # ns by m\n",
    "\n",
    "    def update_tc(self, log_z):\n",
    "        self.tcs = np.mean(log_z, axis=0)\n",
    "        self.tc_history.append(np.sum(self.tcs))\n",
    "\n",
    "    def print_verbose(self):\n",
    "        if self.verbose:\n",
    "            print(self.tcs)\n",
    "        if self.verbose > 1:\n",
    "            print(self.alpha[:, :, 0])\n",
    "            print(self.theta)\n",
    "\n",
    "    def convergence(self):\n",
    "        if len(self.tc_history) > 10:\n",
    "            dist = -np.mean(self.tc_history[-10:-5]) + np.mean(self.tc_history[-5:])\n",
    "            return np.abs(dist) < self.eps  # Check for convergence.\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # In principle, if there were variables that are themselves classes... we have to handle it to pickle correctly\n",
    "        # But I think I programmed around all that.\n",
    "        self_dict = self.__dict__.copy()\n",
    "        return self_dict\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\" Pickle a class instance. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        #TODO: figure out why Unicode sometimes causes an issue with loading after pickling\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        import pickle\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        pickle.dump(self, open(filename, 'wb'), protocol=-1)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def save_joblib(self, filename):\n",
    "        \"\"\" Serialize a class instance with joblib - better for larger models. E.g., corex.save('saved.dat') \"\"\"\n",
    "        # Avoid saving words with object.\n",
    "        if self.words is not None:\n",
    "            temp_words = self.words\n",
    "            self.words = None\n",
    "        else:\n",
    "            temp_words = None\n",
    "        # Save CorEx object\n",
    "        if path.dirname(filename) and not path.exists(path.dirname(filename)):\n",
    "            makedirs(path.dirname(filename))\n",
    "        joblib.dump(self, filename)\n",
    "        # Restore words to CorEx object\n",
    "        self.words = temp_words\n",
    "\n",
    "    def sort_and_output(self, X):\n",
    "        order = np.argsort(self.tcs)[::-1]  # Order components from strongest TC to weakest\n",
    "        self.tcs = self.tcs[order]  # TC for each component\n",
    "        self.alpha = self.alpha[order]  # Connections between X_i and Y_j\n",
    "        self.log_p_y = self.log_p_y[order]  # Parameters defining the representation\n",
    "        self.theta = self.theta[:, :, order]  # Parameters defining the representation\n",
    "\n",
    "    def calculate_mis(self, theta, log_p_y):\n",
    "        \"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\n",
    "        p_y = np.exp(log_p_y).reshape((-1, 1))  # size n_hidden, 1\n",
    "        mis = self.h_x - p_y * binary_entropy(np.exp(theta[3]).T) - (1 - p_y) * binary_entropy(np.exp(theta[2]).T)\n",
    "        return (mis - 1. / (2. * self.n_samples)).clip(0.)  # P-T bias correction\n",
    "\n",
    "    def get_topics(self, n_words=10, topic=None, print_words=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top words for a topic\n",
    "        and each tuple is a pair (word, mutual information). If 'words' was not provided\n",
    "        to CorEx, then 'word' will be an integer column index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        print_words : boolean, get_topics will attempt to print topics using\n",
    "                      provided column labels (through 'words') if possible. Otherwise,\n",
    "                      topics will be consist of column indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return column word labels or indices\n",
    "        if self.words is None:\n",
    "            print_words = False\n",
    "            print(\"NOTE: 'words' not provided to CorEx. Returning topics as lists of column indices\")\n",
    "        elif len(self.words) != self.alpha.shape[1]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of column labels != number of columns of X. Cannot reliably add labels to topics. Check len(words) and X.shape[1]. Use .set_words() to fix')\n",
    "\n",
    "        topics = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            # Get indices of which words belong to the topic\n",
    "            inds = np.where(self.alpha[n] >= 1.)[0]\n",
    "            # Sort topic words according to mutual information\n",
    "            inds = inds[np.argsort(-self.alpha[n,inds] * self.mis[n,inds])]\n",
    "            # Create topic to return\n",
    "            if print_words is True:\n",
    "                topic = [(self.col_index2word[ind], self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            else:\n",
    "                topic = [(ind, self.sign[n,ind]*self.mis[n,ind]) for ind in inds[:n_words]]\n",
    "            # Add topic to list of topics if returning all topics. Otherwise, return topic\n",
    "            if len(topic_ns) != 1:\n",
    "                topics.append(topic)\n",
    "            else:\n",
    "                return topic\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def get_top_docs(self, n_docs=10, topic=None, sort_by='log_prob', print_docs=True):\n",
    "        \"\"\"\n",
    "        Return list of lists of tuples. Each list consists of the top docs for a topic\n",
    "        and each tuple is a pair (doc, pointwise TC or probability). If 'docs' was not\n",
    "        provided to CorEx, then each doc will be an integer row index of X\n",
    "        topic_n : integer specifying which topic to get (0-indexed)\n",
    "        sort_by: 'log_prob' or 'tc', use either 'log_p_y_given_x' or 'log_z' respectively\n",
    "                 to return top docs per each topic\n",
    "        print_docs : boolean, get_top_docs will attempt to print topics using\n",
    "                     provided row labels (through 'docs') if possible. Otherwise,\n",
    "                     top docs will be consist of row indices of X\n",
    "        \"\"\"\n",
    "        # Determine which topics to iterate over\n",
    "        if topic is not None:\n",
    "            topic_ns = [topic]\n",
    "        else:\n",
    "            topic_ns = list(range(self.labels.shape[1]))\n",
    "        # Determine whether to return row doc labels or indices\n",
    "        if self.docs is None:\n",
    "            print_docs = False\n",
    "            print(\"NOTE: 'docs' not provided to CorEx. Returning top docs as lists of row indices\")\n",
    "        elif len(self.docs) != self.labels.shape[0]:\n",
    "            print_words = False\n",
    "            print('WARNING: number of row labels != number of rows of X. Cannot reliably add labels. Check len(docs) and X.shape[0]. Use .set_docs() to fix')\n",
    "        # Get appropriate matrix to sort\n",
    "        if sort_by == 'log_prob':\n",
    "            doc_values = self.log_p_y_given_x\n",
    "        elif sort_by == 'tc':\n",
    "            print('WARNING: sorting by logz not well tested')\n",
    "            doc_values = self.log_z\n",
    "        else:\n",
    "            print(\"Invalid 'sort_by' parameter, must be 'prob' or 'tc'\")\n",
    "            return\n",
    "        # Get top docs for each topic\n",
    "        doc_inds = np.argsort(-doc_values, axis=0)\n",
    "        top_docs = [] # TODO: make this faster, it's slower than it should be\n",
    "        for n in topic_ns:\n",
    "            if print_docs is True:\n",
    "                topic_docs = [(self.row_index2doc[ind], doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            else:\n",
    "                topic_docs = [(ind, doc_values[ind,n]) for ind in doc_inds[:n_docs,n]]\n",
    "            # Add docs to list of top docs per topic if returning all topics. Otherwise, return\n",
    "            if len(topic_ns) != 1:\n",
    "                top_docs.append(topic_docs)\n",
    "            else:\n",
    "                return topic_docs\n",
    "\n",
    "        return top_docs\n",
    "\n",
    "    def set_words(self, words):\n",
    "        self.words = words\n",
    "        if words is not None:\n",
    "            if len(words) != self.alpha.shape[1]:\n",
    "                print('WARNING: number of column labels != number of columns of X. Check len(words) and .alpha.shape[1]')\n",
    "            col_index2word = {index:word for index,word in enumerate(words)}\n",
    "            word2col_index = {word:index for index,word in enumerate(words)}\n",
    "            self.col_index2word = col_index2word\n",
    "            self.word2col_index = word2col_index\n",
    "\n",
    "    def set_docs(self, docs):\n",
    "        self.docs = docs\n",
    "        if docs is not None:\n",
    "            if len(docs) != self.labels.shape[0]:\n",
    "                print('WARNING: number of row labels != number of rows of X. Check len(docs) and .labels.shape[0]')\n",
    "            row_index2doc = {index:doc for index,doc in enumerate(docs)}\n",
    "            self.row_index2doc = row_index2doc\n",
    "\n",
    "\n",
    "def log_1mp(x):\n",
    "    return np.log1p(-np.exp(x))\n",
    "\n",
    "\n",
    "def binary_entropy(p):\n",
    "    return np.where(p > 0, - p * np.log2(p) - (1 - p) * np.log2(1 - p), 0)\n",
    "\n",
    "\n",
    "def flatten(a):\n",
    "    b = []\n",
    "    for ai in a:\n",
    "        if type(ai) is list:\n",
    "            b += ai\n",
    "        else:\n",
    "            b.append(ai)\n",
    "    return b\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    \"\"\" Unpickle class instance. \"\"\"\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_joblib(filename):\n",
    "    \"\"\" Load class instance with joblib. \"\"\"\n",
    "    return joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "import corex_topic as ct\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data which is in .txt file line by line, split by tab(\\t) to a list\n",
    "list_data=[]\n",
    "with open('A:\\\\Greg-NLP\\\\urom\\\\il6_original.txt',encoding='utf8',errors='ignore') as fp:\n",
    "    for line in fp:\n",
    "        list_data.append(line.split('\\t'))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the columns of datframe\n",
    "raw_data = pd.DataFrame(list_data,columns=['doc_id','text_data','class_type','additional']) \n",
    "#drop if any additional columns gets created as part of reading process\n",
    "raw_data=raw_data.drop('additional',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip columns for leading and trailing white spaces\n",
    "raw_data['doc_id']=raw_data.doc_id.str.strip()\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n docs x m attributes\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL6_SN_000370_20160808_H0T0060AI</td>\n",
       "      <td>RT @Advocacy4Oromia: #OromOProtests- Hiriira G...</td>\n",
       "      <td>eval_incident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL6_WL_020519_20170712_H0040N5K3</td>\n",
       "      <td>\"Godina Arsii Aanota hedduu keessatti Ummanni ...</td>\n",
       "      <td>unk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             doc_id  \\\n",
       "0  IL6_SN_000370_20160808_H0T0060AI   \n",
       "1  IL6_WL_020519_20170712_H0040N5K3   \n",
       "\n",
       "                                           text_data     class_type  \n",
       "0  RT @Advocacy4Oromia: #OromOProtests- Hiriira G...  eval_incident  \n",
       "1  \"Godina Arsii Aanota hedduu keessatti Ummanni ...            unk  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek into the data\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set doc_id as index\n",
    "raw_data= raw_data.set_index('doc_id')\n",
    "# change \"class_type\" column to \"categorical\" datatype\n",
    "raw_data['class_type'] = raw_data['class_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http links removal on 'text_data' column\n",
    "# regex : ((http|https)://t.co/[a-zA-Z0-9]+)\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('((http|https)://t.co/[a-zA-Z0-9]+)','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RT (Retweet) keyword removal\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('RT','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punctuation removal on 'text_data' column\n",
    "#print(string.punctuation)\n",
    "punct='!\"$%&()*+,-./:;<=>?[\\]^_`{|}~'+\"'\"\n",
    "#print(punct)\n",
    "regex = re.compile('[%s]' % re.escape(punct))\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: regex.sub('', x))\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7016, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove @names mentioned as part of tweets\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('\\@[a-zA-Z0-9]+','',x))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word count (length) of the document\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emoji's from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\"\\U0001F600-\\U0001F64F\" \"\\U0001F300-\\U0001F5FF\" \"\\U0001F680-\\U0001F6FF\"  \"\\U0001F1E0-\\U0001F1FF\"]+',' ',x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Removal/ number removal from the documents\n",
    "\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: re.sub('[\\d]+','',x))\n",
    "\n",
    "# strip whitespaces again \n",
    "\n",
    "raw_data['text_data']=raw_data.text_data.str.strip()\n",
    "raw_data['class_type']=raw_data['class_type'].str.strip()\n",
    "\n",
    "# calculate length again\n",
    "raw_data['length'] = raw_data['text_data'].apply(lambda x: len(x.split()))\n",
    "# sort by file length\n",
    "raw_data=raw_data.sort_values(by='length', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text_data</th>\n",
       "      <th>class_type</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>IL6_SN_000370_20170329_H0T003I57</td>\n",
       "      <td>YUTARARATUNG</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>IL6_SN_000370_20170330_H0T003N16</td>\n",
       "      <td>Kiri</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                doc_id     text_data class_type  length\n",
       "2501  IL6_SN_000370_20170329_H0T003I57  YUTARARATUNG        unk       1\n",
       "576   IL6_SN_000370_20170330_H0T003N16          Kiri        unk       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the documents with no words after pre-processing\n",
    "raw_data = raw_data.loc[raw_data.length>0]\n",
    "raw_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing documents with less than 5 words\n",
    "raw_data = raw_data.loc[raw_data.length>5]\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the letters to lower case\n",
    "raw_data['text_data'] = raw_data['text_data'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unk              3901\n",
      "eval_incident    1605\n",
      "indomain          201\n",
      "nondomain         164\n",
      "Name: class_type, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2242cf666a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# document count across 4 class labels (unk,nondomain,indomain,eval_incident)\n",
    "print(raw_data.class_type.value_counts())\n",
    "plt.hist('class_type',data=raw_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the formatted data to a new text file for future reference\n",
    "\n",
    "reset_data=raw_data.reset_index()\n",
    "reset_data.to_csv('A:\\\\Greg-NLP\\\\urom\\\\new.txt', header=True, index=False, sep='\\t', mode='w',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eval_incident</th>\n",
       "      <td>1605.0</td>\n",
       "      <td>35.611838</td>\n",
       "      <td>108.167412</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>981.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indomain</th>\n",
       "      <td>201.0</td>\n",
       "      <td>37.840796</td>\n",
       "      <td>100.880447</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>768.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nondomain</th>\n",
       "      <td>164.0</td>\n",
       "      <td>42.073171</td>\n",
       "      <td>112.330503</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>684.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unk</th>\n",
       "      <td>3901.0</td>\n",
       "      <td>125.700333</td>\n",
       "      <td>344.687689</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6062.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               length                                                       \n",
       "                count        mean         std  min   25%   50%   75%     max\n",
       "class_type                                                                  \n",
       "eval_incident  1605.0   35.611838  108.167412  6.0  10.0  12.0  14.0   981.0\n",
       "indomain        201.0   37.840796  100.880447  6.0  11.0  12.0  14.0   768.0\n",
       "nondomain       164.0   42.073171  112.330503  6.0  11.0  13.0  14.0   684.0\n",
       "unk            3901.0  125.700333  344.687689  6.0   9.0  14.0  27.0  6062.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# basic stats to understand document length across 4 class labels\n",
    "raw_data.groupby('class_type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Term matrix (binary matrix) with max_df =0.995, min_df =0.001\n",
    "vectorizer = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = True,lowercase=True)\n",
    "doc_word_mat = vectorizer.fit_transform(raw_data.text_data)\n",
    "doc_word_mat = ss.csr_matrix(doc_word_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5871, 8360)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word_mat.shape # n_docs x m_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 50 topics\n",
    "\n",
    "topic_model = Corex(n_hidden=25, words=words, max_iter=500, verbose=False, seed=3192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corex at 0x2242e7729e8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.fit(doc_word_mat, words=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: oromoo,qabsoo,keenya,ummata,maqaa,abbaa,osoo,wayyaanee,ilmaan,bilisummaa\n",
      "1: dhugaa,ifatti,aadaa,gaafa,abdii,qabna,caalaa,lammii,keessan,qooda\n",
      "2: warra,tana,achi,qofaa,fakkaata,kunoo,ofitti,dandaama,habashaa,gahuu\n",
      "3: akka,kan,kun,jira,adda,qabu,garuu,waliin,kanaan,bara\n",
      "4: lafa,mirga,aangoo,yeroon,warri,as,murtii,duula,harkaa,qabuu\n",
      "5: guddaa,gadi,ammaa,hojiin,kaayyoo,ija,gahuuf,bittaa,akeeka,tarkaanfiin\n",
      "6: jedhan,washington,dc,obbo,xurree,kuma,marsariitii,dubbatan,taphachisi,has\n",
      "7: isa,miti,dhimma,qofa,hunda,rakkoo,addunyaa,siyaasaa,yaada,biyyoota\n",
      "8: irraa,wal,nama,isaan,karaa,nu,akkuma,amma,jedhanii,biraa\n",
      "9: of,ykn,qabne,biyyaa,garaa,isaaf,hundaan,dantaa,yakka,jiraatu\n",
      "10: irra,jedhu,inni,harka,jedhee,ittiin,keenyaa,jalaa,namaa,sana\n",
      "11: hin,waan,malee,haa,qaba,taane,taee,bira,namni,taeef\n",
      "12: isaa,keessaa,tokko,yeroo,jechuun,tauu,kanaaf,hedduu,walitti,ammoo\n",
      "13: ni,ofii,buaa,dhiiga,jireenya,jirru,lafaa,hamaa,akkanaa,mee\n",
      "14: kana,jiru,fi,ture,mootummaa,humna,akkasumas,jiraachuu,uummata,dandeenye\n",
      "15: jiran,yoo,kanneen,oromiyaa,tau,sadarkaa,naannoo,waggaa,waraanaa,haaluma\n",
      "16: tae,namoota,gama,alaa,namoonni,fedhii,biyyattii,barbaachisu,qaama,dabalatee\n",
      "17: mana,aku,nak,yang,yg,ada,tak,oromo,macam,dia\n",
      "18: keessa,haalli,waraana,tokkoo,mooraa,jiraatan,sochiin,barattoota,barnootaa,xumura\n",
      "19: keessatti,itti,godina,magaalaa,aanaa,bahaa,fxg,lixaa,fufee,qeerroon\n",
      "20: irratti,biyya,kanaa,tauun,tarkaanfii,duraa,guddaan,hawaasa,mormii,marii\n",
      "21: isaanii,gara,hojii,jechuu,ffaa,kaasee,darbe,gahaa,isaatti,barnoota\n",
      "22: booda,dura,mootummaan,erga,guutuu,jirtu,kunis,jiruuf,lamaan,ishee\n",
      "23: bakka,jala,nagaa,bulchiinsa,qabeenya,jedhaman,olii,waraanni,qawwee,ganama\n",
      "24: gaaffii,deebii,ibsa,abo,sabaa,ilaalchisee,gaaffiin,kutaa,barbaachisaa,oduu\n"
     ]
    }
   ],
   "source": [
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e-06 1.00000000e-06 1.00000000e-06 ... 4.71636343e-04\n",
      "  3.74097308e-04 4.58551485e-04]\n",
      " [1.00000000e-06 1.00000000e-06 1.00000000e-06 ... 5.00894996e-04\n",
      "  3.95908511e-04 4.98719191e-04]\n",
      " [1.00000000e-06 1.00000000e-06 1.00000000e-06 ... 4.98006855e-04\n",
      "  3.94037181e-04 4.92243816e-04]\n",
      " ...\n",
      " [9.99999000e-01 9.99999000e-01 9.99999000e-01 ... 9.99999000e-01\n",
      "  9.99999000e-01 9.99999000e-01]\n",
      " [9.99999000e-01 9.99999000e-01 9.99999000e-01 ... 9.99999000e-01\n",
      "  9.99999000e-01 9.99999000e-01]\n",
      " [9.99999000e-01 9.99999000e-01 9.99999000e-01 ... 9.99999000e-01\n",
      "  9.99999000e-01 9.99999000e-01]]\n"
     ]
    }
   ],
   "source": [
    "#The estimated probabilities of topics for each document can be accessed through p_y_given_x.\n",
    "print(topic_model.p_y_given_x) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True False]]\n"
     ]
    }
   ],
   "source": [
    "#softmax binary classification of which documents belong to each topic.\n",
    "print(topic_model.labels) # n_docs x k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor words\n",
    "#[\"chocho'a\", \"lafaa\",\"socho'a\",\"lafa\"], \n",
    "\n",
    "#sochoaa sochoan sochouun  sochouu  \n",
    "anchor_words = [[\"hoongee\",\"hongee\"],[\"galaana\",\"galaanaa\"],[\"balaa\"]]\n",
    "\n",
    "anchored_topic_model = Corex(n_hidden=25, seed=3192)\n",
    "anchored_topic_model.fit(doc_word_mat, words=words, anchors=anchor_words, anchor_strength=6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: booda,addunyaa,tokkummaa,hongee,lamaan,hamaa,jiranii,dhuma,ammo,caalaatti\n",
      "1: malee,isa,haa,osoo,inni,miti,qofa,gadi,seenaa,harka\n",
      "2: balaa,barbaachisu,qabaachuu,adeemsa,cunqursaa,barbaadan,faallaa,biyyi,addunyaatti,balaan\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_ = zip(*anchored_topic_model.get_topics(topic=n))\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach guided-LDA\n",
    "# reference links below\n",
    "#1) https://medium.freecodecamp.org/how-we-changed-unsupervised-lda-to-semi-supervised-guidedlda-e36a95f3a164\n",
    "#2)https://github.com/vi3k6i5/GuidedLDA\n",
    "\n",
    "import numpy as np\n",
    "import guidedlda\n",
    "\n",
    "\n",
    "model = guidedlda.GuidedLDA(n_topics=3, n_iter=100, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5871x8360 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 247375 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document-Term Matrix-(count matrix)\n",
    "vectorizer2 = CountVectorizer(encoding='utf-8',stop_words=None,max_df =0.995, min_df =0.001, binary = False,lowercase=True)\n",
    "doc_word_mat2 = vectorizer2.fit_transform(raw_data.text_data)\n",
    "doc_word_mat2 = ss.csr_matrix(doc_word_mat2)\n",
    "doc_word_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words2 = list(np.asarray(vectorizer2.get_feature_names()))\n",
    "word2id = dict((v, idx) for idx, v in enumerate(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding anchor words\n",
    "seed_topic_list =[[\"hoongee\",\"hongee\"],[\"galaana\",\"galaanaa\"],[\"balaa\"]]\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 5871\n",
      "INFO:guidedlda:vocab_size: 8360\n",
      "INFO:guidedlda:n_words: 442803\n",
      "INFO:guidedlda:n_topics: 3\n",
      "INFO:guidedlda:n_iter: 100\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\guidedlda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -3943915\n",
      "INFO:guidedlda:<20> log likelihood: -3555684\n",
      "INFO:guidedlda:<40> log likelihood: -3512991\n",
      "INFO:guidedlda:<60> log likelihood: -3495506\n",
      "INFO:guidedlda:<80> log likelihood: -3480338\n",
      "INFO:guidedlda:<99> log likelihood: -3461993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x22433300198>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(doc_word_mat2, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: hin mana waan kan akka ni kana malee yoo tokko\n",
      "Topic 1: fi oromoo kan akka itti irraa isaa keessatti hin jiru\n",
      "Topic 2: akka fi kan irratti kana hin keessatti isaanii jiru kun\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(words2)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
